{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline \n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, RocCurveDisplay, make_scorer\n",
    "from sklearn.model_selection import learning_curve\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "import datetime\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_transformed = pd.read_csv('new_data/clean-data_without_outliers.csv')\n",
    "#competition_transformed = pd.read_csv('new_data/clean-comp.csv')\n",
    "#data = pd.read_csv('new_data/clean-data_without_outliers.csv')\n",
    "competition = pd.read_csv('new_data/clean-comp.csv')\n",
    "\n",
    "data = pd.read_csv('new_data/complete-data.csv')\n",
    "#data_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>playerID</th>\n",
       "      <th>year</th>\n",
       "      <th>tmID</th>\n",
       "      <th>GP</th>\n",
       "      <th>oRebounds</th>\n",
       "      <th>dRebounds</th>\n",
       "      <th>dq</th>\n",
       "      <th>ft%</th>\n",
       "      <th>fg%</th>\n",
       "      <th>three%</th>\n",
       "      <th>gs%</th>\n",
       "      <th>efg%</th>\n",
       "      <th>ts%</th>\n",
       "      <th>ppg</th>\n",
       "      <th>rpg</th>\n",
       "      <th>apg</th>\n",
       "      <th>spg</th>\n",
       "      <th>bpg</th>\n",
       "      <th>eff</th>\n",
       "      <th>pp36</th>\n",
       "      <th>defensive_prowess</th>\n",
       "      <th>defensive_discipline</th>\n",
       "      <th>mpg</th>\n",
       "      <th>pos</th>\n",
       "      <th>college</th>\n",
       "      <th>PostStat</th>\n",
       "      <th>playoff</th>\n",
       "      <th>confID</th>\n",
       "      <th>playoff_progression</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>award_count</th>\n",
       "      <th>career_year</th>\n",
       "      <th>playoff_progression_rolling</th>\n",
       "      <th>playoff_rolling</th>\n",
       "      <th>pp36_rolling</th>\n",
       "      <th>eff_rolling</th>\n",
       "      <th>award_count_rolling</th>\n",
       "      <th>defensive_prowess_rolling</th>\n",
       "      <th>defensive_discipline_rolling</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>691</th>\n",
       "      <td>397</td>\n",
       "      <td>3</td>\n",
       "      <td>573</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1.38</td>\n",
       "      <td>1.83</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.88</td>\n",
       "      <td>4.71</td>\n",
       "      <td>3.21</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.08</td>\n",
       "      <td>-106.50</td>\n",
       "      <td>9.72</td>\n",
       "      <td>28.3</td>\n",
       "      <td>5.42</td>\n",
       "      <td>17.54</td>\n",
       "      <td>2</td>\n",
       "      <td>590</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>701</td>\n",
       "      <td>0.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>11.34</td>\n",
       "      <td>-112.170</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.65</td>\n",
       "      <td>5.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>191</td>\n",
       "      <td>3</td>\n",
       "      <td>568</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.22</td>\n",
       "      <td>2.33</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.34</td>\n",
       "      <td>13.32</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.00</td>\n",
       "      <td>6.33</td>\n",
       "      <td>2</td>\n",
       "      <td>615</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>701</td>\n",
       "      <td>4.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>180</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15.12</td>\n",
       "      <td>-100.605</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.65</td>\n",
       "      <td>6.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>70</td>\n",
       "      <td>3</td>\n",
       "      <td>559</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1.44</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.06</td>\n",
       "      <td>8.47</td>\n",
       "      <td>5.81</td>\n",
       "      <td>1.66</td>\n",
       "      <td>1.69</td>\n",
       "      <td>1.16</td>\n",
       "      <td>-171.21</td>\n",
       "      <td>10.08</td>\n",
       "      <td>72.2</td>\n",
       "      <td>8.76</td>\n",
       "      <td>29.78</td>\n",
       "      <td>2</td>\n",
       "      <td>619</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>1.0</td>\n",
       "      <td>702</td>\n",
       "      <td>0.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>185</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>10.26</td>\n",
       "      <td>-212.575</td>\n",
       "      <td>0.0</td>\n",
       "      <td>79.70</td>\n",
       "      <td>8.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>187</td>\n",
       "      <td>3</td>\n",
       "      <td>573</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.96</td>\n",
       "      <td>9.91</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.25</td>\n",
       "      <td>1.12</td>\n",
       "      <td>0.09</td>\n",
       "      <td>-243.63</td>\n",
       "      <td>13.32</td>\n",
       "      <td>27.2</td>\n",
       "      <td>7.32</td>\n",
       "      <td>26.84</td>\n",
       "      <td>1</td>\n",
       "      <td>582</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>701</td>\n",
       "      <td>0.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>165</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.24</td>\n",
       "      <td>-171.655</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.05</td>\n",
       "      <td>6.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>900</th>\n",
       "      <td>504</td>\n",
       "      <td>3</td>\n",
       "      <td>558</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.97</td>\n",
       "      <td>3.98</td>\n",
       "      <td>0.05</td>\n",
       "      <td>1.70</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.83</td>\n",
       "      <td>1.44</td>\n",
       "      <td>1.78</td>\n",
       "      <td>1.88</td>\n",
       "      <td>9.37</td>\n",
       "      <td>4.95</td>\n",
       "      <td>5.84</td>\n",
       "      <td>2.71</td>\n",
       "      <td>0.18</td>\n",
       "      <td>-103.95</td>\n",
       "      <td>13.68</td>\n",
       "      <td>68.8</td>\n",
       "      <td>13.88</td>\n",
       "      <td>46.17</td>\n",
       "      <td>1</td>\n",
       "      <td>581</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>2.0</td>\n",
       "      <td>701</td>\n",
       "      <td>2.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>138</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.30</td>\n",
       "      <td>-101.850</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.90</td>\n",
       "      <td>5.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>247</td>\n",
       "      <td>11</td>\n",
       "      <td>556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>575</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>702</td>\n",
       "      <td>0.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>196</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>19.98</td>\n",
       "      <td>-267.790</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53.75</td>\n",
       "      <td>10.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>260</td>\n",
       "      <td>11</td>\n",
       "      <td>574</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>636</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>702</td>\n",
       "      <td>0.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>180</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>12.42</td>\n",
       "      <td>-228.340</td>\n",
       "      <td>0.0</td>\n",
       "      <td>54.55</td>\n",
       "      <td>8.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>262</td>\n",
       "      <td>11</td>\n",
       "      <td>745</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>667</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>701</td>\n",
       "      <td>0.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>175</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>10.44</td>\n",
       "      <td>-131.025</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.25</td>\n",
       "      <td>6.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836</th>\n",
       "      <td>466</td>\n",
       "      <td>11</td>\n",
       "      <td>573</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>746</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>701</td>\n",
       "      <td>0.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>168</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.52</td>\n",
       "      <td>-173.405</td>\n",
       "      <td>0.0</td>\n",
       "      <td>46.45</td>\n",
       "      <td>10.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>968</th>\n",
       "      <td>549</td>\n",
       "      <td>11</td>\n",
       "      <td>574</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>700</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>702</td>\n",
       "      <td>0.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>166</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>20.16</td>\n",
       "      <td>-148.090</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42.75</td>\n",
       "      <td>9.27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>969 rows Ã— 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     playerID  year  tmID    GP  oRebounds  dRebounds    dq   ft%   fg%  \\\n",
       "691       397     3   573  24.0       1.38       1.83  0.00  0.61  0.41   \n",
       "287       191     3   568   3.0       0.33       0.33  0.00  0.75  0.50   \n",
       "97         70     3   559  32.0       1.44       4.38  0.00  0.83  0.46   \n",
       "284       187     3   573  32.0       0.50       1.50  0.00  0.76  0.38   \n",
       "900       504     3   558  32.0       0.97       3.98  0.05  1.70  0.71   \n",
       "..        ...   ...   ...   ...        ...        ...   ...   ...   ...   \n",
       "413       247    11   556   0.0       0.00       0.00  0.00  0.00  0.00   \n",
       "424       260    11   574   0.0       0.00       0.00  0.00  0.00  0.00   \n",
       "427       262    11   745   0.0       0.00       0.00  0.00  0.00  0.00   \n",
       "836       466    11   573   0.0       0.00       0.00  0.00  0.00  0.00   \n",
       "968       549    11   574   0.0       0.00       0.00  0.00  0.00  0.00   \n",
       "\n",
       "     three%   gs%  efg%   ts%   ppg   rpg   apg   spg   bpg     eff   pp36  \\\n",
       "691    0.00  0.50  0.82  0.88  4.71  3.21  0.58  0.92  0.08 -106.50   9.72   \n",
       "287    0.00  0.00  1.00  1.22  2.33  0.67  0.33  0.33  0.00   -0.34  13.32   \n",
       "97     0.40  1.00  1.00  1.06  8.47  5.81  1.66  1.69  1.16 -171.21  10.08   \n",
       "284    0.32  0.97  0.86  0.96  9.91  2.00  3.25  1.12  0.09 -243.63  13.32   \n",
       "900    0.83  1.44  1.78  1.88  9.37  4.95  5.84  2.71  0.18 -103.95  13.68   \n",
       "..      ...   ...   ...   ...   ...   ...   ...   ...   ...     ...    ...   \n",
       "413    0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00    0.00   0.00   \n",
       "424    0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00    0.00   0.00   \n",
       "427    0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00    0.00   0.00   \n",
       "836    0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00    0.00   0.00   \n",
       "968    0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00    0.00   0.00   \n",
       "\n",
       "     defensive_prowess  defensive_discipline    mpg  pos  college  PostStat  \\\n",
       "691               28.3                  5.42  17.54    2      590      0.00   \n",
       "287                6.7                  2.00   6.33    2      615      0.00   \n",
       "97                72.2                  8.76  29.78    2      619     -0.26   \n",
       "284               27.2                  7.32  26.84    1      582      0.00   \n",
       "900               68.8                 13.88  46.17    1      581     -0.11   \n",
       "..                 ...                   ...    ...  ...      ...       ...   \n",
       "413                0.0                  0.00   0.00    2      575      0.00   \n",
       "424                0.0                  0.00   0.00    2      636      0.00   \n",
       "427                0.0                  0.00   0.00    2      667      0.00   \n",
       "836                0.0                  0.00   0.00    2      746      0.00   \n",
       "968                0.0                  0.00   0.00    2      700      0.00   \n",
       "\n",
       "     playoff  confID  playoff_progression  height  weight  award_count  \\\n",
       "691      0.0     701                  0.0    71.0     150          0.0   \n",
       "287      1.0     701                  4.0    73.0     180          0.0   \n",
       "97       1.0     702                  0.0    75.0     185          0.0   \n",
       "284      0.0     701                  0.0    71.0     165          0.0   \n",
       "900      2.0     701                  2.0    66.0     138          0.0   \n",
       "..       ...     ...                  ...     ...     ...          ...   \n",
       "413      0.0     702                  0.0    74.0     196          0.0   \n",
       "424      0.0     702                  0.0    76.0     180          0.0   \n",
       "427      0.0     701                  0.0    75.0     175          0.0   \n",
       "836      0.0     701                  0.0    73.0     168          0.0   \n",
       "968      0.0     702                  0.0    74.0     166          0.0   \n",
       "\n",
       "     career_year  playoff_progression_rolling  playoff_rolling  pp36_rolling  \\\n",
       "691            3                          0.5              0.5         11.34   \n",
       "287            3                          1.5              1.0         15.12   \n",
       "97             3                          0.5              0.5         10.26   \n",
       "284            3                          0.0              0.0         12.24   \n",
       "900            3                          2.5              1.0          6.30   \n",
       "..           ...                          ...              ...           ...   \n",
       "413            9                          0.5              0.5         19.98   \n",
       "424            6                          1.0              0.5         12.42   \n",
       "427            5                          0.5              0.5         10.44   \n",
       "836            9                          4.0              1.0         20.52   \n",
       "968            3                          0.5              0.5         20.16   \n",
       "\n",
       "     eff_rolling  award_count_rolling  defensive_prowess_rolling  \\\n",
       "691     -112.170                  0.0                      24.65   \n",
       "287     -100.605                  0.0                      30.65   \n",
       "97      -212.575                  0.0                      79.70   \n",
       "284     -171.655                  0.0                      22.05   \n",
       "900     -101.850                  0.0                      33.90   \n",
       "..           ...                  ...                        ...   \n",
       "413     -267.790                  0.0                      53.75   \n",
       "424     -228.340                  0.0                      54.55   \n",
       "427     -131.025                  0.0                      20.25   \n",
       "836     -173.405                  0.0                      46.45   \n",
       "968     -148.090                  0.0                      42.75   \n",
       "\n",
       "     defensive_discipline_rolling  \n",
       "691                          5.90  \n",
       "287                          6.40  \n",
       "97                           8.90  \n",
       "284                          6.06  \n",
       "900                          5.27  \n",
       "..                            ...  \n",
       "413                         10.39  \n",
       "424                          8.59  \n",
       "427                          6.59  \n",
       "836                         10.29  \n",
       "968                          9.27  \n",
       "\n",
       "[969 rows x 40 columns]"
      ]
     },
     "execution_count": 577,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sort_values('year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a teams_conf dictionary by getting unique pairs of teams and conferences\n",
    "teams_conf = data[['tmID', 'confID']].drop_duplicates()\n",
    "teams_conf = dict(zip(teams_conf.tmID, teams_conf.confID))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['playerID', 'year', 'stint', 'tmID', 'height', 'weight', 'pos',\n",
       "       'college', 'confID'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 579,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "competition.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['playerID', 'year', 'tmID', 'pos', 'college', 'height', 'weight', 'career_year', 'confID', 'playoff_progression_rolling', 'playoff_rolling', 'pp36_rolling', 'eff_rolling', 'award_count_rolling', 'defensive_prowess_rolling', 'defensive_discipline_rolling']\n"
     ]
    }
   ],
   "source": [
    "input_cols = ['playerID', 'year', 'tmID', 'pos', 'college', 'height', 'weight', 'career_year', 'confID']\n",
    "\n",
    "#add all columns ending in rolling from data to input cols\n",
    "input_cols+=[c for c in data.columns if c.endswith(\"_rolling\")]\n",
    "\n",
    "# The output columns are the genres\n",
    "output_cols = 'playoff'\n",
    "\n",
    "known_columns = ['playerID', 'year', 'tmID', 'height', 'weight', 'pos','college', 'confID']\n",
    "\n",
    "# Averages to calculate for precision, recall, and f1-score\n",
    "averages = [None, \"macro\", \"weighted\", \"micro\", \"samples\"]\n",
    "\n",
    "print(input_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop any columns that are not in input_cols or output_cols\n",
    "data = data[input_cols+[output_cols]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from https://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import learning_curve\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "def plot_learning_curve(\n",
    "    title,\n",
    "    train_sizes, \n",
    "    train_scores, \n",
    "    test_scores, \n",
    "    fit_times,\n",
    "    score_times,\n",
    "    axes=None,\n",
    "    ylim=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate 3 plots: the test and training learning curve, the training\n",
    "    samples vs fit times curve, the fit times vs score curve.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : estimator instance\n",
    "        An estimator instance implementing `fit` and `predict` methods which\n",
    "        will be cloned for each validation.\n",
    "\n",
    "    title : str\n",
    "        Title for the chart.\n",
    "\n",
    "    X : array-like of shape (n_samples, n_features)\n",
    "        Training vector, where ``n_samples`` is the number of samples and\n",
    "        ``n_features`` is the number of features.\n",
    "\n",
    "    y : array-like of shape (n_samples) or (n_samples, n_features)\n",
    "        Target relative to ``X`` for classification or regression;\n",
    "        None for unsupervised learning.\n",
    "\n",
    "    axes : array-like of shape (3,), default=None\n",
    "        Axes to use for plotting the curves.\n",
    "\n",
    "    ylim : tuple of shape (2,), default=None\n",
    "        Defines minimum and maximum y-values plotted, e.g. (ymin, ymax).\n",
    "\n",
    "    cv : int, cross-validation generator or an iterable, default=None\n",
    "        Determines the cross-validation splitting strategy.\n",
    "        Possible inputs for cv are:\n",
    "\n",
    "          - None, to use the default 5-fold cross-validation,\n",
    "          - integer, to specify the number of folds.\n",
    "          - :term:`CV splitter`,\n",
    "          - An iterable yielding (train, test) splits as arrays of indices.\n",
    "\n",
    "        For integer/None inputs, if ``y`` is binary or multiclass,\n",
    "        :class:`StratifiedKFold` used. If the estimator is not a classifier\n",
    "        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n",
    "\n",
    "        Refer :ref:`User Guide <cross_validation>` for the various\n",
    "        cross-validators that can be used here.\n",
    "\n",
    "    n_jobs : int or None, default=None\n",
    "        Number of jobs to run in parallel.\n",
    "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
    "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
    "        for more details.\n",
    "\n",
    "    train_sizes : array-like of shape (n_ticks,)\n",
    "        Relative or absolute numbers of training examples that will be used to\n",
    "        generate the learning curve. If the ``dtype`` is float, it is regarded\n",
    "        as a fraction of the maximum size of the training set (that is\n",
    "        determined by the selected validation method), i.e. it has to be within\n",
    "        (0, 1]. Otherwise it is interpreted as absolute sizes of the training\n",
    "        sets. Note that for classification the number of samples usually have\n",
    "        to be big enough to contain at least one sample from each class.\n",
    "        (default: np.linspace(0.1, 1.0, 5))\n",
    "    \"\"\"\n",
    "    if axes is None:\n",
    "        fig, axes = plt.subplots(3, 2, figsize=(10, 15))\n",
    "\n",
    "    axes = axes.reshape(-1)\n",
    "    fig.subplots_adjust(hspace=0.5, wspace=0.5)\n",
    "    fig = fig.delaxes(axes[-1])\n",
    "    \n",
    "    axes[0].set_title(title)\n",
    "    if ylim is not None:\n",
    "        axes[0].set_ylim(*ylim)\n",
    "    axes[0].set_xlabel(\"Training examples\")\n",
    "    axes[0].set_ylabel(\"Score\")\n",
    "\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    fit_times_mean = np.mean(fit_times, axis=1)\n",
    "    fit_times_std = np.std(fit_times, axis=1)\n",
    "    score_times_mean = np.mean(score_times, axis=1)\n",
    "    score_times_std = np.std(score_times, axis=1)\n",
    "\n",
    "    # Plot learning curve\n",
    "    axes[0].grid()\n",
    "    axes[0].fill_between(\n",
    "        train_sizes,\n",
    "        train_scores_mean - train_scores_std,\n",
    "        train_scores_mean + train_scores_std,\n",
    "        alpha=0.1,\n",
    "        color=\"r\",\n",
    "    )\n",
    "    axes[0].fill_between(\n",
    "        train_sizes,\n",
    "        test_scores_mean - test_scores_std,\n",
    "        test_scores_mean + test_scores_std,\n",
    "        alpha=0.1,\n",
    "        color=\"g\",\n",
    "    )\n",
    "    axes[0].plot(\n",
    "        train_sizes, train_scores_mean, \"o-\", color=\"r\", label=\"Training score\"\n",
    "    )\n",
    "    axes[0].plot(\n",
    "        train_sizes, test_scores_mean, \"o-\", color=\"g\", label=\"Cross-validation score\"\n",
    "    )\n",
    "    axes[0].legend(loc=\"best\")\n",
    "\n",
    "    # Plot n_samples vs fit_times\n",
    "    axes[1].grid()\n",
    "    axes[1].plot(train_sizes, fit_times_mean, \"o-\")\n",
    "    axes[1].fill_between(\n",
    "        train_sizes,\n",
    "        fit_times_mean - fit_times_std,\n",
    "        fit_times_mean + fit_times_std,\n",
    "        alpha=0.1,\n",
    "    )\n",
    "    axes[1].set_xlabel(\"Training examples\")\n",
    "    axes[1].set_ylabel(\"fit_times\")\n",
    "    axes[1].set_title(\"Scalability of the model\")\n",
    "\n",
    "    # Plot fit_time vs score\n",
    "    fit_time_argsort = fit_times_mean.argsort()\n",
    "    fit_time_sorted = fit_times_mean[fit_time_argsort]\n",
    "    test_scores_mean_sorted = test_scores_mean[fit_time_argsort]\n",
    "    test_scores_std_sorted = test_scores_std[fit_time_argsort]\n",
    "    axes[2].grid()\n",
    "    axes[2].plot(fit_time_sorted, test_scores_mean_sorted, \"o-\")\n",
    "    axes[2].fill_between(\n",
    "        fit_time_sorted,\n",
    "        test_scores_mean_sorted - test_scores_std_sorted,\n",
    "        test_scores_mean_sorted + test_scores_std_sorted,\n",
    "        alpha=0.1,\n",
    "    )\n",
    "    axes[2].set_xlabel(\"fit_times\")\n",
    "    axes[2].set_ylabel(\"Score\")\n",
    "    axes[2].set_title(\"Performance of the model\")\n",
    "\n",
    "    # Plot n_samples vs score_times\n",
    "    axes[3].grid()\n",
    "    axes[3].plot(train_sizes, score_times_mean, \"o-\")\n",
    "    axes[3].fill_between(\n",
    "        train_sizes,\n",
    "        score_times_mean - score_times_std,\n",
    "        score_times_mean + score_times_std,\n",
    "        alpha=0.1,\n",
    "    )\n",
    "    axes[3].set_xlabel(\"Training examples\")\n",
    "    axes[3].set_ylabel(\"score_times\")\n",
    "    axes[3].set_title(\"Scalability of the model\")\n",
    "\n",
    "    # Plot score_time vs score\n",
    "    score_time_argsort = score_times_mean.argsort()\n",
    "    score_time_sorted = score_times_mean[score_time_argsort]\n",
    "    test_scores_mean_sorted = test_scores_mean[score_time_argsort]\n",
    "    test_scores_std_sorted = test_scores_std[score_time_argsort]\n",
    "    axes[4].grid()\n",
    "    axes[4].plot(score_time_sorted, test_scores_mean_sorted, \"o-\")\n",
    "    axes[4].fill_between(\n",
    "        score_time_sorted,\n",
    "        test_scores_mean_sorted - test_scores_std_sorted,\n",
    "        test_scores_mean_sorted + test_scores_std_sorted,\n",
    "        alpha=0.1,\n",
    "    )\n",
    "    axes[4].set_xlabel(\"score_times\")\n",
    "    axes[4].set_ylabel(\"Score\")\n",
    "    axes[4].set_title(\"Performance of the model\")\n",
    "\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3  4  5  6  7  8  9 10 11]\n"
     ]
    }
   ],
   "source": [
    "model = DecisionTreeClassifier()\n",
    "#loop in rolling window method through all data\n",
    "unique_years = data['year'].unique()\n",
    "#sort the years\n",
    "unique_years.sort()\n",
    "rolling_window_results = []\n",
    "competition_predictions = []\n",
    "trained_models = list()\n",
    "predited_teams = list()\n",
    "actual_teams = list()\n",
    "\n",
    "\n",
    "\n",
    "print(unique_years)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_columns_to_remove():\n",
    "    return ['playoff']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "toZeroArr = [c for c in data.columns if c not in input_cols]\n",
    "#add playoff to the list of columns to remove\n",
    "toZeroArr.remove('playoff')\n",
    "print(toZeroArr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_training_data(data, unique_years, i):\n",
    "    train_years_before = unique_years[:i]\n",
    "    train_years = unique_years[:i+1]\n",
    "    test_year = unique_years[i]\n",
    "\n",
    "    if i + 1 > len(unique_years): \n",
    "        return\n",
    "\n",
    "    predict_year = unique_years[i+1]  # Predicting for year i+2 \n",
    "\n",
    "    if predict_year < test_year:\n",
    "        return\n",
    "    # print(\"Train years:\", train_years)\n",
    "    # print(\"Predict Year: \", predict_year)\n",
    "    \n",
    "    train_before = data[data['year'].isin(train_years_before)]\n",
    "    train = data[data['year'].isin(train_years)]\n",
    "    test = data[data['year'] == test_year]\n",
    "    actual = data[data['year'] == predict_year]\n",
    "\n",
    "    return train_before, train, test, actual, predict_year, train_years, test_year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_data(train_before, train, test, actual, data, unique_years):\n",
    "\n",
    "    remove_columns = get_columns_to_remove()\n",
    "    actual[toZeroArr] = 0\n",
    "\n",
    "    X_train = train.drop(remove_columns, axis=1)\n",
    "    y_train = train[output_cols]\n",
    "    X_actual = actual.drop(remove_columns, axis=1)\n",
    "    y_actual = actual[output_cols]\n",
    "\n",
    "\n",
    "    return X_train, y_train, X_actual, y_actual\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_and_evaluate_model(X_train, y_train, model, parameters=None):\n",
    "    classifier = model.fit(X_train, y_train)\n",
    "    return classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_and_evaluate(classifier, X_actual, y_actual):\n",
    "    accuracy = 0.0\n",
    "    auc_score = 0.0\n",
    "    # Evaluate the model\n",
    "    predictions = classifier.predict(X_actual)\n",
    "    predictions_prob = classifier.predict_proba(X_actual)[:, 1]\n",
    "\n",
    "    if X_actual['year'].unique()[0]==11:\n",
    "        competition_predictions.append(predictions)\n",
    "    else:\n",
    "        accuracy = accuracy_score(y_actual, predictions)\n",
    "        #auc_score = roc_auc_score(y_actual, predictions_prob, multi_class='ovr')\n",
    "\n",
    "    return accuracy, auc_score, predictions, predictions_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_team_prediction(X_actual, predicted, actual):\n",
    "    #make a dictionary of the player and their predicted\n",
    "    player_prediction = dict(zip(X_actual['playerID'], predicted))\n",
    "    #get the unique teams\n",
    "    teams = X_actual['tmID'].unique()\n",
    "    #make a dictionary of the team and the prediction\n",
    "    team_prediction = dict()\n",
    "    for team in teams:\n",
    "        team_prediction[team] = 0\n",
    "    #loop through the player predictions and make the mean for the team\n",
    "    for player in player_prediction:\n",
    "        team_prediction[X_actual[X_actual['playerID']==player]['tmID'].values[0]] += player_prediction[player]\n",
    "    #divide by the number of players on the team\n",
    "    for team in team_prediction:\n",
    "        team_prediction[team] = team_prediction[team]/len(X_actual[X_actual['tmID']==team])\n",
    "    #add the actual team\n",
    "    teams_actual = dict()\n",
    "    for team in teams:\n",
    "        #the index is the same in actual and predicted, so get the playoff from actual (its just one column) and the tmID from X_actual\n",
    "        teams_actual[team] = actual[X_actual['tmID']==team]\n",
    "    #return the dictionary\n",
    "    return team_prediction, teams_actual\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_team_prediction(team_prediction, teams_actual):\n",
    "    #4 teams from each conference make the playoffs, get the best 4 teams from each conference (get from teams_conf)\n",
    "    west = [k for k, v in sorted(team_prediction.items(), key=lambda item: item[1], reverse=True) if teams_conf[k]==701][:4]\n",
    "    east = [k for k, v in sorted(team_prediction.items(), key=lambda item: item[1], reverse=True) if teams_conf[k]==702][:4]\n",
    "    #get the actual teams\n",
    "    west_actual = [k for k, v in sorted(teams_actual.items(), key=lambda item: item[1], reverse=True) if teams_conf[k]==701][:4]\n",
    "    east_actual = [k for k, v in sorted(teams_actual.items(), key=lambda item: item[1], reverse=True) if teams_conf[k]==702][:4]\n",
    "\n",
    "    return west, east, west_actual, east_actual\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rolling_window_method(i, model, data, unique_years):\n",
    "    train_before, train, test, actual, predict_year, train_years, test_year = prepare_training_data(data, unique_years, i)\n",
    "    \n",
    "    if train_before is None:\n",
    "        return None\n",
    "\n",
    "    X_train, y_train, X_actual, y_actual= process_data(train_before, train, test, actual, data, unique_years)\n",
    "\n",
    "    #print years in X_train and X_actual\n",
    "    #create a csv file for each year, storing the data for that year and that model, in the folder output/rolling_window\n",
    "    #X_train.to_csv(f'output/rolling_window/{model.__class__.__name__}_{test_year}_train.csv', index=False)\n",
    "    #X_test.to_csv(f'output/rolling_window/{model.__class__.__name__}_{test_year}_test.csv', index=False)\n",
    "    #X_actual.to_csv(f'output/rolling_window/{model.__class__.__name__}_{test_year}_actual.csv', index=False)\n",
    "    #y_actual.to_csv(f'output/rolling_window/{model.__class__.__name__}_{test_year}_y_actual.csv', index=False)\n",
    "\n",
    "\n",
    "    classifier = train_and_evaluate_model(X_train, y_train, model)\n",
    "    # Calculate accuracy\n",
    "\n",
    "    accuracy, auc_score, predictions, predictions_prob = predict_and_evaluate(classifier, X_actual, y_actual)\n",
    "\n",
    "    # teams_prediction, teams_actual = get_team_prediction(X_actual, predictions, y_actual)\n",
    "\n",
    "    # print(\"Predicted Teams: \", teams_prediction)\n",
    "    # print(\"Actual Teams: \", teams_actual)\n",
    "\n",
    "    # west, east, west_actual, east_actual= make_team_prediction(teams_prediction, teams_actual)\n",
    "\n",
    "    # print(\"Predicted Teams: \", west, east)\n",
    "    # print(\"Actual Teams: \", west_actual, east_actual)\n",
    "\n",
    "\n",
    "\n",
    "    return {\n",
    "        'train_years': train_years,\n",
    "        'test_year': test_year,\n",
    "        'Year': predict_year,\n",
    "        'accuracy': accuracy,\n",
    "        'auc': auc_score,\n",
    "        'model': classifier,\n",
    "        'predictions': predictions,\n",
    "        'predictions_prob': predictions_prob,\n",
    "        'actual_results': y_actual,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def model_training_loop(model, data, unique_years):\n",
    "    rolling_window_results = []\n",
    "    for i in range(len(unique_years) - 1):\n",
    "        result= rolling_window_method(i, model, data, unique_years)\n",
    "        if result:\n",
    "            rolling_window_results.append(result)\n",
    "    return rolling_window_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = DecisionTreeClassifier()\n",
    "# rolling_window_results += model_training_loop(model, data, unique_years)\n",
    "# trained_models.append(rolling_window_results[-1]['model'])  # Get the last trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier()\n",
    "rolling_window_results += model_training_loop(model, data, unique_years)\n",
    "trained_models.append(rolling_window_results[-1]['model'])  # Get the last trained mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GradientBoostingClassifier()\n",
    "rolling_window_results += model_training_loop(model, data, unique_years)\n",
    "trained_models.append(rolling_window_results[-1]['model'])  # Get the last trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gaussian naive bayes\n",
    "model = GaussianNB()\n",
    "rolling_window_results += model_training_loop(model, data, unique_years)\n",
    "trained_models.append(rolling_window_results[-1]['model'])  # Get the last trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logistic regression\n",
    "model = LogisticRegression()\n",
    "rolling_window_results += model_training_loop(model, data, unique_years)\n",
    "trained_models.append(rolling_window_results[-1]['model'])  # Get the last trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ada boost\n",
    "model = AdaBoostClassifier()\n",
    "rolling_window_results += model_training_loop(model, data, unique_years)\n",
    "trained_models.append(rolling_window_results[-1]['model'])  # Get the last trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [],
   "source": [
    "level0 = list()\n",
    "#append models in trained_models to level0\n",
    "for i in range(len(trained_models)):\n",
    "    level0.append((str(i), trained_models[i]))\n",
    "\n",
    "level1 = RandomForestClassifier(n_estimators=100, min_samples_split=2, min_samples_leaf=2, max_features='sqrt', max_depth=9, bootstrap=True, criterion='entropy', random_state=20)\n",
    "clf = StackingClassifier(estimators=level0, final_estimator=level1, cv=3)\n",
    "\n",
    "rolling_window_results += model_training_loop(clf, data, unique_years)\n",
    "trained_models.append(rolling_window_results[-1]['model'])  # Get the last trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {\n",
    "#         'train_years': train_years,\n",
    "#         'test_year': test_year,\n",
    "#         'Year': predict_year,\n",
    "#         'accuracy': accuracy,\n",
    "#         'auc': auc_score,\n",
    "#         'model': classifier,\n",
    "#         'predictions': predictions,\n",
    "#         'predictions_prob': predictions_prob,\n",
    "#         'actual_results': y_actual,\n",
    "# }\n",
    "\n",
    "#make a dictionary of the model: [accuracy]\n",
    "model_accuracy = dict()\n",
    "for result in rolling_window_results:\n",
    "    model_name = result['model'].__class__.__name__\n",
    "    if model_name not in model_accuracy:\n",
    "        model_accuracy[model_name] = []\n",
    "    model_accuracy[model_name].append(result['accuracy'])\n",
    "\n",
    "print(model_accuracy)\n",
    "\n",
    "#plot a line graph of the accuracy of each model\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.title(\"Accuracy of each model\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "for model_name in model_accuracy:\n",
    "    plt.plot(model_accuracy[model_name], label=model_name)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

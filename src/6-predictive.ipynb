{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline \n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, RocCurveDisplay, make_scorer\n",
    "from sklearn.model_selection import learning_curve\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "import datetime\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_transformed = pd.read_csv('new_data/clean-data_without_outliers.csv')\n",
    "#competition_transformed = pd.read_csv('new_data/clean-comp.csv')\n",
    "#data = pd.read_csv('new_data/clean-data_without_outliers.csv')\n",
    "competition = pd.read_csv('new_data/clean-comp.csv')\n",
    "\n",
    "data = pd.read_csv('new_data/complete-data.csv')\n",
    "#data_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>playerID</th>\n",
       "      <th>year</th>\n",
       "      <th>stint</th>\n",
       "      <th>tmID</th>\n",
       "      <th>GP</th>\n",
       "      <th>oRebounds</th>\n",
       "      <th>dRebounds</th>\n",
       "      <th>dq</th>\n",
       "      <th>PostMinutes</th>\n",
       "      <th>PostPoints</th>\n",
       "      <th>PostoRebounds</th>\n",
       "      <th>PostdRebounds</th>\n",
       "      <th>PostRebounds</th>\n",
       "      <th>PostAssists</th>\n",
       "      <th>PostSteals</th>\n",
       "      <th>PostBlocks</th>\n",
       "      <th>PostTurnovers</th>\n",
       "      <th>PostPF</th>\n",
       "      <th>PostDQ</th>\n",
       "      <th>ft%</th>\n",
       "      <th>fg%</th>\n",
       "      <th>three%</th>\n",
       "      <th>gs%</th>\n",
       "      <th>Postft%</th>\n",
       "      <th>Postfg%</th>\n",
       "      <th>Postthree%</th>\n",
       "      <th>Postgs%</th>\n",
       "      <th>efg%</th>\n",
       "      <th>ts%</th>\n",
       "      <th>ppg</th>\n",
       "      <th>rpg</th>\n",
       "      <th>apg</th>\n",
       "      <th>spg</th>\n",
       "      <th>bpg</th>\n",
       "      <th>eff</th>\n",
       "      <th>pp36</th>\n",
       "      <th>defensive_prowess</th>\n",
       "      <th>defensive_discipline</th>\n",
       "      <th>mpg</th>\n",
       "      <th>pos</th>\n",
       "      <th>college</th>\n",
       "      <th>playoff</th>\n",
       "      <th>confID</th>\n",
       "      <th>playoff_progression</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>award_count</th>\n",
       "      <th>career_year</th>\n",
       "      <th>playoff_progression_rolling</th>\n",
       "      <th>playoff_rolling</th>\n",
       "      <th>pp36_rolling</th>\n",
       "      <th>eff_rolling</th>\n",
       "      <th>award_count_rolling</th>\n",
       "      <th>defensive_prowess_rolling</th>\n",
       "      <th>defensive_discipline_rolling</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>191</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>569</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.90</td>\n",
       "      <td>2.77</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-28.38</td>\n",
       "      <td>10.44</td>\n",
       "      <td>11.5</td>\n",
       "      <td>2.76</td>\n",
       "      <td>9.46</td>\n",
       "      <td>2</td>\n",
       "      <td>615</td>\n",
       "      <td>0.0</td>\n",
       "      <td>702</td>\n",
       "      <td>0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>180</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.90</td>\n",
       "      <td>-86.415</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.90</td>\n",
       "      <td>5.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>720</th>\n",
       "      <td>393</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>567</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1.18</td>\n",
       "      <td>1.96</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.67</td>\n",
       "      <td>0.33</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.80</td>\n",
       "      <td>2.68</td>\n",
       "      <td>3.14</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.50</td>\n",
       "      <td>-82.72</td>\n",
       "      <td>6.48</td>\n",
       "      <td>28.6</td>\n",
       "      <td>5.64</td>\n",
       "      <td>14.86</td>\n",
       "      <td>2</td>\n",
       "      <td>612</td>\n",
       "      <td>1.0</td>\n",
       "      <td>702</td>\n",
       "      <td>1</td>\n",
       "      <td>76.0</td>\n",
       "      <td>174</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.04</td>\n",
       "      <td>-44.760</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.00</td>\n",
       "      <td>6.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>136</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>573</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.40</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.80</td>\n",
       "      <td>1.02</td>\n",
       "      <td>9.40</td>\n",
       "      <td>1.90</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-63.30</td>\n",
       "      <td>16.20</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10.40</td>\n",
       "      <td>20.80</td>\n",
       "      <td>1</td>\n",
       "      <td>588</td>\n",
       "      <td>0.0</td>\n",
       "      <td>701</td>\n",
       "      <td>0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>160</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.52</td>\n",
       "      <td>-181.040</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.75</td>\n",
       "      <td>8.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>133</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>563</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.66</td>\n",
       "      <td>1.04</td>\n",
       "      <td>1.33</td>\n",
       "      <td>0.67</td>\n",
       "      <td>1.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-1.34</td>\n",
       "      <td>11.16</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.34</td>\n",
       "      <td>4.33</td>\n",
       "      <td>1</td>\n",
       "      <td>578</td>\n",
       "      <td>0.0</td>\n",
       "      <td>701</td>\n",
       "      <td>0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>13.68</td>\n",
       "      <td>-68.565</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.85</td>\n",
       "      <td>5.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>615</th>\n",
       "      <td>343</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>561</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1.08</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.66</td>\n",
       "      <td>1.42</td>\n",
       "      <td>1.21</td>\n",
       "      <td>3.04</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-65.74</td>\n",
       "      <td>2.88</td>\n",
       "      <td>16.7</td>\n",
       "      <td>6.84</td>\n",
       "      <td>17.04</td>\n",
       "      <td>1</td>\n",
       "      <td>658</td>\n",
       "      <td>0.0</td>\n",
       "      <td>701</td>\n",
       "      <td>0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>147</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.40</td>\n",
       "      <td>-67.345</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.00</td>\n",
       "      <td>6.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>231</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>745</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>615</td>\n",
       "      <td>0.0</td>\n",
       "      <td>701</td>\n",
       "      <td>0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>185</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>14.04</td>\n",
       "      <td>-133.615</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42.50</td>\n",
       "      <td>6.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>835</th>\n",
       "      <td>442</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>562</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>588</td>\n",
       "      <td>0.0</td>\n",
       "      <td>702</td>\n",
       "      <td>0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>183</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>12.24</td>\n",
       "      <td>-82.975</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.65</td>\n",
       "      <td>3.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>111</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>559</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>598</td>\n",
       "      <td>0.0</td>\n",
       "      <td>702</td>\n",
       "      <td>0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>177</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>14.22</td>\n",
       "      <td>-248.860</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44.40</td>\n",
       "      <td>10.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>218</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>745</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>649</td>\n",
       "      <td>0.0</td>\n",
       "      <td>701</td>\n",
       "      <td>0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>168</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>12.06</td>\n",
       "      <td>-120.655</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.60</td>\n",
       "      <td>5.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>58</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>562</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4</td>\n",
       "      <td>597</td>\n",
       "      <td>0.0</td>\n",
       "      <td>702</td>\n",
       "      <td>0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>210</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.00</td>\n",
       "      <td>-183.630</td>\n",
       "      <td>0.0</td>\n",
       "      <td>48.60</td>\n",
       "      <td>8.82</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1029 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     playerID  year  stint  tmID    GP  oRebounds  dRebounds    dq  \\\n",
       "300       191     2      2   569  13.0       0.38       0.85  0.00   \n",
       "720       393     2      0   567  28.0       1.18       1.96  0.00   \n",
       "223       136     2      1   573  10.0       0.50       1.40  0.10   \n",
       "218       133     2      0   563   3.0       0.00       0.67  0.00   \n",
       "615       343     3      0   561  24.0       0.12       1.08  0.04   \n",
       "..        ...   ...    ...   ...   ...        ...        ...   ...   \n",
       "390       231    11      0   745   0.0       0.00       0.00  0.00   \n",
       "835       442    11      0   562   0.0       0.00       0.00  0.00   \n",
       "169       111    11      0   559   0.0       0.00       0.00  0.00   \n",
       "356       218    11      0   745   0.0       0.00       0.00  0.00   \n",
       "77         58    11      0   562   0.0       0.00       0.00  0.00   \n",
       "\n",
       "     PostMinutes  PostPoints  PostoRebounds  PostdRebounds  PostRebounds  \\\n",
       "300         0.00        0.00            0.0            0.0           0.0   \n",
       "720         7.67        0.33            1.0            1.0           2.0   \n",
       "223         0.00        0.00            0.0            0.0           0.0   \n",
       "218         0.00        0.00            0.0            0.0           0.0   \n",
       "615         0.00        0.00            0.0            0.0           0.0   \n",
       "..           ...         ...            ...            ...           ...   \n",
       "390         0.00        0.00            0.0            0.0           0.0   \n",
       "835         0.00        0.00            0.0            0.0           0.0   \n",
       "169         0.00        0.00            0.0            0.0           0.0   \n",
       "356         0.00        0.00            0.0            0.0           0.0   \n",
       "77          0.00        0.00            0.0            0.0           0.0   \n",
       "\n",
       "     PostAssists  PostSteals  PostBlocks  PostTurnovers  PostPF  PostDQ   ft%  \\\n",
       "300          0.0        0.00         0.0            0.0    0.00     0.0  0.57   \n",
       "720          0.0        0.33         0.0            2.0    0.33     0.0  0.75   \n",
       "223          0.0        0.00         0.0            0.0    0.00     0.0  0.79   \n",
       "218          0.0        0.00         0.0            0.0    0.00     0.0  1.00   \n",
       "615          0.0        0.00         0.0            0.0    0.00     0.0  0.44   \n",
       "..           ...         ...         ...            ...     ...     ...   ...   \n",
       "390          0.0        0.00         0.0            0.0    0.00     0.0  0.00   \n",
       "835          0.0        0.00         0.0            0.0    0.00     0.0  0.00   \n",
       "169          0.0        0.00         0.0            0.0    0.00     0.0  0.00   \n",
       "356          0.0        0.00         0.0            0.0    0.00     0.0  0.00   \n",
       "77           0.0        0.00         0.0            0.0    0.00     0.0  0.00   \n",
       "\n",
       "      fg%  three%   gs%  Postft%  Postfg%  Postthree%  Postgs%  efg%   ts%  \\\n",
       "300  0.41    0.00  0.00      0.0      0.0         0.0      0.0  0.82  0.90   \n",
       "720  0.36    0.25  0.11      0.5      0.0         0.0      0.0  0.74  0.80   \n",
       "223  0.37    0.36  0.90      0.0      0.0         0.0      0.0  0.80  1.02   \n",
       "218  0.33    0.00  0.00      0.0      0.0         0.0      0.0  0.66  1.04   \n",
       "615  0.27    0.21  0.33      0.0      0.0         0.0      0.0  0.62  0.66   \n",
       "..    ...     ...   ...      ...      ...         ...      ...   ...   ...   \n",
       "390  0.00    0.00  0.00      0.0      0.0         0.0      0.0  0.00  0.00   \n",
       "835  0.00    0.00  0.00      0.0      0.0         0.0      0.0  0.00  0.00   \n",
       "169  0.00    0.00  0.00      0.0      0.0         0.0      0.0  0.00  0.00   \n",
       "356  0.00    0.00  0.00      0.0      0.0         0.0      0.0  0.00  0.00   \n",
       "77   0.00    0.00  0.00      0.0      0.0         0.0      0.0  0.00  0.00   \n",
       "\n",
       "      ppg   rpg   apg   spg   bpg    eff   pp36  defensive_prowess  \\\n",
       "300  2.77  1.23  0.31  0.31  0.00 -28.38  10.44               11.5   \n",
       "720  2.68  3.14  0.57  0.39  0.50 -82.72   6.48               28.6   \n",
       "223  9.40  1.90  1.80  0.50  0.10 -63.30  16.20               20.0   \n",
       "218  1.33  0.67  1.33  0.33  0.00  -1.34  11.16               10.0   \n",
       "615  1.42  1.21  3.04  0.42  0.17 -65.74   2.88               16.7   \n",
       "..    ...   ...   ...   ...   ...    ...    ...                ...   \n",
       "390  0.00  0.00  0.00  0.00  0.00   0.00   0.00                0.0   \n",
       "835  0.00  0.00  0.00  0.00  0.00   0.00   0.00                0.0   \n",
       "169  0.00  0.00  0.00  0.00  0.00   0.00   0.00                0.0   \n",
       "356  0.00  0.00  0.00  0.00  0.00   0.00   0.00                0.0   \n",
       "77   0.00  0.00  0.00  0.00  0.00   0.00   0.00                0.0   \n",
       "\n",
       "     defensive_discipline    mpg  pos  college  playoff  confID  \\\n",
       "300                  2.76   9.46    2      615      0.0     702   \n",
       "720                  5.64  14.86    2      612      1.0     702   \n",
       "223                 10.40  20.80    1      588      0.0     701   \n",
       "218                  3.34   4.33    1      578      0.0     701   \n",
       "615                  6.84  17.04    1      658      0.0     701   \n",
       "..                    ...    ...  ...      ...      ...     ...   \n",
       "390                  0.00   0.00    2      615      0.0     701   \n",
       "835                  0.00   0.00    1      588      0.0     702   \n",
       "169                  0.00   0.00    2      598      0.0     702   \n",
       "356                  0.00   0.00    2      649      0.0     701   \n",
       "77                   0.00   0.00    4      597      0.0     702   \n",
       "\n",
       "     playoff_progression  height  weight  award_count  career_year  \\\n",
       "300                    0    73.0     180          0.0            3   \n",
       "720                    1    76.0     174          0.0            3   \n",
       "223                    0    70.0     160          0.0            3   \n",
       "218                    0    69.0     150          0.0            3   \n",
       "615                    0    67.0     147          0.0            3   \n",
       "..                   ...     ...     ...          ...          ...   \n",
       "390                    0    75.0     185          0.0            4   \n",
       "835                    0    75.0     183          0.0            4   \n",
       "169                    0    72.0     177          0.0            6   \n",
       "356                    0    72.0     168          0.0            3   \n",
       "77                     0    78.0     210          0.0            6   \n",
       "\n",
       "     playoff_progression_rolling  playoff_rolling  pp36_rolling  eff_rolling  \\\n",
       "300                          1.5              1.0          9.90      -86.415   \n",
       "720                          0.0              0.0          5.04      -44.760   \n",
       "223                          2.0              1.0         11.52     -181.040   \n",
       "218                          1.0              0.5         13.68      -68.565   \n",
       "615                          1.5              1.0          5.40      -67.345   \n",
       "..                           ...              ...           ...          ...   \n",
       "390                          1.0              0.5         14.04     -133.615   \n",
       "835                          1.0              0.5         12.24      -82.975   \n",
       "169                          0.5              0.5         14.22     -248.860   \n",
       "356                          0.5              0.5         12.06     -120.655   \n",
       "77                           3.0              1.0         18.00     -183.630   \n",
       "\n",
       "     award_count_rolling  defensive_prowess_rolling  \\\n",
       "300                  0.0                      24.90   \n",
       "720                  0.0                      28.00   \n",
       "223                  0.0                      27.75   \n",
       "218                  0.0                      14.85   \n",
       "615                  0.0                      20.00   \n",
       "..                   ...                        ...   \n",
       "390                  0.0                      42.50   \n",
       "835                  0.0                      13.65   \n",
       "169                  0.0                      44.40   \n",
       "356                  0.0                      28.60   \n",
       "77                   0.0                      48.60   \n",
       "\n",
       "     defensive_discipline_rolling  \n",
       "300                          5.02  \n",
       "720                          6.04  \n",
       "223                          8.52  \n",
       "218                          5.68  \n",
       "615                          6.87  \n",
       "..                            ...  \n",
       "390                          6.64  \n",
       "835                          3.15  \n",
       "169                         10.24  \n",
       "356                          5.18  \n",
       "77                           8.82  \n",
       "\n",
       "[1029 rows x 55 columns]"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sort_values('year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['playerID', 'year', 'stint', 'tmID', 'height', 'weight', 'pos',\n",
       "       'college', 'confID'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "competition.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['playerID', 'year', 'stint', 'tmID', 'pos', 'college', 'height', 'weight', 'career_year', 'confID', 'playoff_progression_rolling', 'playoff_rolling', 'pp36_rolling', 'eff_rolling', 'award_count_rolling', 'defensive_prowess_rolling', 'defensive_discipline_rolling']\n"
     ]
    }
   ],
   "source": [
    "input_cols = ['playerID', 'year','stint', 'tmID', 'pos', 'college', 'height', 'weight', 'career_year', 'confID']\n",
    "\n",
    "#add all columns ending in rolling from data to input cols\n",
    "input_cols+=[c for c in data.columns if c.endswith(\"_rolling\")]\n",
    "\n",
    "# The output columns are the genres\n",
    "output_cols = 'playoff'\n",
    "\n",
    "known_columns = ['playerID', 'year', 'stint', 'tmID', 'height', 'weight', 'pos','college', 'confID']\n",
    "\n",
    "# Averages to calculate for precision, recall, and f1-score\n",
    "averages = [None, \"macro\", \"weighted\", \"micro\", \"samples\"]\n",
    "\n",
    "print(input_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>playerID</th>\n",
       "      <th>year</th>\n",
       "      <th>stint</th>\n",
       "      <th>tmID</th>\n",
       "      <th>GP</th>\n",
       "      <th>oRebounds</th>\n",
       "      <th>dRebounds</th>\n",
       "      <th>dq</th>\n",
       "      <th>PostMinutes</th>\n",
       "      <th>PostPoints</th>\n",
       "      <th>PostoRebounds</th>\n",
       "      <th>PostdRebounds</th>\n",
       "      <th>PostRebounds</th>\n",
       "      <th>PostAssists</th>\n",
       "      <th>PostSteals</th>\n",
       "      <th>PostBlocks</th>\n",
       "      <th>PostTurnovers</th>\n",
       "      <th>PostPF</th>\n",
       "      <th>PostDQ</th>\n",
       "      <th>ft%</th>\n",
       "      <th>fg%</th>\n",
       "      <th>three%</th>\n",
       "      <th>gs%</th>\n",
       "      <th>Postft%</th>\n",
       "      <th>Postfg%</th>\n",
       "      <th>Postthree%</th>\n",
       "      <th>Postgs%</th>\n",
       "      <th>efg%</th>\n",
       "      <th>ts%</th>\n",
       "      <th>ppg</th>\n",
       "      <th>rpg</th>\n",
       "      <th>apg</th>\n",
       "      <th>spg</th>\n",
       "      <th>bpg</th>\n",
       "      <th>eff</th>\n",
       "      <th>pp36</th>\n",
       "      <th>defensive_prowess</th>\n",
       "      <th>defensive_discipline</th>\n",
       "      <th>mpg</th>\n",
       "      <th>pos</th>\n",
       "      <th>college</th>\n",
       "      <th>playoff</th>\n",
       "      <th>confID</th>\n",
       "      <th>playoff_progression</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>award_count</th>\n",
       "      <th>career_year</th>\n",
       "      <th>playoff_progression_rolling</th>\n",
       "      <th>playoff_rolling</th>\n",
       "      <th>pp36_rolling</th>\n",
       "      <th>eff_rolling</th>\n",
       "      <th>award_count_rolling</th>\n",
       "      <th>defensive_prowess_rolling</th>\n",
       "      <th>defensive_discipline_rolling</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>555</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.47</td>\n",
       "      <td>3.23</td>\n",
       "      <td>0.00</td>\n",
       "      <td>23.0</td>\n",
       "      <td>7.67</td>\n",
       "      <td>0.33</td>\n",
       "      <td>1.33</td>\n",
       "      <td>1.67</td>\n",
       "      <td>1.33</td>\n",
       "      <td>1.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>2.67</td>\n",
       "      <td>2.67</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.83</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.96</td>\n",
       "      <td>10.60</td>\n",
       "      <td>4.70</td>\n",
       "      <td>2.73</td>\n",
       "      <td>1.47</td>\n",
       "      <td>0.37</td>\n",
       "      <td>-272.13</td>\n",
       "      <td>14.40</td>\n",
       "      <td>50.7</td>\n",
       "      <td>11.26</td>\n",
       "      <td>26.40</td>\n",
       "      <td>2</td>\n",
       "      <td>575</td>\n",
       "      <td>1.0</td>\n",
       "      <td>701</td>\n",
       "      <td>1</td>\n",
       "      <td>74.0</td>\n",
       "      <td>169</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.40</td>\n",
       "      <td>-301.960</td>\n",
       "      <td>0.0</td>\n",
       "      <td>63.35</td>\n",
       "      <td>12.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>555</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.77</td>\n",
       "      <td>2.59</td>\n",
       "      <td>0.00</td>\n",
       "      <td>33.5</td>\n",
       "      <td>10.00</td>\n",
       "      <td>1.50</td>\n",
       "      <td>3.00</td>\n",
       "      <td>4.50</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.50</td>\n",
       "      <td>3.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.92</td>\n",
       "      <td>6.64</td>\n",
       "      <td>3.36</td>\n",
       "      <td>2.05</td>\n",
       "      <td>1.36</td>\n",
       "      <td>0.09</td>\n",
       "      <td>-137.50</td>\n",
       "      <td>11.52</td>\n",
       "      <td>40.5</td>\n",
       "      <td>7.72</td>\n",
       "      <td>21.00</td>\n",
       "      <td>2</td>\n",
       "      <td>575</td>\n",
       "      <td>1.0</td>\n",
       "      <td>701</td>\n",
       "      <td>1</td>\n",
       "      <td>74.0</td>\n",
       "      <td>169</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>14.22</td>\n",
       "      <td>-299.970</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53.70</td>\n",
       "      <td>11.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>555</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.94</td>\n",
       "      <td>2.52</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.98</td>\n",
       "      <td>9.81</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1.94</td>\n",
       "      <td>1.55</td>\n",
       "      <td>0.19</td>\n",
       "      <td>-250.06</td>\n",
       "      <td>14.04</td>\n",
       "      <td>42.6</td>\n",
       "      <td>10.70</td>\n",
       "      <td>25.06</td>\n",
       "      <td>2</td>\n",
       "      <td>575</td>\n",
       "      <td>0.0</td>\n",
       "      <td>701</td>\n",
       "      <td>0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>169</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.96</td>\n",
       "      <td>-204.815</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.60</td>\n",
       "      <td>9.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>555</td>\n",
       "      <td>34.0</td>\n",
       "      <td>1.29</td>\n",
       "      <td>1.82</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.98</td>\n",
       "      <td>7.74</td>\n",
       "      <td>3.12</td>\n",
       "      <td>1.59</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-210.52</td>\n",
       "      <td>12.96</td>\n",
       "      <td>28.5</td>\n",
       "      <td>7.48</td>\n",
       "      <td>21.29</td>\n",
       "      <td>2</td>\n",
       "      <td>575</td>\n",
       "      <td>0.0</td>\n",
       "      <td>701</td>\n",
       "      <td>0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>169</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>12.78</td>\n",
       "      <td>-193.780</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41.55</td>\n",
       "      <td>9.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>555</td>\n",
       "      <td>34.0</td>\n",
       "      <td>1.56</td>\n",
       "      <td>2.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.04</td>\n",
       "      <td>1.08</td>\n",
       "      <td>10.15</td>\n",
       "      <td>4.41</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1.29</td>\n",
       "      <td>0.12</td>\n",
       "      <td>-222.53</td>\n",
       "      <td>14.76</td>\n",
       "      <td>42.6</td>\n",
       "      <td>9.06</td>\n",
       "      <td>24.79</td>\n",
       "      <td>2</td>\n",
       "      <td>575</td>\n",
       "      <td>0.0</td>\n",
       "      <td>701</td>\n",
       "      <td>0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>169</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.50</td>\n",
       "      <td>-230.290</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.55</td>\n",
       "      <td>9.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1024</th>\n",
       "      <td>548</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>564</td>\n",
       "      <td>33.0</td>\n",
       "      <td>2.09</td>\n",
       "      <td>4.42</td>\n",
       "      <td>0.03</td>\n",
       "      <td>32.0</td>\n",
       "      <td>19.33</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.33</td>\n",
       "      <td>5.33</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.67</td>\n",
       "      <td>1.67</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.31</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.94</td>\n",
       "      <td>1.06</td>\n",
       "      <td>18.18</td>\n",
       "      <td>6.52</td>\n",
       "      <td>1.64</td>\n",
       "      <td>1.33</td>\n",
       "      <td>0.48</td>\n",
       "      <td>-340.85</td>\n",
       "      <td>19.44</td>\n",
       "      <td>62.4</td>\n",
       "      <td>7.82</td>\n",
       "      <td>33.73</td>\n",
       "      <td>2</td>\n",
       "      <td>634</td>\n",
       "      <td>1.0</td>\n",
       "      <td>701</td>\n",
       "      <td>1</td>\n",
       "      <td>73.0</td>\n",
       "      <td>165</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.90</td>\n",
       "      <td>-305.255</td>\n",
       "      <td>0.0</td>\n",
       "      <td>58.50</td>\n",
       "      <td>9.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1025</th>\n",
       "      <td>548</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>564</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>634</td>\n",
       "      <td>0.0</td>\n",
       "      <td>701</td>\n",
       "      <td>0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>165</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.62</td>\n",
       "      <td>-320.180</td>\n",
       "      <td>0.0</td>\n",
       "      <td>62.10</td>\n",
       "      <td>8.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1026</th>\n",
       "      <td>549</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>574</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.92</td>\n",
       "      <td>5.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.40</td>\n",
       "      <td>-33.20</td>\n",
       "      <td>14.04</td>\n",
       "      <td>31.0</td>\n",
       "      <td>5.80</td>\n",
       "      <td>12.90</td>\n",
       "      <td>2</td>\n",
       "      <td>700</td>\n",
       "      <td>0.0</td>\n",
       "      <td>702</td>\n",
       "      <td>0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>166</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>13.14</td>\n",
       "      <td>-131.490</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.25</td>\n",
       "      <td>6.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027</th>\n",
       "      <td>549</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>574</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>700</td>\n",
       "      <td>0.0</td>\n",
       "      <td>702</td>\n",
       "      <td>0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>166</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>14.40</td>\n",
       "      <td>-30.555</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.30</td>\n",
       "      <td>4.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1028</th>\n",
       "      <td>550</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>573</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.78</td>\n",
       "      <td>1.20</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-8.20</td>\n",
       "      <td>5.76</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.20</td>\n",
       "      <td>7.40</td>\n",
       "      <td>3</td>\n",
       "      <td>582</td>\n",
       "      <td>0.0</td>\n",
       "      <td>701</td>\n",
       "      <td>0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>174</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.72</td>\n",
       "      <td>-24.625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.30</td>\n",
       "      <td>3.78</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1029 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      playerID  year  stint  tmID    GP  oRebounds  dRebounds    dq  \\\n",
       "0            0     4      0   555  30.0       1.47       3.23  0.00   \n",
       "1            0     5      0   555  22.0       0.77       2.59  0.00   \n",
       "2            0     6      0   555  31.0       0.94       2.52  0.00   \n",
       "3            0     7      0   555  34.0       1.29       1.82  0.00   \n",
       "4            0     8      0   555  34.0       1.56       2.85  0.00   \n",
       "...        ...   ...    ...   ...   ...        ...        ...   ...   \n",
       "1024       548    10      0   564  33.0       2.09       4.42  0.03   \n",
       "1025       548    11      0   564   0.0       0.00       0.00  0.00   \n",
       "1026       549    10      2   574  10.0       1.00       2.00  0.00   \n",
       "1027       549    11      0   574   0.0       0.00       0.00  0.00   \n",
       "1028       550     3      2   573   5.0       0.00       0.60  0.00   \n",
       "\n",
       "      PostMinutes  PostPoints  PostoRebounds  PostdRebounds  PostRebounds  \\\n",
       "0            23.0        7.67           0.33           1.33          1.67   \n",
       "1            33.5       10.00           1.50           3.00          4.50   \n",
       "2             0.0        0.00           0.00           0.00          0.00   \n",
       "3             0.0        0.00           0.00           0.00          0.00   \n",
       "4             0.0        0.00           0.00           0.00          0.00   \n",
       "...           ...         ...            ...            ...           ...   \n",
       "1024         32.0       19.33           2.00           3.33          5.33   \n",
       "1025          0.0        0.00           0.00           0.00          0.00   \n",
       "1026          0.0        0.00           0.00           0.00          0.00   \n",
       "1027          0.0        0.00           0.00           0.00          0.00   \n",
       "1028          0.0        0.00           0.00           0.00          0.00   \n",
       "\n",
       "      PostAssists  PostSteals  PostBlocks  PostTurnovers  PostPF  PostDQ  \\\n",
       "0            1.33        1.33        0.33           2.67    2.67     0.0   \n",
       "1            1.50        0.50        1.00           1.50    3.50     0.0   \n",
       "2            0.00        0.00        0.00           0.00    0.00     0.0   \n",
       "3            0.00        0.00        0.00           0.00    0.00     0.0   \n",
       "4            0.00        0.00        0.00           0.00    0.00     0.0   \n",
       "...           ...         ...         ...            ...     ...     ...   \n",
       "1024         2.00        2.00        0.67           1.67    3.00     0.0   \n",
       "1025         0.00        0.00        0.00           0.00    0.00     0.0   \n",
       "1026         0.00        0.00        0.00           0.00    0.00     0.0   \n",
       "1027         0.00        0.00        0.00           0.00    0.00     0.0   \n",
       "1028         0.00        0.00        0.00           0.00    0.00     0.0   \n",
       "\n",
       "       ft%   fg%  three%   gs%  Postft%  Postfg%  Postthree%  Postgs%  efg%  \\\n",
       "0     0.70  0.39    0.30  0.83     1.00     0.27        0.43      1.0  0.88   \n",
       "1     0.61  0.35    0.38  0.50     0.50     0.35        0.25      1.0  0.84   \n",
       "2     0.73  0.39    0.40  1.00     0.00     0.00        0.00      0.0  0.90   \n",
       "3     0.66  0.41    0.37  0.06     0.00     0.00        0.00      0.0  0.92   \n",
       "4     0.84  0.44    0.45  0.85     0.00     0.00        0.00      0.0  1.04   \n",
       "...    ...   ...     ...   ...      ...      ...         ...      ...   ...   \n",
       "1024  0.77  0.45    0.31  1.00     0.68     0.46        0.50      1.0  0.94   \n",
       "1025  0.00  0.00    0.00  0.00     0.00     0.00        0.00      0.0  0.00   \n",
       "1026  0.89  0.35    0.25  0.60     0.00     0.00        0.00      0.0  0.74   \n",
       "1027  0.00  0.00    0.00  0.00     0.00     0.00        0.00      0.0  0.00   \n",
       "1028  0.50  0.33    0.00  0.00     0.00     0.00        0.00      0.0  0.66   \n",
       "\n",
       "       ts%    ppg   rpg   apg   spg   bpg     eff   pp36  defensive_prowess  \\\n",
       "0     0.96  10.60  4.70  2.73  1.47  0.37 -272.13  14.40               50.7   \n",
       "1     0.92   6.64  3.36  2.05  1.36  0.09 -137.50  11.52               40.5   \n",
       "2     0.98   9.81  3.45  1.94  1.55  0.19 -250.06  14.04               42.6   \n",
       "3     0.98   7.74  3.12  1.59  1.00  0.03 -210.52  12.96               28.5   \n",
       "4     1.08  10.15  4.41  2.50  1.29  0.12 -222.53  14.76               42.6   \n",
       "...    ...    ...   ...   ...   ...   ...     ...    ...                ...   \n",
       "1024  1.06  18.18  6.52  1.64  1.33  0.48 -340.85  19.44               62.4   \n",
       "1025  0.00   0.00  0.00  0.00  0.00  0.00    0.00   0.00                0.0   \n",
       "1026  0.92   5.00  3.00  0.70  0.70  0.40  -33.20  14.04               31.0   \n",
       "1027  0.00   0.00  0.00  0.00  0.00  0.00    0.00   0.00                0.0   \n",
       "1028  0.78   1.20  0.60  0.00  0.00  0.00   -8.20   5.76                6.0   \n",
       "\n",
       "      defensive_discipline    mpg  pos  college  playoff  confID  \\\n",
       "0                    11.26  26.40    2      575      1.0     701   \n",
       "1                     7.72  21.00    2      575      1.0     701   \n",
       "2                    10.70  25.06    2      575      0.0     701   \n",
       "3                     7.48  21.29    2      575      0.0     701   \n",
       "4                     9.06  24.79    2      575      0.0     701   \n",
       "...                    ...    ...  ...      ...      ...     ...   \n",
       "1024                  7.82  33.73    2      634      1.0     701   \n",
       "1025                  0.00   0.00    2      634      0.0     701   \n",
       "1026                  5.80  12.90    2      700      0.0     702   \n",
       "1027                  0.00   0.00    2      700      0.0     702   \n",
       "1028                  3.20   7.40    3      582      0.0     701   \n",
       "\n",
       "      playoff_progression  height  weight  award_count  career_year  \\\n",
       "0                       1    74.0     169          0.0            3   \n",
       "1                       1    74.0     169          0.0            4   \n",
       "2                       0    74.0     169          0.0            5   \n",
       "3                       0    74.0     169          0.0            6   \n",
       "4                       0    74.0     169          0.0            7   \n",
       "...                   ...     ...     ...          ...          ...   \n",
       "1024                    1    73.0     165          0.0            4   \n",
       "1025                    0    73.0     165          0.0            5   \n",
       "1026                    0    74.0     166          0.0            3   \n",
       "1027                    0    74.0     166          0.0            4   \n",
       "1028                    0    78.0     174          0.0            3   \n",
       "\n",
       "      playoff_progression_rolling  playoff_rolling  pp36_rolling  eff_rolling  \\\n",
       "0                             0.0              0.0         14.40     -301.960   \n",
       "1                             0.5              0.5         14.22     -299.970   \n",
       "2                             1.0              1.0         12.96     -204.815   \n",
       "3                             0.5              0.5         12.78     -193.780   \n",
       "4                             0.0              0.0         13.50     -230.290   \n",
       "...                           ...              ...           ...          ...   \n",
       "1024                          2.5              1.0         18.90     -305.255   \n",
       "1025                          2.0              1.0         19.62     -320.180   \n",
       "1026                          0.5              0.5         13.14     -131.490   \n",
       "1027                          0.5              0.5         14.40      -30.555   \n",
       "1028                          0.0              0.0          9.72      -24.625   \n",
       "\n",
       "      award_count_rolling  defensive_prowess_rolling  \\\n",
       "0                     0.0                      63.35   \n",
       "1                     0.0                      53.70   \n",
       "2                     0.0                      45.60   \n",
       "3                     0.0                      41.55   \n",
       "4                     0.0                      35.55   \n",
       "...                   ...                        ...   \n",
       "1024                  0.0                      58.50   \n",
       "1025                  0.0                      62.10   \n",
       "1026                  0.0                      27.25   \n",
       "1027                  0.0                      22.30   \n",
       "1028                  0.0                      12.30   \n",
       "\n",
       "      defensive_discipline_rolling  \n",
       "0                            12.07  \n",
       "1                            11.74  \n",
       "2                             9.49  \n",
       "3                             9.21  \n",
       "4                             9.09  \n",
       "...                            ...  \n",
       "1024                          9.21  \n",
       "1025                          8.24  \n",
       "1026                          6.37  \n",
       "1027                          4.54  \n",
       "1028                          3.78  \n",
       "\n",
       "[1029 rows x 55 columns]"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 20)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "playerID                        False\n",
       "year                            False\n",
       "stint                           False\n",
       "tmID                            False\n",
       "GP                              False\n",
       "                                ...  \n",
       "pp36_rolling                    False\n",
       "eff_rolling                     False\n",
       "award_count_rolling             False\n",
       "defensive_prowess_rolling       False\n",
       "defensive_discipline_rolling    False\n",
       "Length: 55, dtype: bool"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from https://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import learning_curve\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "def plot_learning_curve(\n",
    "    title,\n",
    "    train_sizes, \n",
    "    train_scores, \n",
    "    test_scores, \n",
    "    fit_times,\n",
    "    score_times,\n",
    "    axes=None,\n",
    "    ylim=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate 3 plots: the test and training learning curve, the training\n",
    "    samples vs fit times curve, the fit times vs score curve.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : estimator instance\n",
    "        An estimator instance implementing `fit` and `predict` methods which\n",
    "        will be cloned for each validation.\n",
    "\n",
    "    title : str\n",
    "        Title for the chart.\n",
    "\n",
    "    X : array-like of shape (n_samples, n_features)\n",
    "        Training vector, where ``n_samples`` is the number of samples and\n",
    "        ``n_features`` is the number of features.\n",
    "\n",
    "    y : array-like of shape (n_samples) or (n_samples, n_features)\n",
    "        Target relative to ``X`` for classification or regression;\n",
    "        None for unsupervised learning.\n",
    "\n",
    "    axes : array-like of shape (3,), default=None\n",
    "        Axes to use for plotting the curves.\n",
    "\n",
    "    ylim : tuple of shape (2,), default=None\n",
    "        Defines minimum and maximum y-values plotted, e.g. (ymin, ymax).\n",
    "\n",
    "    cv : int, cross-validation generator or an iterable, default=None\n",
    "        Determines the cross-validation splitting strategy.\n",
    "        Possible inputs for cv are:\n",
    "\n",
    "          - None, to use the default 5-fold cross-validation,\n",
    "          - integer, to specify the number of folds.\n",
    "          - :term:`CV splitter`,\n",
    "          - An iterable yielding (train, test) splits as arrays of indices.\n",
    "\n",
    "        For integer/None inputs, if ``y`` is binary or multiclass,\n",
    "        :class:`StratifiedKFold` used. If the estimator is not a classifier\n",
    "        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n",
    "\n",
    "        Refer :ref:`User Guide <cross_validation>` for the various\n",
    "        cross-validators that can be used here.\n",
    "\n",
    "    n_jobs : int or None, default=None\n",
    "        Number of jobs to run in parallel.\n",
    "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
    "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
    "        for more details.\n",
    "\n",
    "    train_sizes : array-like of shape (n_ticks,)\n",
    "        Relative or absolute numbers of training examples that will be used to\n",
    "        generate the learning curve. If the ``dtype`` is float, it is regarded\n",
    "        as a fraction of the maximum size of the training set (that is\n",
    "        determined by the selected validation method), i.e. it has to be within\n",
    "        (0, 1]. Otherwise it is interpreted as absolute sizes of the training\n",
    "        sets. Note that for classification the number of samples usually have\n",
    "        to be big enough to contain at least one sample from each class.\n",
    "        (default: np.linspace(0.1, 1.0, 5))\n",
    "    \"\"\"\n",
    "    if axes is None:\n",
    "        fig, axes = plt.subplots(3, 2, figsize=(10, 15))\n",
    "\n",
    "    axes = axes.reshape(-1)\n",
    "    fig.subplots_adjust(hspace=0.5, wspace=0.5)\n",
    "    fig = fig.delaxes(axes[-1])\n",
    "    \n",
    "    axes[0].set_title(title)\n",
    "    if ylim is not None:\n",
    "        axes[0].set_ylim(*ylim)\n",
    "    axes[0].set_xlabel(\"Training examples\")\n",
    "    axes[0].set_ylabel(\"Score\")\n",
    "\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    fit_times_mean = np.mean(fit_times, axis=1)\n",
    "    fit_times_std = np.std(fit_times, axis=1)\n",
    "    score_times_mean = np.mean(score_times, axis=1)\n",
    "    score_times_std = np.std(score_times, axis=1)\n",
    "\n",
    "    # Plot learning curve\n",
    "    axes[0].grid()\n",
    "    axes[0].fill_between(\n",
    "        train_sizes,\n",
    "        train_scores_mean - train_scores_std,\n",
    "        train_scores_mean + train_scores_std,\n",
    "        alpha=0.1,\n",
    "        color=\"r\",\n",
    "    )\n",
    "    axes[0].fill_between(\n",
    "        train_sizes,\n",
    "        test_scores_mean - test_scores_std,\n",
    "        test_scores_mean + test_scores_std,\n",
    "        alpha=0.1,\n",
    "        color=\"g\",\n",
    "    )\n",
    "    axes[0].plot(\n",
    "        train_sizes, train_scores_mean, \"o-\", color=\"r\", label=\"Training score\"\n",
    "    )\n",
    "    axes[0].plot(\n",
    "        train_sizes, test_scores_mean, \"o-\", color=\"g\", label=\"Cross-validation score\"\n",
    "    )\n",
    "    axes[0].legend(loc=\"best\")\n",
    "\n",
    "    # Plot n_samples vs fit_times\n",
    "    axes[1].grid()\n",
    "    axes[1].plot(train_sizes, fit_times_mean, \"o-\")\n",
    "    axes[1].fill_between(\n",
    "        train_sizes,\n",
    "        fit_times_mean - fit_times_std,\n",
    "        fit_times_mean + fit_times_std,\n",
    "        alpha=0.1,\n",
    "    )\n",
    "    axes[1].set_xlabel(\"Training examples\")\n",
    "    axes[1].set_ylabel(\"fit_times\")\n",
    "    axes[1].set_title(\"Scalability of the model\")\n",
    "\n",
    "    # Plot fit_time vs score\n",
    "    fit_time_argsort = fit_times_mean.argsort()\n",
    "    fit_time_sorted = fit_times_mean[fit_time_argsort]\n",
    "    test_scores_mean_sorted = test_scores_mean[fit_time_argsort]\n",
    "    test_scores_std_sorted = test_scores_std[fit_time_argsort]\n",
    "    axes[2].grid()\n",
    "    axes[2].plot(fit_time_sorted, test_scores_mean_sorted, \"o-\")\n",
    "    axes[2].fill_between(\n",
    "        fit_time_sorted,\n",
    "        test_scores_mean_sorted - test_scores_std_sorted,\n",
    "        test_scores_mean_sorted + test_scores_std_sorted,\n",
    "        alpha=0.1,\n",
    "    )\n",
    "    axes[2].set_xlabel(\"fit_times\")\n",
    "    axes[2].set_ylabel(\"Score\")\n",
    "    axes[2].set_title(\"Performance of the model\")\n",
    "\n",
    "    # Plot n_samples vs score_times\n",
    "    axes[3].grid()\n",
    "    axes[3].plot(train_sizes, score_times_mean, \"o-\")\n",
    "    axes[3].fill_between(\n",
    "        train_sizes,\n",
    "        score_times_mean - score_times_std,\n",
    "        score_times_mean + score_times_std,\n",
    "        alpha=0.1,\n",
    "    )\n",
    "    axes[3].set_xlabel(\"Training examples\")\n",
    "    axes[3].set_ylabel(\"score_times\")\n",
    "    axes[3].set_title(\"Scalability of the model\")\n",
    "\n",
    "    # Plot score_time vs score\n",
    "    score_time_argsort = score_times_mean.argsort()\n",
    "    score_time_sorted = score_times_mean[score_time_argsort]\n",
    "    test_scores_mean_sorted = test_scores_mean[score_time_argsort]\n",
    "    test_scores_std_sorted = test_scores_std[score_time_argsort]\n",
    "    axes[4].grid()\n",
    "    axes[4].plot(score_time_sorted, test_scores_mean_sorted, \"o-\")\n",
    "    axes[4].fill_between(\n",
    "        score_time_sorted,\n",
    "        test_scores_mean_sorted - test_scores_std_sorted,\n",
    "        test_scores_mean_sorted + test_scores_std_sorted,\n",
    "        alpha=0.1,\n",
    "    )\n",
    "    axes[4].set_xlabel(\"score_times\")\n",
    "    axes[4].set_ylabel(\"Score\")\n",
    "    axes[4].set_title(\"Performance of the model\")\n",
    "\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2  3  4  5  6  7  8  9 10 11]\n"
     ]
    }
   ],
   "source": [
    "model = DecisionTreeClassifier()\n",
    "#loop in rolling window method through all data\n",
    "unique_years = data['year'].unique()\n",
    "#sort the years\n",
    "unique_years.sort()\n",
    "rolling_window_results = []\n",
    "competition_predictions = []\n",
    "trained_models = list()\n",
    "\n",
    "\n",
    "\n",
    "print(unique_years)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_columns_to_remove():\n",
    "    return ['playoff']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GP', 'oRebounds', 'dRebounds', 'dq', 'PostMinutes', 'PostPoints', 'PostoRebounds', 'PostdRebounds', 'PostRebounds', 'PostAssists', 'PostSteals', 'PostBlocks', 'PostTurnovers', 'PostPF', 'PostDQ', 'ft%', 'fg%', 'three%', 'gs%', 'Postft%', 'Postfg%', 'Postthree%', 'Postgs%', 'efg%', 'ts%', 'ppg', 'rpg', 'apg', 'spg', 'bpg', 'eff', 'pp36', 'defensive_prowess', 'defensive_discipline', 'mpg', 'playoff_progression', 'award_count']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "toZeroArr = [c for c in data.columns if c not in input_cols]\n",
    "#add playoff to the list of columns to remove\n",
    "toZeroArr.remove('playoff')\n",
    "print(toZeroArr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_training_data(data, unique_years, i):\n",
    "    train_years_before = unique_years[:i]\n",
    "    train_years = unique_years[:i+1]\n",
    "    test_year = unique_years[i]\n",
    "\n",
    "    if i + 1 > len(unique_years): \n",
    "        return\n",
    "\n",
    "    predict_year = unique_years[i+1]  # Predicting for year i+2 \n",
    "\n",
    "    if predict_year < test_year:\n",
    "        return\n",
    "\n",
    "    # print(\"Train years:\", train_years)\n",
    "    # print(\"Predict Year: \", predict_year)\n",
    "    \n",
    "    train_before = data[data['year'].isin(train_years_before)]\n",
    "    train = data[data['year'].isin(train_years)]\n",
    "    test = data[data['year'] == test_year]\n",
    "    actual = data[data['year'] == predict_year]\n",
    "\n",
    "    return train_before, train, test, actual, predict_year, train_years, test_year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_data(train_before, train, test, actual, data, unique_years):\n",
    "    #one hot encoding\n",
    "    # train_before = pd.get_dummies(train_before, columns=['tmID', 'confID', 'pos', 'college', 'playerID'])\n",
    "    # train = pd.get_dummies(train, columns=['tmID', 'confID', 'pos', 'college', 'playerID']) \n",
    "    # Removing and adding rows based on certain conditions\n",
    "    # train_before_indices_to_remove = []\n",
    "    # train_indices_to_remove = []\n",
    "    # #check if a player have a entry in the train df with the next year\n",
    "    # for index, row in train_before.iterrows():\n",
    "    #     player_id = row['playerID']\n",
    "    #     year = row['year'] + 1 \n",
    "    #     if row['stint'] > 1:\n",
    "    #         train_before_indices_to_remove.append(index)\n",
    "    #     elif not (((train['playerID'] == player_id) & (train['year'] == year)).any()):\n",
    "    #         data_to_add = {\n",
    "    #             'playerID': player_id, \n",
    "    #             'year': year,        \n",
    "    #         }\n",
    "    #         new_row = pd.DataFrame(data_to_add, index=[0])\n",
    "    #         train = pd.concat([train, new_row], ignore_index=True)\n",
    "       \n",
    "\n",
    "    # train_before = train_before.drop(train_before_indices_to_remove)\n",
    "    # for index, row in train.iterrows():\n",
    "    #     player_id = row['playerID']\n",
    "    #     year = row['year'] - 1 \n",
    "    #     if row['stint'] > 1:\n",
    "    #         train_indices_to_remove.append(index)\n",
    "    #     elif not (((train_before['playerID'] == player_id) & (train_before['year'] == year)).any()):\n",
    "    #         data_to_add = {\n",
    "    #             'playerID': player_id, \n",
    "    #             'year': year,        \n",
    "    #         }\n",
    "    #         new_row = pd.DataFrame(data_to_add, index=[0])\n",
    "    #         train_before = pd.concat([train_before , new_row], ignore_index=True)\n",
    "\n",
    "    # train = train_before.drop(train_indices_to_remove)\n",
    "    # train.fillna(0, inplace=True)\n",
    "    # train.sort_values(by='playerID', inplace=True)\n",
    "    # train_before.fillna(0, inplace=True)\n",
    "    # train_before.sort_values(by='playerID', inplace=True)\n",
    "    remove_columns = get_columns_to_remove()\n",
    "    actual[toZeroArr] = 0\n",
    "\n",
    "    #remove stint > 0\n",
    "    train = train[train['stint'] == 0]\n",
    "    actual = actual[actual['stint'] == 0]\n",
    "\n",
    "\n",
    "    X_train = train.drop(remove_columns, axis=1)\n",
    "    y_train = train[output_cols]\n",
    "    X_actual = actual.drop(remove_columns, axis=1)\n",
    "    y_actual = actual[output_cols]\n",
    "\n",
    "    return X_train, y_train, X_actual, y_actual\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_and_evaluate_model(X_train, y_train, model):\n",
    "    classifier = model.fit(X_train, y_train)\n",
    "    return classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_and_evaluate(classifier, X_actual, y_actual):\n",
    "    accuracy = 0.0\n",
    "    # Evaluate the model\n",
    "    predictions = classifier.predict(X_actual)\n",
    "    predictions_prob = classifier.predict_proba(X_actual)[:, 1]\n",
    "\n",
    "    if X_actual['year'].unique()[0]==11:\n",
    "        competition_predictions.append(predictions)\n",
    "    else:\n",
    "        accuracy = accuracy_score(y_actual, predictions)\n",
    "    \n",
    "    auc_score = accuracy#roc_auc_score(y_actual, predictions_prob)\n",
    "\n",
    "    return accuracy, auc_score, predictions, predictions_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rolling_window_method(i, model, data, unique_years):\n",
    "    train_before, train, test, actual, predict_year, train_years, test_year = prepare_training_data(data, unique_years, i)\n",
    "    \n",
    "    if train_before is None:\n",
    "        return None\n",
    "\n",
    "    X_train, y_train, X_actual, y_actual= process_data(train_before, train, test, actual, data, unique_years)\n",
    "\n",
    "    #print years in X_train and X_actual\n",
    "    print(\"Unique years in X_train:\", X_train['year'].unique())\n",
    "    print(\"Unique years in X_actual:\", X_actual['year'].unique())\n",
    "    \n",
    "    #create a csv file for each year, storing the data for that year and that model, in the folder output/rolling_window\n",
    "    #X_train.to_csv(f'output/rolling_window/{model.__class__.__name__}_{test_year}_train.csv', index=False)\n",
    "    #X_test.to_csv(f'output/rolling_window/{model.__class__.__name__}_{test_year}_test.csv', index=False)\n",
    "    #X_actual.to_csv(f'output/rolling_window/{model.__class__.__name__}_{test_year}_actual.csv', index=False)\n",
    "    # y_actual.to_csv(f'output/rolling_window/{model.__class__.__name__}_{test_year}_y_actual.csv', index=False)\n",
    "\n",
    "\n",
    "    classifier = train_and_evaluate_model(X_train, y_train, model)\n",
    "\n",
    "    # Calculate accuracy\n",
    "\n",
    "    accuracy, auc_score, predictions, predictions_prob = predict_and_evaluate(classifier, X_actual, y_actual)\n",
    "\n",
    "    #print(f\"Best params for {model.__class__.__name_}: {model.best_params_}\")\n",
    "    # print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    # print(f\"AUC: {auc_score:.4f}\")\n",
    "\n",
    "\n",
    "    return {\n",
    "        'train_years': train_years,\n",
    "        'test_year': test_year,\n",
    "        'Year': predict_year,\n",
    "        'accuracy': accuracy,\n",
    "        'auc': auc_score,\n",
    "        'model': classifier,\n",
    "        'predictions': predictions,\n",
    "        'predictions_prob': predictions_prob,\n",
    "        'actual_results': y_actual,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def model_training_loop(model, data, unique_years):\n",
    "    rolling_window_results = []\n",
    "    for i in range(len(unique_years) - 1):\n",
    "        result= rolling_window_method(i, model, data, unique_years)\n",
    "        if result:\n",
    "            rolling_window_results.append(result)\n",
    "    return rolling_window_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = DecisionTreeClassifier()\n",
    "# rolling_window_results += model_training_loop(model, data, unique_years)\n",
    "# trained_models.append(rolling_window_results[-1]['model'])  # Get the last trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique years in X_train: [2]\n",
      "Unique years in X_actual: [3]\n",
      "Unique years in X_train: [3 2]\n",
      "Unique years in X_actual: [4]\n",
      "Unique years in X_train: [4 3 2]\n",
      "Unique years in X_actual: [5]\n",
      "Unique years in X_train: [4 5 3 2]\n",
      "Unique years in X_actual: [6]\n",
      "Unique years in X_train: [4 5 6 3 2]\n",
      "Unique years in X_actual: [7]\n",
      "Unique years in X_train: [4 5 6 7 3 2]\n",
      "Unique years in X_actual: [8]\n",
      "Unique years in X_train: [4 5 6 7 8 3 2]\n",
      "Unique years in X_actual: [9]\n",
      "Unique years in X_train: [4 5 6 7 8 9 3 2]\n",
      "Unique years in X_actual: [10]\n",
      "Unique years in X_train: [ 4  5  6  7  8  9  3 10  2]\n",
      "Unique years in X_actual: [11]\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier()\n",
    "rolling_window_results += model_training_loop(model, data, unique_years)\n",
    "trained_models.append(rolling_window_results[-1]['model'])  # Get the last trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique years in X_train: [2]\n",
      "Unique years in X_actual: [3]\n",
      "Unique years in X_train: [3 2]\n",
      "Unique years in X_actual: [4]\n",
      "Unique years in X_train: [4 3 2]\n",
      "Unique years in X_actual: [5]\n",
      "Unique years in X_train: [4 5 3 2]\n",
      "Unique years in X_actual: [6]\n",
      "Unique years in X_train: [4 5 6 3 2]\n",
      "Unique years in X_actual: [7]\n",
      "Unique years in X_train: [4 5 6 7 3 2]\n",
      "Unique years in X_actual: [8]\n",
      "Unique years in X_train: [4 5 6 7 8 3 2]\n",
      "Unique years in X_actual: [9]\n",
      "Unique years in X_train: [4 5 6 7 8 9 3 2]\n",
      "Unique years in X_actual: [10]\n",
      "Unique years in X_train: [ 4  5  6  7  8  9  3 10  2]\n",
      "Unique years in X_actual: [11]\n"
     ]
    }
   ],
   "source": [
    "model = GradientBoostingClassifier()\n",
    "rolling_window_results += model_training_loop(model, data, unique_years)\n",
    "trained_models.append(rolling_window_results[-1]['model'])  # Get the last trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique years in X_train: [2]\n",
      "Unique years in X_actual: [3]\n",
      "Unique years in X_train: [3 2]\n",
      "Unique years in X_actual: [4]\n",
      "Unique years in X_train: [4 3 2]\n",
      "Unique years in X_actual: [5]\n",
      "Unique years in X_train: [4 5 3 2]\n",
      "Unique years in X_actual: [6]\n",
      "Unique years in X_train: [4 5 6 3 2]\n",
      "Unique years in X_actual: [7]\n",
      "Unique years in X_train: [4 5 6 7 3 2]\n",
      "Unique years in X_actual: [8]\n",
      "Unique years in X_train: [4 5 6 7 8 3 2]\n",
      "Unique years in X_actual: [9]\n",
      "Unique years in X_train: [4 5 6 7 8 9 3 2]\n",
      "Unique years in X_actual: [10]\n",
      "Unique years in X_train: [ 4  5  6  7  8  9  3 10  2]\n",
      "Unique years in X_actual: [11]\n"
     ]
    }
   ],
   "source": [
    "#gaussian naive bayes\n",
    "model = GaussianNB()\n",
    "rolling_window_results += model_training_loop(model, data, unique_years)\n",
    "trained_models.append(rolling_window_results[-1]['model'])  # Get the last trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique years in X_train: [2]\n",
      "Unique years in X_actual: [3]\n",
      "Unique years in X_train: [3 2]\n",
      "Unique years in X_actual: [4]\n",
      "Unique years in X_train: [4 3 2]\n",
      "Unique years in X_actual: [5]\n",
      "Unique years in X_train: [4 5 3 2]\n",
      "Unique years in X_actual: [6]\n",
      "Unique years in X_train: [4 5 6 3 2]\n",
      "Unique years in X_actual: [7]\n",
      "Unique years in X_train: [4 5 6 7 3 2]\n",
      "Unique years in X_actual: [8]\n",
      "Unique years in X_train: [4 5 6 7 8 3 2]\n",
      "Unique years in X_actual: [9]\n",
      "Unique years in X_train: [4 5 6 7 8 9 3 2]\n",
      "Unique years in X_actual: [10]\n",
      "Unique years in X_train: [ 4  5  6  7  8  9  3 10  2]\n",
      "Unique years in X_actual: [11]\n"
     ]
    }
   ],
   "source": [
    "#logistic regression\n",
    "model = LogisticRegression()\n",
    "rolling_window_results += model_training_loop(model, data, unique_years)\n",
    "trained_models.append(rolling_window_results[-1]['model'])  # Get the last trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique years in X_train: [2]\n",
      "Unique years in X_actual: [3]\n",
      "Unique years in X_train: [3 2]\n",
      "Unique years in X_actual: [4]\n",
      "Unique years in X_train: [4 3 2]\n",
      "Unique years in X_actual: [5]\n",
      "Unique years in X_train: [4 5 3 2]\n",
      "Unique years in X_actual: [6]\n",
      "Unique years in X_train: [4 5 6 3 2]\n",
      "Unique years in X_actual: [7]\n",
      "Unique years in X_train: [4 5 6 7 3 2]\n",
      "Unique years in X_actual: [8]\n",
      "Unique years in X_train: [4 5 6 7 8 3 2]\n",
      "Unique years in X_actual: [9]\n",
      "Unique years in X_train: [4 5 6 7 8 9 3 2]\n",
      "Unique years in X_actual: [10]\n",
      "Unique years in X_train: [ 4  5  6  7  8  9  3 10  2]\n",
      "Unique years in X_actual: [11]\n"
     ]
    }
   ],
   "source": [
    "#ada boost\n",
    "model = AdaBoostClassifier()\n",
    "rolling_window_results += model_training_loop(model, data, unique_years)\n",
    "trained_models.append(rolling_window_results[-1]['model'])  # Get the last trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique years in X_train: [2]\n",
      "Unique years in X_actual: [3]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot have number of splits n_splits=3 greater than the number of samples: n_samples=2.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/juanbellon/Documents/feup/ac/src/6-predictive.ipynb Cell 25\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/juanbellon/Documents/feup/ac/src/6-predictive.ipynb#X33sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m level1 \u001b[39m=\u001b[39m RandomForestClassifier(n_estimators\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m, min_samples_split\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, min_samples_leaf\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, max_features\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msqrt\u001b[39m\u001b[39m'\u001b[39m, max_depth\u001b[39m=\u001b[39m\u001b[39m9\u001b[39m, bootstrap\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, criterion\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mentropy\u001b[39m\u001b[39m'\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/juanbellon/Documents/feup/ac/src/6-predictive.ipynb#X33sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m clf \u001b[39m=\u001b[39m StackingClassifier(estimators\u001b[39m=\u001b[39mlevel0, final_estimator\u001b[39m=\u001b[39mlevel1, cv\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/juanbellon/Documents/feup/ac/src/6-predictive.ipynb#X33sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m rolling_window_results \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m model_training_loop(clf, data, unique_years)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juanbellon/Documents/feup/ac/src/6-predictive.ipynb#X33sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m trained_models\u001b[39m.\u001b[39mappend(rolling_window_results[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m'\u001b[39m])  \u001b[39m# Get the last trained model\u001b[39;00m\n",
      "\u001b[1;32m/home/juanbellon/Documents/feup/ac/src/6-predictive.ipynb Cell 25\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/juanbellon/Documents/feup/ac/src/6-predictive.ipynb#X33sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m rolling_window_results \u001b[39m=\u001b[39m []\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/juanbellon/Documents/feup/ac/src/6-predictive.ipynb#X33sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(unique_years) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/juanbellon/Documents/feup/ac/src/6-predictive.ipynb#X33sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     result\u001b[39m=\u001b[39m rolling_window_method(i, model, data, unique_years)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/juanbellon/Documents/feup/ac/src/6-predictive.ipynb#X33sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mif\u001b[39;00m result:\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/juanbellon/Documents/feup/ac/src/6-predictive.ipynb#X33sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m         rolling_window_results\u001b[39m.\u001b[39mappend(result)\n",
      "\u001b[1;32m/home/juanbellon/Documents/feup/ac/src/6-predictive.ipynb Cell 25\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juanbellon/Documents/feup/ac/src/6-predictive.ipynb#X33sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mUnique years in X_actual:\u001b[39m\u001b[39m\"\u001b[39m, X_actual[\u001b[39m'\u001b[39m\u001b[39myear\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39munique())\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juanbellon/Documents/feup/ac/src/6-predictive.ipynb#X33sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m#create a csv file for each year, storing the data for that year and that model, in the folder output/rolling_window\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juanbellon/Documents/feup/ac/src/6-predictive.ipynb#X33sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m#X_train.to_csv(f'output/rolling_window/{model.__class__.__name__}_{test_year}_train.csv', index=False)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juanbellon/Documents/feup/ac/src/6-predictive.ipynb#X33sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m#X_test.to_csv(f'output/rolling_window/{model.__class__.__name__}_{test_year}_test.csv', index=False)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juanbellon/Documents/feup/ac/src/6-predictive.ipynb#X33sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m#X_actual.to_csv(f'output/rolling_window/{model.__class__.__name__}_{test_year}_actual.csv', index=False)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juanbellon/Documents/feup/ac/src/6-predictive.ipynb#X33sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# y_actual.to_csv(f'output/rolling_window/{model.__class__.__name__}_{test_year}_y_actual.csv', index=False)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/juanbellon/Documents/feup/ac/src/6-predictive.ipynb#X33sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m classifier \u001b[39m=\u001b[39m train_and_evaluate_model(X_train, y_train, model)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juanbellon/Documents/feup/ac/src/6-predictive.ipynb#X33sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# Calculate accuracy\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juanbellon/Documents/feup/ac/src/6-predictive.ipynb#X33sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m accuracy, auc_score, predictions, predictions_prob \u001b[39m=\u001b[39m predict_and_evaluate(classifier, X_actual, y_actual)\n",
      "\u001b[1;32m/home/juanbellon/Documents/feup/ac/src/6-predictive.ipynb Cell 25\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/juanbellon/Documents/feup/ac/src/6-predictive.ipynb#X33sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_and_evaluate_model\u001b[39m(X_train, y_train, model):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/juanbellon/Documents/feup/ac/src/6-predictive.ipynb#X33sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     classifier \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/juanbellon/Documents/feup/ac/src/6-predictive.ipynb#X33sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m classifier\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/ensemble/_stacking.py:660\u001b[0m, in \u001b[0;36mStackingClassifier.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    658\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_label_encoder\u001b[39m.\u001b[39mclasses_\n\u001b[1;32m    659\u001b[0m     y_encoded \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_label_encoder\u001b[39m.\u001b[39mtransform(y)\n\u001b[0;32m--> 660\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit(X, y_encoded, sample_weight)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/ensemble/_stacking.py:252\u001b[0m, in \u001b[0;36m_BaseStacking.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    247\u001b[0m         cv\u001b[39m.\u001b[39mrandom_state \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mRandomState()\n\u001b[1;32m    249\u001b[0m     fit_params \u001b[39m=\u001b[39m (\n\u001b[1;32m    250\u001b[0m         {\u001b[39m\"\u001b[39m\u001b[39msample_weight\u001b[39m\u001b[39m\"\u001b[39m: sample_weight} \u001b[39mif\u001b[39;00m sample_weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    251\u001b[0m     )\n\u001b[0;32m--> 252\u001b[0m     predictions \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs)(\n\u001b[1;32m    253\u001b[0m         delayed(cross_val_predict)(\n\u001b[1;32m    254\u001b[0m             clone(est),\n\u001b[1;32m    255\u001b[0m             X,\n\u001b[1;32m    256\u001b[0m             y,\n\u001b[1;32m    257\u001b[0m             cv\u001b[39m=\u001b[39;49mdeepcopy(cv),\n\u001b[1;32m    258\u001b[0m             method\u001b[39m=\u001b[39;49mmeth,\n\u001b[1;32m    259\u001b[0m             n_jobs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs,\n\u001b[1;32m    260\u001b[0m             fit_params\u001b[39m=\u001b[39;49mfit_params,\n\u001b[1;32m    261\u001b[0m             verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[1;32m    262\u001b[0m         )\n\u001b[1;32m    263\u001b[0m         \u001b[39mfor\u001b[39;49;00m est, meth \u001b[39min\u001b[39;49;00m \u001b[39mzip\u001b[39;49m(all_estimators, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstack_method_)\n\u001b[1;32m    264\u001b[0m         \u001b[39mif\u001b[39;49;00m est \u001b[39m!=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mdrop\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m    265\u001b[0m     )\n\u001b[1;32m    267\u001b[0m \u001b[39m# Only not None or not 'drop' estimators will be used in transform.\u001b[39;00m\n\u001b[1;32m    268\u001b[0m \u001b[39m# Remove the None from the method as well.\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstack_method_ \u001b[39m=\u001b[39m [\n\u001b[1;32m    270\u001b[0m     meth\n\u001b[1;32m    271\u001b[0m     \u001b[39mfor\u001b[39;00m (meth, est) \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstack_method_, all_estimators)\n\u001b[1;32m    272\u001b[0m     \u001b[39mif\u001b[39;00m est \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdrop\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    273\u001b[0m ]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     58\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[1;32m     59\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[1;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     61\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[1;32m     62\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/joblib/parallel.py:1085\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1076\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1077\u001b[0m     \u001b[39m# Only set self._iterating to True if at least a batch\u001b[39;00m\n\u001b[1;32m   1078\u001b[0m     \u001b[39m# was dispatched. In particular this covers the edge\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1082\u001b[0m     \u001b[39m# was very quick and its callback already dispatched all the\u001b[39;00m\n\u001b[1;32m   1083\u001b[0m     \u001b[39m# remaining jobs.\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m-> 1085\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[1;32m   1086\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1088\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/joblib/parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    899\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    900\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 901\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[1;32m    902\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/joblib/parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    818\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[0;32m--> 819\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[1;32m    820\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    821\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    823\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    824\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m     \u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[1;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/joblib/_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[1;32m    595\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    596\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 597\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[1;32m    122\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[0;32m--> 123\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:960\u001b[0m, in \u001b[0;36mcross_val_predict\u001b[0;34m(estimator, X, y, groups, cv, n_jobs, verbose, fit_params, pre_dispatch, method)\u001b[0m\n\u001b[1;32m    957\u001b[0m X, y, groups \u001b[39m=\u001b[39m indexable(X, y, groups)\n\u001b[1;32m    959\u001b[0m cv \u001b[39m=\u001b[39m check_cv(cv, y, classifier\u001b[39m=\u001b[39mis_classifier(estimator))\n\u001b[0;32m--> 960\u001b[0m splits \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n\u001b[1;32m    962\u001b[0m test_indices \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate([test \u001b[39mfor\u001b[39;00m _, test \u001b[39min\u001b[39;00m splits])\n\u001b[1;32m    963\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _check_is_permutation(test_indices, _num_samples(X)):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/model_selection/_split.py:345\u001b[0m, in \u001b[0;36m_BaseKFold.split\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m    343\u001b[0m n_samples \u001b[39m=\u001b[39m _num_samples(X)\n\u001b[1;32m    344\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_splits \u001b[39m>\u001b[39m n_samples:\n\u001b[0;32m--> 345\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    346\u001b[0m         (\n\u001b[1;32m    347\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mCannot have number of splits n_splits=\u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m greater\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    348\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m than the number of samples: n_samples=\u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    349\u001b[0m         )\u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_splits, n_samples)\n\u001b[1;32m    350\u001b[0m     )\n\u001b[1;32m    352\u001b[0m \u001b[39mfor\u001b[39;00m train, test \u001b[39min\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39msplit(X, y, groups):\n\u001b[1;32m    353\u001b[0m     \u001b[39myield\u001b[39;00m train, test\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot have number of splits n_splits=3 greater than the number of samples: n_samples=2."
     ]
    }
   ],
   "source": [
    "level0 = list()\n",
    "#append models in trained_models to level0\n",
    "for i in range(len(trained_models)):\n",
    "    level0.append((str(i), trained_models[i]))\n",
    "\n",
    "level1 = RandomForestClassifier(n_estimators=100, min_samples_split=3, min_samples_leaf=3, max_features='sqrt', max_depth=9, bootstrap=True, criterion='entropy', random_state=42)\n",
    "clf = StackingClassifier(estimators=level0, final_estimator=level1, cv=3)\n",
    "\n",
    "rolling_window_results += model_training_loop(clf, data, unique_years)\n",
    "trained_models.append(rolling_window_results[-1]['model'])  # Get the last trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGdCAYAAADAAnMpAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAA9hAAAPYQGoP6dpAACO7UlEQVR4nO2deXwU9f3/XzOzO7ubGwgkHCEECCEQLgkiKGIr4tHaWrVSrdpWaKVWW+Tbw+Pb1vrtt/jrYbHfFqoVtFoPbD2qLW1NrSKKIlcQIdyBAEkIAXJnd3Zn5vfHHNlNdrM7uzM72c37+XjkQTI7M/lkd5l9zft6MbIsyyAIgiAIgrAJ1u4FEARBEAQxuCExQhAEQRCErZAYIQiCIAjCVkiMEARBEARhKyRGCIIgCIKwFRIjBEEQBEHYCokRgiAIgiBshcQIQRAEQRC24rB7AbEgSRLq6+uRnZ0NhmHsXg5BEARBEDEgyzLa29sxatQosGzk+EdKiJH6+noUFRXZvQyCIAiCIOLgxIkTGDNmTMTH4xIja9aswS9+8Qs0NDRg6tSpWL16NRYsWBB2369+9av44x//2Gf7lClTsHfv3ph+X3Z2NgDlj8nJyYlnyQRBEARBJJm2tjYUFRXpn+ORMCxGNmzYgBUrVmDNmjW4+OKL8fjjj+Pqq6/Gvn37MHbs2D77P/bYY3jkkUf0nwOBAGbMmIEvfvGLMf9OLTWTk5NDYoQgCIIgUoxoJRaMUaO8uXPn4oILLsDatWv1beXl5bjuuuuwatWqqMe/9tpruP7661FbW4vi4uKYfmdbWxtyc3PR2tpKYoQgCIIgUoRYP78NddMIgoAdO3Zg8eLFIdsXL16MLVu2xHSOdevWYdGiRf0KEZ/Ph7a2tpAvgiAIgiDSE0NipLm5GaIooqCgIGR7QUEBGhsbox7f0NCAf/zjH1i2bFm/+61atQq5ubn6FxWvEgRBEET6ElcBa+/cjyzLMbXcPv3008jLy8N1113X7373338/Vq5cqf+sFcAQBEEQ6YcsywgEAhBF0e6lEAbhOA4OhyPhsRuGxEh+fj44jusTBWlqauoTLemNLMtYv349brvtNvA83+++LpcLLpfLyNIIgiCIFEQQBDQ0NKCrq8vupRBxkpGRgZEjR0b9bO8PQ2KE53nMnj0bVVVV+MIXvqBvr6qqwuc///l+j920aRMOHz6MpUuXxrdSgiAIIq2QJAm1tbXgOA6jRo0Cz/M02DKFkGUZgiDgzJkzqK2tRWlpab+DzfrDcJpm5cqVuO2221BZWYl58+bhiSeeQF1dHZYvXw5ASbGcOnUKzzzzTMhx69atw9y5c1FRURHXQgmCIIj0QhAESJKEoqIiZGRk2L0cIg48Hg+cTieOHz8OQRDgdrvjOo9hMbJkyRKcPXsWDz/8MBoaGlBRUYGNGzfq3TENDQ2oq6sLOaa1tRUvv/wyHnvssbgWSRAEQaQv8d5NEwMDM14/w3NG7IDmjBAEQaQfXq8XtbW1KCkpifuOmrCf/l5HS+aMEARBEARBmA2JEYIgCIIgbIXECEEQBEHEyZYtW8BxHK666iq7l5LSkBghCIIg+tDd2Y7avVtNO58sSdj22m9xcOc7pp1zILB+/Xrcc889eO+99/o0byQTv99v2+82AxIjBEEQRB/2Pv5VlPx5MarfetGU8+1591XMqX4Qjr99p9/9ZFlGlxCw5ctoP0dnZydeeuklfPOb38RnP/tZPP300yGPv/7666isrITb7UZ+fj6uv/56/TGfz4fvf//7KCoqgsvlQmlpKdatWwegZ1p5MK+99lrIDJaHHnoIM2fOxPr16zF+/Hi4XC7Isox//vOfuOSSS5CXl4dhw4bhs5/9LI4cORJyrpMnT+JLX/oShg4diszMTFRWVmLr1q04duwYWJbF9u3bQ/b/v//7PxQXFxt+fowQ1zh4giAIIn1pPXsa01vfBhhA2vkn4PIvJXxOb/VfAABZUv/Gp91+EVN+9K+Ef1887Hv4SmTwsX8sbtiwAWVlZSgrK8Ott96Ke+65Bz/84Q/BMAz+/ve/4/rrr8eDDz6IZ599FoIg4O9//7t+7O23344PPvgAv/nNbzBjxgzU1taiubnZ0HoPHz6Ml156CS+//DI4jgOgCKSVK1di2rRp6OzsxI9+9CN84QtfQHV1NViWRUdHBxYuXIjRo0fj9ddfR2FhIXbu3AlJkjBu3DgsWrQITz31FCorK/Xf89RTT+GrX/2qpQPpSIwQBEEQIRx453lcyCg+MVM6PkRH23lk5QyJ+3yCz4vJLZsAADwEU9Y4EFi3bh1uvfVWAMBVV12Fjo4OvPXWW1i0aBH+93//F1/60pfwk5/8RN9/xowZAICDBw/ipZdeQlVVFRYtWgQAGD9+vOHfLwgCnn32WQwfPlzfdsMNN/RZ44gRI7Bv3z5UVFTg+eefx5kzZ7Bt2zYMHToUADBx4kR9/2XLlmH58uV49NFH4XK5sHv3blRXV+OVV14xvD4jkBghCIIgQsg4+Jr+vZvxY/uml1B57Z1xn69myxuYgU4AAC/3X9vgcXLY9/CVcf+uRPA4uZj3PXDgAD766CP9Q9rhcGDJkiVYv349Fi1ahOrqanz9618Pe2x1dTU4jsPChQsTWm9xcXGIEAGAI0eO4Ic//CE+/PBDNDc3Q5IkAEBdXR0qKipQXV2NWbNm6UKkN9dddx3uvvtuvPrqq/jSl76E9evX41Of+hTGjRuX0FqjQWKEIAiC0DlTfwxTvLsBBtiecwUq26rA1bwGJCBGhN0v69/z6F+MMAxjKFViF+vWrUMgEMDo0aP1bbIsw+l04vz58/B4PBGP7e8xQJlo2rs+I1yBamZmZp9t1157LYqKivCHP/wBo0aNgiRJqKiogCAIMf1unudx22234amnnsL111+P559/HqtXr+73GDOgAlaCIAhC58g7z4JlZOx3TsGwK78HAJja+RHaW8/FdT7B50VZy7v6zw5GQsCf2qmaQCCAZ555Br/61a9QXV2tf+3evRvFxcV47rnnMH36dLz11lthj582bRokScKmTZvCPj58+HC0t7ejs7NT31ZdXR11XWfPnkVNTQ3++7//G5dffjnKy8tx/vz5kH2mT5+O6upqnDsX+fVctmwZ/v3vf2PNmjXw+/0hhbdWQWKEIAiC0Bl69HUAQOuEz2Fc+RwcZ8eAZwI4sGlDXOeref915KATLcjSt/m8Xaas1S7+9re/4fz581i6dCkqKipCvm688UasW7cOP/7xj/HCCy/gxz/+MWpqarBnzx78/Oc/BwCMGzcOX/nKV3DHHXfgtddeQ21tLd555x289NJLAIC5c+ciIyMDDzzwAA4fPoznn3++T6dOOIYMGYJhw4bhiSeewOHDh/Gf//wHK1euDNnn5ptvRmFhIa677jq8//77OHr0KF5++WV88MEH+j7l5eW46KKL8IMf/AA333xz1GiKGZAYIQiCIAAAJw9/gkmBgxBlBhMuuxUMy6J+9NUAAEfNa3GdU/hYqak4MGxRzzZvd8JrtZN169Zh0aJFyM3N7fPYDTfcgOrqauTk5ODPf/4zXn/9dcycOROf/vSnsXVrz9yWtWvX4sYbb8Rdd92FyZMn4+tf/7oeCRk6dCj+9Kc/YePGjZg2bRpeeOEFPPTQQ1HXxbIsXnzxRezYsQMVFRW499578Ytf/CJkH57n8eabb2LEiBG45pprMG3aNDzyyCN6N47G0qVLIQgC7rjjjjieIeOQUR5BEAQBAPjgqR9g3vHf42P3bEy/7z8AgGM12zFuw+UQZA7dKw4id0h+zOcTfF54V41HDjqx78oXMfGfXwbPiGj6ejVGjC4ho7wBzP/+7//ixRdfxJ49e6LuS0Z5BEEQhCnIkoRRJ5Q5GN6yL+jbx5VX4hg7Fjwj4uAmYwPQtBRNM/JQNucKCOABAH5faqdp0pmOjg5s27YN//d//4dvf/vbSfu9JEYIgiAIHN37EYqlE/DJTkz+1C0hjzWMUVI1/IG/Gjqn1kVzJP9ycA4HBMYJAPD7UjtNk87cfffduOSSS7Bw4cKkpWgAEiMEQRAEgKYtzwEA9mZdhJy8YSGPjbpYESdTunag9ezpmM4n+Lwoa1W6aLJnf1HZpkdGSIwMVJ5++mn4fD5s2LChTx2JlZAYIQiCGORIooiShn8oP1Tc0Ofx4rKZOMqOg9NAqqbm/deQgy6cwRCUzbkCABBQIyMiiRGiFyRGCIIgBjkHt7+FQpxBh+zBlIVfDLvP6bFKqsZ18PWYzinsfhUAcDT/0+AcyhAzP6NERgICiREiFBIjBEEQg5zWbS8AAGryLoU7IyvsPmMuvhkAMKV7J1qaG/s9n8/b1ZOiqbxJ3x5gFTEi+n0Jr5lIL0iMEARBDGICfgGlzf8GALhm3hRxv6LSGTjCjYeDkXDwnef7Pef+La/rKZrJaooGAAJqZETyU2SECIXECEEQxCBm3/tvYCjacA45KL/42n73bRp7DQDAc6j/VI2wWxl0dnT45WCDiiD1yIjgTWTJRBpCYoQgCGIQ49uljCA/lL8ITt7V775jF6hdNd5qnGs6Ff583i6UtW4G0NNFoyGpYkT2kxghQiExQhAEMUjxdnWgvEUxa8uZ86Wo+48ePxWHuIngGBmHNr0Qdh9l0FnfFA0AiKwidiQSI0QvSIwQBEEMUva9+xdkMd1oxHCUVS6KfgCAs8VKqiYrQqrG/3H4FA0ASJwaGQmkTwHrli1bwHEcrrrqqpDt77zzDhiGQUtLS59jZs6c2cdrZteuXfjiF7+IgoICuN1uTJo0CV//+tdx8OBBC1c/cCAxQhAEkQZ4uzogS5Kxg/YoE1JrR17ZRzhEYuyltwIAJvs+RnNjXchjPm8XJrcoXTQ5lX2LYSVOiYzIgfSJjKxfvx733HMP3nvvPdTV1UU/IAx/+9vfcNFFF8Hn8+G5555DTU0Nnn32WeTm5uKHP/yhySsemDjsXgBBEASRGPW1++H54xVoZPPALHkWxWUzox7T1nIWUzs+ABhgxLwvx/y7Ro0rw0HHJEwKHMSRTS8gf8kP9Mdq3n8dM5luNGFo2EiLrIoR9BcZkWXAb5N3jTMDYJiYd+/s7MRLL72Ebdu2obGxEU8//TR+9KMfGfqVXV1d+NrXvoZrrrkGr776qr69pKQEc+fODRtZSUdIjBAEQaQ4dRt/iYvQhiFSGzqevwrVlzyKmVfc0u8xB955AXMYP46zRRhfcZGh33du3GeAwweRdeQNAD1ixP+xEmk5OvxyjAgTaekRI/1ERvxdwM9GGVqPaTxQD/CZMe++YcMGlJWVoaysDLfeeivuuece/PCHPwRjQND861//QnNzM77//e+HfTwvLy/mc6UylKYhCIJIYdpazmJa0xsAgDp2NLKYbsx8/5v4YP33IIlixONc+5Xajvqiz4BhjX0UFC9QBqCV+z7BmfpjALQUjdJFk1sZfoqr7FDECJMmNSPr1q3DrbcqaaurrroKHR0deOuttwyd49ChQwCAyZMnm76+VIIiIwRBECnMvo1rcBHjxTG2CKN+sB1b/3AX5ja/jHl1T2DXrz7BxDufQ3bu0JBjzp4+iSnduwAGKFpwm+HfObK4DAcck1EW2I8jm57H8JsfQM17f+03RQMAcLgBAIwkRD65M0OJUNiBMyPmXQ8cOICPPvoIr7yiiDqHw4ElS5Zg/fr1WLQotmJgAJBl2fAy0xESIwRBECmKGAig6NCzAIDT5V/FOJcbc+9ej49enYUZ1T/BrK4tqHvsEpxb8nxIHcnhd/6EuYyk1H5MrIjrd58f/1ng4H7kHv0bgAfg39PTRRMuRQMAjBoZYfuLjDCMoVSJXaxbtw6BQACjR4/Wt8myDKfTifPnzyMnJwcA0Nra2ifV0tLSgtzcXADApEmTAAD79+/HvHnzkrP4AQilaQiCIFKUPW+/hNHyabQiE9OvuVPffuEX7sHxz7+MJgzFWOkUhj1/Faqreka45x7+KwDgXEn/E1f7o+RSpSal3L8Xp47W6CmavDBdNDpqZISVUjtNEwgE8Mwzz+BXv/oVqqur9a/du3ejuLgYzz33HEpLS8GyLLZt2xZybENDA06dOoWysjIAwOLFi5Gfn4+f//znYX8XFbASBEEQAxrntt8DAPaNvAHzMrNDHpt0wUI0j3oX+9Z9CVP8nyh1JCd2ofhTSzHZvw+SzGDCp26P+3cXjJmAGucUlPv34fyG5RitpmgmVV4e8Rg9MiKmthj529/+hvPnz2Pp0qV6hEPjxhtvxLp163D33XfjzjvvxH/913/B4XBgxowZqK+vx4MPPojy8nIsXrwYAJCZmYknn3wSX/ziF/G5z30O3/72tzFx4kQ0NzfjpZdeQl1dHV588UU7/sykQpERgiCIFOToJ1sxVdiNgMyi5JrvhN0nv7AIpd/7D7bm3wAAmFf3BBx/vBoAUOOajuGjxiW0htbxnwUAVPiqlTWNWNTvvBLWqURGuP5qRlKAdevWYdGiRX2ECADccMMNqK6uxs6dO/HrX/8ay5YtwwMPPICpU6fiy1/+MkpKSvDmm2/C4eiJBXz+85/Hli1b4HQ6ccstt2Dy5Mm4+eab0draip/+9KfJ/NNsgyIjBEEQKUjzW49hPICPsxfggqKJEfdz8q6QOpIRzDkAQOek6xJew4SFX4a0/xdgGaUIM292+C4aDZZPDzHyxhtvRHzsggsuCClK/eEPfxjT4LLKykq8/PLLpqwvFaHICEEQRIpxrukUZpx7EwCQseDumI7R6kgakY9zyEHZp4130fRm+Khx2O9SCmCjpWgAgHN6AACOFBcjhPlQZIQgCCLFOLDx/zCP8eOQoxRlc2JvI510wUIIU/ci4PchI6tviiEeuqbfDmz/Ho6M/WLELhoNTo2MOGQSI0QoJEYIgiBSCL/gw4RjGwAArdOXGh5Yxrvc4F1u09ZT+dlv4PTMyzF35Lio+3K8GhkhMUL0gtI0BEEQKcTuN/+IETiHZuRh+pVfs3s5AJTOmliM9jin0k3jJDFC9ILECEEQRAqRU/0HAMChsUtMjXAkA4dLmXDqlP0h22kKaWpjxusXlxhZs2YNSkpK4Ha7MXv2bGzevLnf/X0+Hx588EEUFxfD5XJhwoQJWL9+fVwLJgiCGKzs3/4WJgUOQpAdKL3mHruXYxinKp54KJERp9MJQHGuJVIX7fXTXs94MFwzsmHDBqxYsQJr1qzBxRdfjMcffxxXX3019u3bh7Fjx4Y95qabbsLp06exbt06TJw4EU1NTQgEAnEvmiAIYjDSuem3AIDdQ67AnMIim1djHKdLqRnh1cgIx3HIy8tDU1MTACAjI8OQ4y1hL7Iso6urC01NTcjLywMXQ6ouEobFyKOPPoqlS5di2bJlAIDVq1fjX//6F9auXYtVq1b12f+f//wnNm3ahKNHj2LoUMWsady4cf3+Dp/PB5+vZ0JfW1ub0WUSBEGkFadPHsH0tk0AAwz99LftXk5cONU0jQt+yJIEhmVRWFgIALogIVKPvLw8/XWMF0NiRBAE7NixA/fdd1/I9sWLF2PLli1hj3n99ddRWVmJn//853j22WeRmZmJz33uc/if//kfeDyesMesWrUKP/nJT4wsjSAIIq05uvExFDAi9vHTMGX6fLuXExdaZIRlZAh+AbzLDYZhMHLkSIwYMQJ+vz/KGYiBhtPpTCgiomFIjDQ3N0MURRQUFIRsLygoQGNjY9hjjh49ivfeew9utxuvvvoqmpubcdddd+HcuXMR60buv/9+rFy5Uv+5ra0NRUWpF5IkCIIwA29XBybXK664vtlft3k18eNy99yA+rxdIQW4HMeZ8qFGpCZxzRnpndOTZTlink+SJDAMg+eee06f4//oo4/ixhtvxO9+97uw0RGXywWXyxXP0giCINKOj//xB1yIdtQzIzD98i/bvZy4cbl6rvd+X7eNKyEGGoa6afLz88FxXJ8oSFNTU59oicbIkSMxevToEEOh8vJyyLKMkydPxrFkgiCIwYMsSRi+9ykAQN2EL4NzpO6sSoZl4ZOVjguBxAgRhCExwvM8Zs+ejaqqqpDtVVVVmD8/fA7z4osvRn19PTo6OvRtBw8eBMuyGDNmTBxLJgiCGDzsff8NlEjH0SW7UP6Z2HxoBjI+RhEjFBkhgjE8Z2TlypV48sknsX79etTU1ODee+9FXV0dli9fDkCp97j99tv1/W+55RYMGzYMX/va17Bv3z68++67+N73voc77rgjYgErQRBEOhPwCzhWsx2njtag9expiP2MOgh8sBYAsGf4Z5A7JD9ZS7QMPxQxEvDRbBGiB8PxviVLluDs2bN4+OGH0dDQgIqKCmzcuBHFxcUAgIaGBtTV1en7Z2VloaqqCvfccw8qKysxbNgw3HTTTfjpT39q3l9BEASRQuz4/TLMPfvXkG0dsgcdTCa62Cx4HdnwObIhOjJxQeeHAAOMXPwdm1ZrLn7wyr8+r80rIQYSjJwCc3jb2tqQm5uL1tZW5OTk2L0cgiCIuPF2d0J8ZDwyGS+6ZR4eJrpPy273HMy4799JWJ31nPhJOYrketRc/RLK515p93IIi4n18zt1K6EIgiBSkP1b3sBMxosmDEX+jw7DLwbQ3tKMzrZz6G47B1/HOQgdLRC7zkPqboEc8GH8Fd+we9mm4Wd4QAYCFBkhgiAxQhAEkUSEPUp6pnb4pzGC48ByHIaOGI2hI0bbvLLkEGB5QAJEPxWwEj2Qay9BEESSCPgFTGp5FwCQNfN6m1djDwFGqRmRBBIjRA8kRgiCIJLE/q3/RB46cB45KLvwCruXYwsiq4oRP6VpiB5IjBAEQSSJzupXAQCHhiyAw8nbvBp7EDllurbk90XZkxhMkBghCIJIApIoYnzz2wAA1/Qv2Lwa+5DUyIgcoMgI0QOJEYIgiCRwcOfbGI7zaJc9mDzvM3Yvxza0yIhMaRoiCBIjBEEQSaBlh+K6eyD3YrjcGTavxj5kiowQYSAxQhAEYTGyJGHsaWVoGTvlWptXYy+yGhlBgGpGiB5IjBAEQVjM0U8+xCj5NLplHpMvGbz1IgAgO9wAAIbECBEEiRGCIAiLOfPRXwAA+7MuREZWrs2rsRfZoURGGJHECNEDiRGCIAiLKayvAgCIZZ+1eSUDABIjRBhIjBAEQVhI3cFqjJPq4Jc5lC74ot3LsR2GxAgRBhIjBEEQFnLqg5cAADWeWcgdkm/zauyHUWtGOBIjRBAkRgiCICwk/8SbAADvxME7WyQYxqkWsEp+m1dCDCRIjBAEQVhEY90hlAYOQZIZTFhwk93LGRCwqhjhJIqMED2QGCEIgrCIY++9CADY76rAsIIxNq9mYKCJEYck2LwSYiBBYoQgCMIicmr/CQBoG3eVzSsZOPSIEYqMED2QGCEIgrCA5sYTmCzsBQCMW/Alm1czcOB4iowQfSExQhAEYQFHNm8Ay8g46JiEwqKJdi9nwMDxHgCAUyYxQvRAYoQgCMICPEc2AgDOFi22eSUDC4cqRhwyddMQPZAYIQiCMJnWc2dQ3l0NABgzn7pognG6lDSNEyRGiB5IjBAEQZjMwc1/hpMRUcsWo6h0ht3LGVA41JoRHpSmIXogMUIQBGEyjgNvAAAaR19h80oGHk5XBgDARWkaIggSIwRBECbS2d6C8s5tAIARF5IXTW+cLqVmxMX4IUuSzashBgokRgiCIEzkwHuvws34cYopwPipF9q9nAEH787QvxcEr40rIQYSJEYIgiBMRNqnpGhOFCwCw9IltjeuIDHi83bbuBJiIEH/UwiCIEzC5+3C5LYtAIC8yhtsXs3AxOnkIckMAEDwdtm8GmKgQGKEIAjCJPZveR1ZTDeaMBSTZl1m93IGJAzLQoADAOD3kRghFAa1GDlxeA+2//0PqD92wO6lDApkScKhXe+iu7Pd7qUMGo7v34mzp0/avYxBg+/jvwIAavMvA8txNq9m4CIwPADA70ssTdPd2Y5D1ZupEDYNGNRi5Pxf7kXltu/ixEev272UQcGuqudQ+tdr8fFT37Z7KYOCM/XHMOaFT6P18c/YvZRBw/gWJUWTOfMLNq9kYCPACQAICImZ5X3yxDKUvvZZfPLeX81YFmEjg1qMdA4tV745vcfehQwShDOHAQD553fZvJLBwdn6I+AYGeOlY2hurLN7OWmPLEkYIrcCAArH06Cz/tAiIwEhschIZpcS9es49F7CayLsZVCLEX60csHIa91v80oGB7J64SkKnIA/wTsiIjoBb6f+fd3uTTauZHAgCF5wjAwA4DOybF7NwMaviZEE0zROSWkNdp+ja3iqM6jFyPDSSgDAWH8txEDA5tUMAvzKhYdnAqg/utfmxaQ/gaDiQKH2AxtXMjjwdvWIPw+JkX4JMEqaRkwwMsLLyk1NftfRhNdE2MugFiOjx1egS3bBwwg4eeQTu5eT9rCBng/H5qO7bVzJ4ED09Xw45p2l1JjV+Lo7AACCzMHJu2xezcAmoEZGxASHnvFqZGS01ABvV0fC6yLsY1CLEc7hwAlnCQDgzOFtNq8m/WECPXdBQgOJP6sRgyIjJcIh+Gimg6VoYsTLkBCJRoBVxYg/sXStC8rxLCPjxEES3KnMoBYjANCSOxkA4D/5sc0rSX/YQM9dEOV4rUcWesSHi/Gj9hNK1ViJ0K1EonwgMRINkVWeI8mfWJrGLfeImZZjFG1NZQa9GEHhNABA5vl9Ni8k/eHEngsP5XitR/aHRkJaDrxv00oGB36vEhnxUWQkKqIaGZH98adpZEmCG0LPORvpGp7KxCVG1qxZg5KSErjdbsyePRubN2+OuO8777wDhmH6fO3fPzDujPNKLgAAjPIetnkl6Y9D7LnwUI7XerTuJW30Nt9AqUgr0bqX/Izb5pUMfCRVjEgJiBGfrxus2r0EABktBxNeF2EfhsXIhg0bsGLFCjz44IPYtWsXFixYgKuvvhp1df3PMThw4AAaGhr0r9LS0rgXbSZjy+dAlBnko4VmMViMQ+q58FCONwmoIfAjjgkAgDEdn9CkSgvRupcEliIj0ZA49TkKJCBGet3MFHqPJLIkwmYMi5FHH30US5cuxbJly1BeXo7Vq1ejqKgIa9eu7fe4ESNGoLCwUP/iBsioZE9mNk5yowEA9TUf2bya9EarfBfVO3XK8VoL61fu1M/mz0FAZjEC53D6JF2wrSLgUz4cAyxFRqKhiRE5EH8Ba3eXYiuhRf5G4Bxaz51JfHGELRgSI4IgYMeOHVi8eHHI9sWLF2PLli39Hjtr1iyMHDkSl19+Od5+++1+9/X5fGhrawv5spLmzEkAgM46ulO3Ek2MHOeKAVCO12q07iU5YxhqHeMBAKf20PAzq5DVyEiAIzESDVmPjMQvRgS1e6mD8aARwwEA9Yd2Jrw2wh4MiZHm5maIooiCgoKQ7QUFBWhsbAx7zMiRI/HEE0/g5ZdfxiuvvIKysjJcfvnlePfddyP+nlWrViE3N1f/KioqMrJMwwjDKwAAfDMN4rISbUBRc55SNEw5Xmvh1Bodhs/AuaEzAQD+Yx/auKL0RlK7l0QSI1GRHYoYYRJI0wR3L532KGK77ThFW1MVRzwHMQwT8rMsy322aZSVlaGsrEz/ed68eThx4gR++ctf4tJLLw17zP3334+VK1fqP7e1tVkqSDLHzgSOAsM76cPRSrSZABhdCZx7g3K8FsOqYoTlM8COuwg48xcMO19t76LSGFkXIx6bVzLwYdTICCPGHxnRCoZ9jBtdeWVA91agqcaU9RHJx1BkJD8/HxzH9YmCNDU19YmW9MdFF12EQ4cORXzc5XIhJycn5MtKRk2+EAAwRqxHV0erpb9rMKPNBMgvmweAcrxW41BbqVk+A6MrFOE/zn8U3Z3tdi4rbZHVgmHJmWHzSlIAhxI9SkSM+NUJwwLjgnPkFABAdlvkzxViYGNIjPA8j9mzZ6Oqqipke1VVFebPnx/zeXbt2oWRI0ca+dWWkl9YhGbkgWVk1O3fbvdy0hJJFOFm/ACAvBFj0Ih8AJTjtRKnpFzoOVcmCotK0YShcDIiaj8mh1MrYNS5LrKDIiNRcSqREdaEyIifdWPIOMX0dJRQSx1jKYrhbpqVK1fiySefxPr161FTU4N7770XdXV1WL58OQAlxXL77bfr+69evRqvvfYaDh06hL179+L+++/Hyy+/jLvvvtu8v8IE6t1Kq3HrUfpwtAJvd08bnjsjKyjHS5NvrUJzNHW4MsCwLE5mKrVRrYf6LzYn4kMvGHaSGIkGo0ZGWEmIsmdkRDUt5ufcGF06AwGZRR46aERDimK4ZmTJkiU4e/YsHn74YTQ0NKCiogIbN25EcbHSIdHQ0BAyc0QQBHz3u9/FqVOn4PF4MHXqVPz973/HNddcY95fYQKdQ8uB+m3A6T12LyUt8XZ1QAteuz1Z6MqbBHR/BDRRR41VaN1LDncmAEAYVQkcehfuRor+WYFejElpmqiwTlWMiPGLEUnrXmLdcHsycZwbhWLpJBoP7cTwUePMWCaRROIqYL3rrrtw1113hX3s6aefDvn5+9//Pr7//e/H82uSCj96BlAP5LUOjMmw6YZuIiY74eY4OAqnAg2U47USrXvJqYqRvEkXA4ceRXGXMvyMYckNwkw0uwOGIiNRYdRuGk6KP03Tu3vpbMZ4FHecROfJPQCuT3iNRHKhq5HK8NJKAMBYfy3EQMDm1aQfWhue5mhKOV7rcUO5U3e6swAAJdPmQ5AdGIo2nDpKESmz4QI9BcNE/7C8IiAcCaRp9IJhtUbHN1QxPeXOUEdNKkJiRGX0+Ap0yS54GAEnj5C9vdloA4o0R9Mxk2ZClBnK8VqIW1Yu9C6PEhlxuTNwlFcG/DV8QsPPzEazO2BdJEaiwWliRI5fjDCCcoOjiRH3aKUmKq+DfMZSERIjKpzDgRPOEgDAmcNkKGY2fm9PGx4AuD2ZOMWNAgA0UkeN6YiBAFxq95I7I1vf3jJUiUhJJ8j6wGyc6lwXjiIjUeHUVJZD8sd/kl7dS/njZwIAxviPQxLFhNZHJB8SI0G05CphPv9J6vAwG60NTwjy7Wj2KAZuSo6XMJPe3UsarvHKjJfhNPzMdJy9CoaJyDh4VYwkEhnp1b00qmQqvLITHkZAfS2lalINEiPBFCpjyjPPUz7dbEShrxjxDVPEH3uGiobNxhvkaOpy99ypF027DABQLB5He+u5ZC8rrdELhl0kRqKhpWmcCYgRVu1eYtTuJc7hwEnHWADAmaPViS2QSDokRoLIK7kAADDKSzlHsxGD2vA0XKOmAgCGdFBHjdn41ILhLtkV0jWTP6oY9cwIcIyMY7s327W8tEQXI56sKHsSDpcSzeARvxjRu5eC0mLnsyYCALynKNqaapAYCWJs+RyIMoN8tFBRpcmIYRxNKcdrHX6tYFit0QmmPkuJAHYcoeFnZuJWvZd4D0VGouFU0zS8HH/NCBfkvaQh5ivRVv7cgQRWR9gBiZEgPJnZOMmNBgDU11CBn5nI/r4mYqNKpsCn5ngbjlOqxkwEb4+jaW/E0XMAAJmndyR1TemO5r3EU2QkKk63FhmJX4w4NFfqoO4lzxhFaA/rpOh2qkFipBfNmUrrY2fdLptXkmYI2kyAnsiIw8njhJrjbTpMz7eZaN1LPravGBk6+RIAwDjvXopImUTAL4BnlPlEbhIjUeFVAcEzgbjfg1ortYPviUQVliqp9tFiPXzergRXSSQTEiO9EIYrvep8816bV5JeyBFMxM5nKR01lOM1F91ELEyaZtyUC9Elu5CDLpw4WJ3klaUnXrVGBwBcGSRGouF09dyUCL7uuM7BBxlBaowYVYI2ZMDJiKg/QteUVILESC8yi5Q5DMM7D9q8kvRCa8OTHKEzGMRhlOO1goBPqRnxBxUMazh5F2pdZQCA0/veTeq60hVvVzsAQJIZuFw0Dj4awR1ePm98YsQpaxOGe87FsCxOOccBAM5SR01KQWKkF6PKLwQAjBHr0dXRavNq0gdNjKCXb0dPjvdIspeU1mgFw36urxgBgLb8Wco3NPzMFHxdqt0BePL8iQGHwwlRZgAA/jjTKW5djIQWDLflKA7s/gaKbqcS9L+mF/mFY9GMPLCMjLr95G5qFpyapmH4UDFSoOd4T0HweZO+rnSlx0Qs/F26Z8J8AEBBGw34MwPBqxpBhkmLEX1hWBY+8AAAQYjv/71LjtC9NGIKAMDTQtHtVILESBjq3UqvemstFVWaBSuGDijSKBg9Hu2yB05GxKnDu+1YWloi93I07U3x9IXKv9JJtJ49nbR1pSv+frqXiPAIjBMA4I+zZsStzihxZeSEbM8qmg4AGNF9NIHVEcmGxEgYOocoyhqNVABlFlobHttrOiXDsjjFK55AZ2tJjJiFVjAsOcJHRoYMH4kTjOINdGw3meYlir+774Rhon8ENTIS8BlP0/gFH5yM0oXj6tW9NHqSOrxSPo2OtvMJrpJIFiRGwuAcoyjrvFaafWEWuqMp33cgVCvleM2nl716OBpzlPd5Fw0/S5iAj8SIUQJaZEQwHhnp7gr2Xgq9puTlF+IMhgAATh2k6HaqQGIkDMMnKkOhivy1EAMBm1eTHvSYiIX5cBxeDgDwnKeOGrNgVDHSu5U6GGmMUqyd3UwX7EQRVTESrnuJCI+fUSIjYhy1YoIqRkSZAc/3fc4bXUq0tfU41USlCiRGwjBmQgW6ZR4ZjA+njn5i93LSAk2McGFMxCjHaz6RupeCGTH1UgDAeG8NAv74PUKI8N5LRP/oYsRvXIz4tIJhuMJ2L3XmKcMrpdMUbU0VSIyEgXM4UOdUlPWZQ9RRYwb9OZqOCsrxdra3JHNZaQsX6Gsi1pvisgvQLnuQwfhwvIbe54mgdS8FIhQME30JaGIkjjSN0N1/9xJXoNT9ZbVSR02qQGIkAi25yjAu4RQVVZpBxDY8KMWUzcgDAJw8sDOZy0pbInUvhezDcaj1KBft5hpy8E0EvWCYxEjMiKwiRqQ4IiOCWjAczggSAHLHzQQAFPqOxbU2IvmQGIlEgTKMK/Ncjc0LSQ+imYg1UI7XVByqvTrbT2QEADqHK1Ep7tQ2y9eUzuit1P3U6BChJCJG/GqaRoggRsZMmgkAyEcLzjWdim+BRFIhMRKB3PFq6sB7yOaVpD6yJOn26i53eDHSmavlePclbV3pjNZKzbn6FyNZE5XhZyPbSQQmRATvJSIyiYgRrR04UsFwRlYuTjEFAID6QxRtTQVIjERg7ORKSDKDfLSgubHO7uWkND5fNzhGBgDwEUzEWDXHm0k5XlPQC4YjiD+N4hkLIckMRsun0dx4IhlLS0v07qV+CoaJUCROiWrIcYiRWLqXmjyKCWdHHc2LSgVIjEQgIysXJzllKFT9fgphJ4IvaCaAJ4IYySuZCQAY6atNxpLSHq1g2BGmYDiYnLxhOM6NBQCc+JiGn8ULG0P3EhGKJkYQ8Bk+VowSGQEA7xDFDJI5Q6n2VIDESD+cyVRSB5111fYuJMXxqpXvgszByYfP8Y4unQmAcrxmwUvhTcTC0ZSnOFX7jtLws3iJpWCYCEWPjMQhRuQo3ksA4Bw1FQCQ20ap9lSAxEg/CPnKm5k/Q7NGEiFaGx4AZGbnoV7N8TYcqk7GstIaFyJ3L/WGKVKGn+WerbZySWlNTyt19OebUJA5pWYEAeNpGq17SXREjowMU6OtY/zHIEuS4d9BJBcSI/2QOVaxWc/vpDqGRNDb8KKYiJ1Wc7ztddROnSh691KUmhEAGFmhDj8TDpJzcpxwUmwFw0QPstoGzYjxREai2x2MnjANgswhi+nG6ZNH4lskkTRIjPTDqHLljrFIPIWujlabV5O66I6mUezVvUOUtBjleBND6V5SJqrGEhkZM2EampEHF+PHrtd/Z/Xy0hJnjN1LRBAO5XrAxBEZQQx2B7zLjVPcGADA6cM7jP8OIqmQGOmH/MKxaEYeWEbGif30Zo4Xv1ctNmP6HwjlHKnmeNsPW76mdMbn6wardi+5IxQMB8OwLA6XfQMAMGHvb8jpNA707iVK08SOJkZE41YEbCC2VupzmUq0teskjYUf6JAYiUK9eyIAoKWWetXjRdQdTfuPjAwrUQopRwu1lONNgODuJXcMkREAuOD6/8JJZiTy0YJPXvofq5aWtji17qUYCoYJBcap3JywknExonkvRSsYFoYpHTXOZoq2DnRIjEShc4gy/wKN1KseLwFBESPRTMRGT5wBv8whm3K8CdHd1Q4AEGQHHE4+pmN4lxtnLrofADDjxLNoOkUt1kZwGeheIhQYNTLCxlEzohUMg+8/MuIZrZhwDumk68lAh8RIFJxjlDdzXut+m1eSusi+2EzElBzvaADA6cNkax8vsXQvhWPmFbehxjkFHkbAsT8/YMXS0ha9e4nESMxokRFOMi5GYm2lHj5BaUIYEzhBztQDHBIjURg+cQ4AoMhfCzEQsHk1qYmWphFjMBE7q+Z4u09SJCpetO4lb5Tupd4wLAvmyp8CACrP/wNHP9lq+trSFa17yRnBe4noC6uLEeMiQfNeilYwPLJ4ErpkF1yMH6eOktXEQIbESBTGTKhAt8wjg/Hh1FGaNxIPslr53t+AIg1hqJLj5c5SJCpeAnr3knEH2cmVl2NH1mVgGRkdf7vf7KWlJZIowsMoH6iuGGt0CIBVp9XGI0acajSFdfUv/liOw0lnMQDg7FGKtg5kSIxEgXM4UOdUHGXPHNpu82pSFM1ePYbplO7Rilvy0A7qqIkXf4wFw5EovP4RCDKH6d4d+Pidl81cWlriU7vFAMCTmW3jSlILjlfEsiMuMaKkaRwxtFK3ZClNCL56upkcyJAYiYGW3MkAAOnQv2koVBwwMcwE0BgxkXK8iaJFRqK1Ukdi9Phy7Cy8CQCQ/e7DlJ6Mgjeke4nSNLHCORWx7JSN/z/X7A5i6V6ShpcDAFznDhj+PUTyIDESA8yoCwAAc1r+gbZVZfhg/ffIydcAWhteLI6mI4sno1vm4WL8qK+lHG88iIJWMBxfZAQAym/6CdqQiRLpGHbSILR+8WkFw7ITLMfZvJrUwaF2wjjiESNajU4MYiSzSIm2Du+ijpqBDImRGJjxmW/gg+LlaEYe8tGCeXVPIGftTGx/9AYc2P4fu5c34OlxNI0eUg3O8TYfoRxvPEh691L8DrK5wwqwr/ROAMC4j39NE4j7wacVDBvsXhrsOFzK+5OPQ4wY6V4aOVG5mRwlNYREsYiBBYmRGHC5MzDva/8POfcfwPbKX2C/oxw8I6Ky7d8o+9sXcPCnc7D99bUhuWOiB1YfUBTbh+P5rFIAgK+epibGg6Q7msaXptGYdcP3UM8UYDjOY/dL/2vG0tISv1f5gIvmvUSE4lQjI04YTwPq3ksxFAwPKyzCeWSDY2ScJBPOAYsjnoPWrFmDX/ziF2hoaMDUqVOxevVqLFiwIOpx77//PhYuXIiKigpUV1fH86tthXe5UfnZbwCf/QYOVW9Gy9u/xYyWf2NS4CCw8z6c3bkKu8bciNJrV2JYwRi7lztg4NSZACwfm2+HNHwy0LIRnrOfoLn+OAIBAWIgADHggxjwQwr4lX9F5fsRJVORXzjWyj8hpTDSvdQfLncGGub8AKM+WokZx59Gc+Pd9DyHQfNeEigyYgit+NRoZEQSRbgZPwDAFUONDsOyqOfHYYiwB2f3v4ec/NEQAwJEvx+iGICkXVfEACS/AEmWMGHmpXC5yWcomRgWIxs2bMCKFSuwZs0aXHzxxXj88cdx9dVXY9++fRg7NvKFqrW1Fbfffjsuv/xynD59OqFFDwRKZy4AZi7A2dMncfAfv8WEYxswAucw7OQ6NK19Fc3LN5ly4ZYlCefO1Ke0uHGoxWZsjCZimWOmAYeAmV0fAE9Mj7q/T3ai5prnUD73yoTWqXGu6RTyhhWmbP6fUSfeyv3Yq8fKBVd9DQd2Po6ywAHseelB5H/72YTP2ZvW883IyMyGk0/ND3O/WjMiRJkwTITidCnPFw+/oeO6u9qhxUNi8V4CgI6cUqB5D+bWrAJqVkXd//ibYzDk2+8iJ2+YobWFQxJFtJ5rwpDhIxM+VzpjOE3z6KOPYunSpVi2bBnKy8uxevVqFBUVYe3atf0ed+edd+KWW27BvHnz4l7sQGRYwRjM++ojGPLAfuy4cDVOMoUYgXNoWn8L/ILxyYLBSKKIHb++EcPWTkX1Wy+atOLkozuaxhgZKZn1KZxgRgEARJmBIDvQJbvQhkycRw7OYAgakY9TTAHOYAhcjB/D//ENNNcfT3itH77wv8j73VRsXb8y4XPZhj82E7FYYFgW8hXqILSzb+BYjbnt7fs+/Cecq6fg4C8XmXreZBJQa3T8JEYMwas1I05GNNQ5F0/3Uuas69ElK2I3ILPwyk50ym60IhNnkYsmDEUDhuMkU4gO2YNi6SSOPHErJFE08Bf1xS/4sOcXVyL3t+XYv/2thM6V7hiKjAiCgB07duC+++4L2b548WJs2bIl4nFPPfUUjhw5gj/96U/46U9/GvX3+Hw++Hw9H+RtbW1GlmkLTt6F2dd8DXUTZ6HjuaswRdiDD9d9Gxd98/G4z7l13QrMa1fewMz2dcDlXzJruUnFSBseAGTlDEHmD/dCkmVwHAcOQCSHla6OVtQ+ugAl0nHUPPUl5Hz3bfCu+D4Udr35J1y4/xdgGRkTTr0OWfo1GDb1yqr07iWTHGQnz12Mne8uwAWdm9H6+v1AeZUp5z1xeA9G/XMpMhgfpgofo7mxLiXTQJI618Uf51yXwQrv7hHLgq87Zh8lrWC4W+bhiTF6WXHxtZAuaoDMMHCwbL8ffId2vYuxr12PWV1b8MGzD2LeVx+J6Xf0RpYk7Pr9Ulzo3QYwQMu2PwOVl8d1rsGAoSttc3MzRFFEQUFByPaCggI0NjaGPebQoUO477778Nxzz8HhiE37rFq1Crm5ufpXUVGRkWXaythJM3Fw/i8AABedfhHb//6HuM6z7ZXHMK/+Gf3nKV070NIc/jke6Dhl1UTMFfuHI8OyMaVJMrJy4bjlObTLHpT792HXk9+Ka42Hqjej7P2VYBkZADAC53BkT2SBPZBhA5pvR+KREY3hX1gFv8xhRvdH2PPuXxM+X0tzI/DcTchDz13usY82JnxeO5CExLuXBiNaZAQA/AbmN/nj9F5iOS6mm4vSWZdi9/T/BgDMrf193IP/tj73EC4894b+84gzH8R1nsFCXLd9DMOE/CzLcp9tACCKIm655Rb85Cc/waRJk2I+//3334/W1lb968SJE/Es0zYuuPI2fDDqdgDAlI8eRO2+bYaO/2TzXzFz908AAB8ULcMRbjycjIgDb//J9LUmA5fFvh1FE6fhyIJfAwDmnvkLtr/ef8qwN40nDiPvtduQwfjwsbsS1Z6LAABndiT+oWsHnOrbwcSYFouFoonTsGPE9QCAjE0PJRS+9nm7UP/EjSiS69GA4diWexUAQD76jhlLTTo93UskRozgcPLwy8oNh+Drjvk4QbM7sLB76cIbVuCjodeCZWSMfefbqD9mbGDazn8+jYuOPAYA+HDMUgDAeOkYmhtT67MsmRgSI/n5+eA4rk8UpKmpqU+0BADa29uxfft23H333XA4HHA4HHj44Yexe/duOBwO/Oc/4Wd0uFwu5OTkhHylGhfe8Wvscc1CBuOD48+3o63lbEzHHa/ZgbFvLYeTEbE9ZxEu+tovcKb4swCArEOvWbhi69BnAljo2zFz0c34YMwdAICpO36EI3s+jOm4jrbz6HrqBgzHedSyxSj55p/hL70GAJB/KjVnyGjdS2aKEQAou+l/0IYMTBCPYsf/fTmumQ2yJGHPmtsxRdiDdtkD35IX4Z6tpB/HtnwEWZJMXXMykDW7AxMKhgcbfjVhInhjFyN695LFabEZ33gCBx2TkIcOdD17c8zv94M738GUD74LANiafwMuWvYoDnOKAeix7f+wbL2pjiExwvM8Zs+ejaqq0JxxVVUV5s+f32f/nJwc7NmzB9XV1frX8uXLUVZWhurqasydOzex1Q9gOIcDY5a9gEYMR5FcH1Mx1NnTJ+F86UvIQRdqnFNR8c1nwLAsxl2mRFnKfZ+g6VRtMpZvKj0zAawdlX3hV3+Bj92V8DAC3K/cjtZzZ/rdP+AXcGTNF5U7FuTB/ZU/Izt3KErmXw9JZlAqHsaZ+mOWrtkKHAYLhmNlyPCRODDzQYgygzkt/8DJX11q+I7xw6fvQ2VbFQIyi2OXr8W48kqUVl4Bn+xEAc7ixOGPTV1zUjBgd0CE4mOUOhGtCDgWAnortbXiz+XOQM5XXsB55GCieAR7Hl8aVSw3HD+Aoa9/BW7Gj92eCzH7zt8DAJqHK9FW6fDblq45lTGcplm5ciWefPJJrF+/HjU1Nbj33ntRV1eH5cuXA1BSLLffrnx4siyLioqKkK8RI0bA7XajoqICmZnp7XA5ZPhItH9+PXyyE7O6tmDrMw9E3Nfb1YHmP9yAUXITTjKFKPzGy3CrkYTCsaWocU4Fy8g4+s4zEc8xEPELPvCMIsKs9u3gHA4Uf/151DMjMFo+jWN/uCWiAJQlCTsevxMzvNvQLfM497lnMLJYcQzOLyzCIaeSVjz6fuoZxWmt1JyBGp1YmXPd3ahZ9Ef9Ap3x9Kdjzqlvf+NxzKtTCrp3VPw3pl36BQBKe+Zh1xQAQMOuf5q+ZqvRvJckB82lMIofTuVfIfbIiCgkr2C4sGgiTi36nSLAW/+Jj/7yy4j7trWche+PNyIfLTjClWDCN1/Si3IzJiuFq6ka/UsGhsXIkiVLsHr1ajz88MOYOXMm3n33XWzcuBHFxcoI74aGBtTVkW+LRumsS/HxjB8CAOYeexwfv/2XPvtIooh9a25BWWA/WpEJ+ZaX+vSkt038PABgWO0bfY4fyHjVyncAcMU4EyARcocVoOu6p+GVnZjR/RG2Pv2DsPttffF/Mbf5FUgyg/3zf4lJFywMefzcmE8raz76puVrNhtetVePxdE0HioWfB6+O/6jh7Ar3l6KD56+r9/IX83Wf2H6dkWMf1j4Zcz94n+FPN4+6hJl7cfftWTNVsLodgcUGTGKoEdGDIgRze4gSa3UFZd8Dtsm3AMAmLX3Eezf9u8++/gFH46vvRHjpDo0YSiyvvYysnKG6I9PrFwEQXagEM04eZQmS4cjrgLWu+66C8eOHYPP58OOHTtw6aWX6o89/fTTeOeddyIe+9BDD6Xk9NVEmHP9d7B16OfAMjKKN30b9bX7Qx7fuu5eXNCxCYLM4eTiJ1FUOqPPOUo/dSsCMovSwCGcOLwnWUtPGM1ETJIZuFzJuVhPnHEx9sx6CAAw78QfsPs/oTNaqquex4UHfgUA+Kj0O5h15Vf6nKOgUrlrn9y1A92d7Qmt56Pf3Iq9P7skaf4uWvdSrK3U8VA4thRj/+sd/X0979ha7P7VZ9F6vrnPvicPf4LCfywFzwSwM3MBLvz6//XZZ+j0xQCACV27EnZr/ujlX+P4wxWoO1id0HliRbc74EmMGCWgiRF/7N00uhhJ0O7ACHNv/Ql2Zl4KnhEx9O9fDylElSUJO3+/DNN8O5V5SF94FgVjJoQcn5GVi0Nq9K9+Z2J1Iy3NjTj40zn44A/fSeg8A43UG6KQosz8xuM46JiEXHSi+089xVAfvfobzKv/IwBg96z/wdT514Q9fuiI0djnUQyfTm5Ona4aX5dqIgY+qTM75lx3N7bmK90fJe+uxCn1buTw7vcw6b0VYBkZW4d+DnNv+XHY40umzEEjhsPN+HHgg7/FvY7avVtx4bk3MFXYg33vbIj7PEZwa63UbmsjUW5PJuZ++1lsm/6wnops/80lqN27Vd+n9expyM99EUPQjoOOSSi/64WwLdsTpl+CNmQgB104mkBLtSSKGLfnNyiWTqDhzcfiPo8RON17idI0RvGrYkQUYhcjspqmSWb3EsOymHTnMzjOFmEEzuH0+pt10bz1+Z9g7rnXIckMDi54DBNnXBL2HG2jLgYAOI9vTmgt+998EpMCBzH75LNhxX+qQmIkSWjFUOeQgwniUex5fCk+ee91zKp+CIDS/jXnuv5nZPgmK3fro078LWXyjoI3vpkAZjDr62ux31GOHHTC96dbUHewGrmv3ooMxoc9rgtwwfInIwokhmVxPF/xWxL2/T3uNTS984T+PVeTnFZhlwETMTOYc/13UPeFV9GA4RgjN6DgpWux/W9PQPB5cfLxG1Ak16MRwzF02cvwZGaHPQfncOBIpiK2z30cf2rswPZ/YwTOAQAmNP8HYsC4CZtRrGilHiyIbBxixKbupaycIcCSP6FD9mCqsAfbn/w2dv3rj7jwkCJ6Pyr7LmYuujni8UMr1Ohf546E3pd5tcr1iGdEHNyUupO5e0NiJIkUFk1EfVAxVGnVHUoLb/blmHtH5MIojcmfugU+2Yli6SSOfhJb66rd+JMwEyASvMuNoV97AWeRi/HSMRQ8twjDcR7H2LEo/uZfonqheCqUluqSc+/FNVfD29WB8jM9IdkpHVvR0Xbe8HmM4oZyxxaLiZhZlM5cAM/d72GP6wJkMD5Ubv8e6v9fJaYKe9Ahe9B90wtRp6sKY5V0b3b9e3Gvo237S/r3+WjB/o+sr/npKRgmMWKUgCpGJH/sNSOwsWC4uGwmDs3/fwCAi06/gKlblEGJW/Ovx9wvRW5QAIAJM5ToXy46447+NZ44jMn+ffrP/IHUnIUUDhIjSabiks9h28RvAwBcjB81zimouOvZmFIY2blDsTdLaRFr+uC5uNewf9u/cfLwJ3EfbwShW6m3sMtEbMToEjQu/j0CMgsX48dZ5IK//S8xGWCVXXQ1OmU3huM8jnz8vuHfvafqGeSgE/XMCJxgRsHF+LH/3T/H82fEjF/wwal2L7kywkchrCIvvxBTvleFD0Z/DQAwTjqBgMzi6Kd+h5Ipc6IeP+oCZfhZqW9vXHU6YiCA8WcU+4R6Rpl71LHT2ucbAJxqwbAV3UvpjqiLkdgjI1r3kmxTwfCsK7+CD0YqHaM8E8Bu9xzMvvPxqNdwh5PHkYxZAIDmj/8V1+8+9u7zAHre3+VdO6KOMEgVSIzYwNwvP4QPRt6Kas9FKPj6X/QW3piouAEAUNLwr7ju1j9573VM/vsNcP/pM/B5Y+/tjxfRpw0osm8g1NT516B69s+wj5+Gc5//E0aNK4vpOJc7AweyLgQAnN3xmuHfm/WJUttzvPgGnBylOApz+4yfxwjdQYOZPEnoXuoN53Bg3tdXo/ritTjgKEP17FWYftkNMR07ZsI0NCIfPBPA4e3G/W/2f/QmhuM82pCB0/N+BCA5qRqj3ktED6Laniv7YzcV7elesi8SNeeOX2HrsOuwM2shJtz155h9dYTixKJ/Q2qV+rUTk+9ALVsMnhFxIE1SNSRGbIBhWcy783eY+YN/YeiI0YaOnbLwi+iQPSjEGRw06ALp83Yh9y2l1TUfLfjkPy8YOj4eAupkRbsdTSs/901MeeA9lM66NPrOQYiliogY3mBsWNHx/TtR7t+LgMxi4uLlGHGRMmV0SudHlqZqBFWMBGQWzhgvkFYw84pbUPbfH6Hyc8tjPoZhWdTlKeKvs6Zv+2Q0tCjIgbyFmLrwRrQhMympGl6zO6DIiGEkThUjgdgjIz0Fw/Z1LzmcPObe80dc8N3XQ1p4o1E4U43+efcanmBcf+wAygIHIMkMJiy8BY1FVwMAXAdfN3SegQqJkRTDnZGFmjzlA7V1mzExsfOFn6BIrtd/5ndb35UjqZXvyZoJYDYT5n9B+c8vHkXjicMxH9fwtjLca0/mRRg+ahzGT72wJ1Wz6aUoR8ePTysYhislHYfZCZcBAPLPGKuJEgMBTGxWxLlrxo3gXW7sz1Nmx1idquGTXDCcTkhqmkY2kKZhLbI7SAZjS6ejCUPhYvw4ZPBmsu49JUVT45qG/MKxGDVfKZZNl1RN6l2tCLhm3gQAmNj8VswzGU4d3YsLjq0DAHw4Tunamebb2Wfmidn0OJqmphgZOmI0DvLlAIDjH7wS0zHe7k5MPq2EU9lKpX6CYdmeVI2FXTVCnI6mA4Vxc5TW9oniEZw/0xDzcTVb/4FhaEUrMlF+8bUAANcMpbXb6lRNsuwO0hGZU6N3gdjTNJrdAZuCYoRhWRzPVaJ/HQajf8OOKV00HRM/B0Appq1lx6VNqobESApSfvG1OI8cDEMrarZEn4EhSxLOvvQduBg/9rhmYe7tP8Uel1JIdfytxy1dq6yKESlFxQgAnFensbpjnMb6yVvPIQ8daEQ+Ki69Xt9eME+5k5nS+RHaW8+Zv1AAQrfm25GaYiS/sAi17DgAwNFtG2M+rnOnMtn4wJDL9C6p8vnXolVL1WyNr2AwGrIkwa0aQbosnuuSjshae64RMSJpYiQ1I1GMFv1rir2j5tTRGpQGDkGUGUxc2NM+3FikpH1cadBVQ2IkBXHyLhwcpnxAendFD/nv/OcfMd27DYLsQN6Nj4FhWQjTbwUATDj5WsITL/tDVivfxRQ2ERt5oTqNtbs6pimqno+fBQDUjr0enMOhby+ZMkdP1RywqKvG70ttMQIAp/MVA00xRlOxgF9A6VnFYdkz80Z9O+9y44CWqtnV14bBDPx+AQ5GmfnD21AwnOrIas0II8YuRpx6wXBqXlPGVSrRvwmBI2g9ezqmY/QUjXsGhhWM0bdrqZop3TtTPlVDYiRFyZ6jvAknn38nxP+lN+2t51D00cMAgB1jv6qPmq+4/BacRzZG4Bz2bo4t/RAX6oCiVHY0LS67APVMgSIitvTvDXTi0G5MFT6GKDMoueLOkMcYlsXJ0cqdDFfzmiVr1R1NU7RGBwA8kxWhPfr8RzHtX/PBPzAUbTiPbJTP/2zIY+4ZSiePVama4BZkO7qXUh41MmJEjPAWGkEmg/xRxTjGjgXLyDiyLbbR8MOPKymaTjVFo6GlapyMiAObrG9IsBISIynK5DlX4DSGIZvpRs27ke/69j53H0bgHE4yhZh1y8P6dpc7AwcKPgMAkLb/0bJ12j0TwAwYlkVdvlI07I8yjfWUmvb6JONCFBZN7PN4gd5Vs82SVI3WSp0MR1OrmFB5Jfwyh9HyaZw6WhN1/+5qJcp0cOin+rRYll9sbapG817yy1zUIXpEGBzKc8bGIUZ4T3Ln6JhJY74yL8p/KHr07+ThTzBRPIKAzGLSZbf0PZeeqkntrhoSIykKy3GoLVTehPKe8GLk8O73Mee0ksY5f9mqPvNMRl72DQDAtM4P0Vx/3Jp1pomjaeY0pShyfMuWiPNdBJ8XkxqVyIl8wVfD7mN1qibZjqZWkJUzBIf5yQCAU1FMxfyCD6Xn3gEAZM66sc/jTt5laapGEyNe2NdGncowcYgRl1qj40zhuS7usssBAKPPbY2yJ3BSTdHs88zq4+YOAKMvDkrVxJj2GYiQGElh8ucpKnlK+wd97rIlUYT4xr3gGBk7sj+FaQuv73N8cfls7HdOgYORcKjqiT6PmwGbJiZiZXOvRLvsQT5acGjXprD77HnrBQxFG5owFBWX9f1gBKxP1WgFw6lcowMALSMVUzHuWPjnWqNmy98wBO04hxxMvujqsPtYmarRCoZ9KVyjYyeMUxHNRsSI3r2UwgXDE+dciYDMYozcgPpjB/rdd/gJpZDbW/q5sI+PnTQTR7VUzbvJMeO0AhIjKcyEafNxghkFN+PH/ndCW7u2vfJrlAUOoEP2YOzNqyOeo22KoqqLjv0lromu0egxEUvduxhAKYY8lK205J3bFT4cyu9W0l1HxlzX70RGK1M1molYMh1NrWDI1CsAAOM7dvT7vvRWK9GOQ8M+HfE5tzJV49eNIFM3EmUnrCZGpNiK6JXuJdV7KSN1rynB0b+TOyJH/+oOVmOCWAu/zGHSwi9F3O+0mqpxp3CqhsRICsOwLE6OUeo++JqeItTmxhMo3/soAOCTyfdg+KhxEc9RccVX0CF7MEZuxL4PY2+ljBVtJkA6mIiJpcp/+IIw01hPHa3BNN8uSDKD4kX9Tx0tmTIHdexoJVVj8gA0WdBMxFJbjEyYtRCdshtD0B7RFFLweVHWokROsi64KeK5QlI1O819vrWCYT+JkbjQxIgjRjHi83WDZWQAyTWCtILzhfMB9B/9O/W+UpRa47kAefmFEffTUjXlKZyqITGS4oxeoLToTuneqQ+Jqn1hJXLQicPcBFTe+L1+j8/IysXefMXa2rv1adPXl06OpqUXXw9RZjBeOoaG46Gh1bp/rwUAfOKZHdX7hmFZnNIGoO03eT6AVjCc4mLEybtwKGMmgMimYjVb3kAuOtGMPEyee2W/59NSNRPPvm1qqiag1ugIKVwwbCeaGOFiFSNBI9QNeXoNQHLV6F9J+/aI0b/CE0rUxFv2+X7PFZyqOZiiA9BIjKQ4YyfNxGFugvImfPtP+OT9NzCn9U1IMgPpM7+OycBp6CXLAADTWjeZrqp1R9MUT9MAiivtAX4qAKAuaBqrX/ChtF4RFeKs22M6V+E8a1I1bEBtpU7xgmEA8BYtAABknApvKiaoKZoj+Z8OmecSDi1VMwytqNkaWztlLGjdS6lcMGwnnBYZkWMTI161YFhIg+6libMuQ5fswlC0oXbftj6PH6/ZgRLpOASZQ1k/KRoNLVXjOtj/+IGBComRNKC5ROn0yD34MrLfug8AsC3/85h0wcKYjp844xIc4UrgYvyoefNJU9fGy+nlaNo2VqmC9xzrcZXd8/ZLyEcLmpGHik9Fv2gAwLhya1I1TJp0LwFA4SzVVKx7Tx+HaZ+3C2Wt7wIAsiuXRD1XcKpGm9ZqBmKK2x3YDccr71NnjGIk1e0OguFdbhzKUOY+ndn9zz6P13+gRDhqMiqRO3R41POleqqGxEgaMG7hbQCAyYEaFEsncRa5mHzrr2I+nmFZNE9SPkQLDm2ALEmmrY2XUr8NL5iRFypdSZO7d+vuu45dSuHqoVGfi/lujWFZ1I9Su2pMTNVwadK9BCjD5pqRBw8j4PCO0DqdmvdfRw66cAZDMHnOFTGdz61OZzUzVSOrkRGRxEhccC5jkRG9ewmpL0YAoHuMEv3znAyN/smShFEnlQieECVFo5HqqRoSI2lAYdFE7HNW6D/XXvAAcofkGzrH5CuWwis7USIdx8Gd75i2Nm0mAJ8mYmRs6XScZEaCZwI4uOWvaDh+ABXd25XHohSu9qZgnnJHP9VErxo2hU3EesOwLI7lVAIA2vZVhTzm//hlAMCR4YvAclxM5yuf/1m0IMvUVI3eSp3i3Ut24dQjI/6Y9verkZF0aaUumKnUOpV2fwzB1+NcfGz/DhRLJyDIDpRdFlu0FQBOj1Xa210HU6+rhsRImtAxRflg2+Oahdmf/Ybh43OHDseevE8BAFrfX2faurSZAM4Ur3zXYFgWJ0co4X6x5h84XvU4WEbGJ66ZGD1+qqFzjSufg+PsGPBMAAc2mTMfwKG2UqeDGAEAqeQyAMDQ0z2mYt7uTkxu2QwAyKuM3EXTGyfvwsEh5qZqNO8lyUGRkXhw8MrzxiO2yEiP91J6PN/jyufgLHKRwfhweGdP9O/0FqWLZm/mhcjJGxbz+cboqZpdaGluNHexFkNiJE2Y8/m78cmiZzHhnr+CYeN7WTMvugMAUHGuSk9BJIIkivAw6kyAFK98DyZruuJ/MrF1C8affBUA4JsRW+FqMAzLokHvqjHnTkYrGGZT1LejN2MrlTu9if6DaD3fDACo2fwqsplunMYwTKq83ND5grtqzDCIZNLAe8lOnGqXHR9jZERvpU6TgmGGZVGbMwcA0LZXif7JkoTRp5TInTg5thSNRlHpDBzhSlIyVUNiJE1gWBYVl3wOGVm5cZ+jfO6VOMGMQgbjw76qpxNek1b5DgDuNDIRK5uzGG3IwBC0YQTO4TxyUPHpm6MfGIYC1XVzaudHaGs5m/DaNEfTdKnRKSyaiDp2NDhGxtFtSpGf+IkiAGsLrog5RaMRnKrZv7Vv0aBRegqG0yMSlWx4lyLiXBBiqlXTCob9aVSjI49TfK/yGpXo39G9H6FIrodPdqJsYeyRP40mbQDaodTqqiExQugwLItT478IAMitSdwB0hs0E8CTkbqmVr1x8i4cyr5I//lA4bVwueP7MBo3ebaeqjn4buJdNbyUXt1LANAwdC4AQDj4FrxdHShvVYr98uYYv1CHpGp2vZzw2piA8nzLadC6bgdO9f8Nx8gIBKJHR6Q08F7qTVHlNQCAif4DaGs5i6YPlWvvvswLkZ071PD5xgR51aRSqobECBHCxCu+Dr/MoSxwALV7o5s49YduIiY7Dd/BDnTkSVfp34/6tPEaHQ2GZVGvedWYkKrh5fTqXgIAfpKSihl5biv2bX4VmYwXjRiOsgs+Fdf5PEFdNYmmanS7gzRopbYDl7vneRN83VH3l/SC4fQRI4VjS3GCGQUHI+HIR/9AUb0y5E+c+oW4zqelahyMlFKpGhIjRAj5hUXYk6WMKT79zh8SOpfWhpcOMwF6U37ZEuxzVmBr/vUYO2lmQufSBqCZkarpcTRNn7TY+DlXQZQZjJVOwbNdmXR7rHBx3LVRk+d9xrRUDZdG3Ut2oKVpAEDwRhcjPQXD6SX+6ocqvlfubWswRm6AV3ai/NIvxn2+M0VKtMV9KHW6akiMEH1wVH4VAFB+ZiO8qqCIB8GbXjMBgsnMzsOUB9/H3LufSvhcZqZqtO6ldCoYzh2Sj8POSQCAcv9eAMDQC6MPOouEmakavXspDewO7IDlOAiyEjUVfF1R9gagFgynmxjhJ30aQM/7e1/WPGRm58V9vtGXaKma1OmqITFC9GHqguvQiOHIRSc++fef4j6P36u14aWfGDGT4FSNI4EBaJIows0oefdUNxHrzbmC+fr39cwIlM5ckND5tFRN6dn/JJSqcYia3QGJkXgRoFhW+GOIjDBp4r3Um/FzroEkM/rP8tTrEjpf0cRpOMKNV1M1idf/JQMSI0QfOIcDtWOVfKVnz3Nxn0drwxPSqNjMKoK9auJN1XR3tevfp1P3EgDkTOmZsno8gRSNhpaqGYq2hFI1vKR8OKZTwXCyERgnAMAvxCBG1O6ldPBeCiZ36HAcdpYCALpkF8ovvTHhc54pUtriU6WrhsQIEZaSRd+AJDOYKuxG06nauM6hOZqmy0wAK1FSNUXgmQAOvf9qXOcI7l6Kt7tnoDJx9qfQISsfQMMvuiXh8ympmssAAJ27449G6QXDaTLXxQ60yEhA8EbZE2DVNE06tlKfLbwEAFCTMz+hEQ0awakaM+ZGWQ2JESIshWNLcZpRRsqfb4hPjEg+bUARpWmiwbAszmSVAQD8LafiOodPre/plvm0615yuTNw/Mr12HHho5g442JTzimPVEzKnF3xm4ppYoQiI/ETUCMjYgzdNFyaTRgOZvqSH+PDiSsw7tbfmnK+oonT0CF74GAktDTFd01JJv37bhODmi4uGxDPwNcen2+KpDuapldI1SpEPgcAIHe3xHW8P8jRNB2f8anzrzH1fFxGHgDA5W+L+xxuzXspjQqGk42f4QEZCMSQpknn7qXM7DxcdOtPTD1nB5OJLHSjqy3xgYpWQ5ERIiJeTqk7ELriC/FpYkRKo5kAViK5lNAs622N6/ie7iV6vmPBmTkEAOASO6LsGRmte4lPs4LhZBJglTSN6I8uRhyqGGGoeykmulnlfenrMMeI00pIjBARERzK1FSxsyXOE6gDitKs8t0qGE8eAIAT4rtT17qXfJQWiwlXtjLdMkOKT4wE/AJ4JgAgvSYMJ5sAo4qRGGpGHNqEYZp4GxPd6jVc6GixdyExQGKEiIifV+7Upe74IiMymYgZglXFiDPOtIFuIkat1DHhyVbcULPk+MRI8AweV5p1LyUTLTIi+X1R9+VVI0iOCoZjwqeKkUAnRUaIFEZyKTUMiDNtoLXhSQ4KqcaCIzMPAMAH2vvfMQKikF6OplaTqfp+ZMldkETR8PFa95IkM3C5SHDHi6hG8mR/9MiIU9aMIOmaEgsBpyJG4q1DSyYkRoiIyFoNgy++O/UeR1O6UMcCn6V8OHrirGEQfennaGolWblKZIRlZHS0txg+Xrc7AJ/w3JPBjKRHRqKLEVcaei9Ziahew+U4byiTCf0PIiKi1TA4hfjeyKwqRhiexEgsuBOsYdDEiEiRkZhwezLhlZW20o6WZsPHC96e7iUifiROESNyIAYxQt1Lhui5oUxTMbJmzRqUlJTA7XZj9uzZ2Lx5c8R933vvPVx88cUYNmwYPB4PJk+ejF//+tdxL5hIHlrro9MfX9qAUy8uTBoOKLKCnhqG+PyAZDVNQwXDsdPBKB9q3XG0PgpqK3U6ei8lE4lTn78Yakaoe8kYjEcRI444i+KTieE5Ixs2bMCKFSuwZs0aXHzxxXj88cdx9dVXY9++fRg7dmyf/TMzM3H33Xdj+vTpyMzMxHvvvYc777wTmZmZ+MY34rdeJ6wn0dZH3V49DWcCWEFWnjJkLoPxQfB5wbuMRTi0gmFqpY6dTjYb+VILvO3Gi7T93WR3YAayKkZksX8x4hd84BmltsedkWP5utIBLkO5hsd7Q5lMDEdGHn30USxduhTLli1DeXk5Vq9ejaKiIqxduzbs/rNmzcLNN9+MqVOnYty4cbj11ltx5ZVX9htN8fl8aGtrC/kiko/e+ijG90bW2vA4mgkQE1k5Q/Tv2+NIGyBN7dWtJJE5DAEfGUGagSZGECVN0x1kd+DOoDRNLDi0wX6Bgf8ZakiMCIKAHTt2YPHixSHbFy9ejC1btsR0jl27dmHLli1YuHBhxH1WrVqF3Nxc/auoqMjIMgmT0GoYMuNMGzipDc8QnMOBdtV/JZ6JibqjKRUMx4zPoYiRQByD/UQfdS+ZgexQxAgT6D8yIqhiRJQZ8Dw957GgF8XHWYeWTAyJkebmZoiiiIKCgpDtBQUFaGxs7PfYMWPGwOVyobKyEt/61rewbNmyiPvef//9aG1t1b9OnDhhZJmESWTkaK2PnZAlyfDxvDagiCIjMdPBKB+O3XGkDXq6l+j5jhV9lk5Xi+FjtYLhAKXFEsOhPH9MlDSNTysYhou6l2LEk6PUoWVI8d1QJpO4vGkYhgn5WZblPtt6s3nzZnR0dODDDz/Efffdh4kTJ+Lmm28Ou6/L5YLLRaFPu8lWaxicjIjOzjZkZucZOp7XZwJQsVmsdHFZgHgG3jgiI1yAanSMovkBIY45DOS9ZA6MGhlho4gRIch7iWKtsaHdUObIHZAlaUCLOENiJD8/HxzH9YmCNDU19YmW9KakpAQAMG3aNJw+fRoPPfRQRDFCDAw8GdnwyxycjIiO1rOGxYhLpjY8o3i5LEAE/J3GIyOsSN1LRtEG+zFxtD5SwbBJqJERVhL63U2b6+KjGp2YyVRn6fBMAF5vF9wDeFKwIZnE8zxmz56NqqqqkO1VVVWYP39+zOeRZRk+X/Q2LsJeGJbVWx+72owX+FEbnnEEh/LhGIgjbeBIY0dTq9Bm6bBxtD7K5L1kCqxTFSPRummoYNgwWdl5EGUla9HRMrCdew2naVauXInbbrsNlZWVmDdvHp544gnU1dVh+fLlAJR6j1OnTuGZZ54BAPzud7/D2LFjMXnyZADK3JFf/vKXuOeee0z8Mwir6GCyMERuMzyHQZYk3V7dRWmamAnwOUAXIMVRUEndS8bR/YDimcOgFQyTGEkILU3DRYmMUMGwcRiWRTuTiTx0oLO1Gfmjiu1eUkQMi5ElS5bg7NmzePjhh9HQ0ICKigps3LgRxcXKH9nQ0IC6ujp9f0mScP/996O2thYOhwMTJkzAI488gjvvvNO8v4KwjG4uCwgAQoexD0dB8MLFyAAAfgCHBgcaeg2D1/iHo1MXI5QWi5We1kfj7etawTB1LyUGq3bGcFL/kZGAV3m+SYwYo5PJRJ7cge72gW2WF1cB61133YW77ror7GNPP/10yM/33HMPRUFSGJ8qRozWMHi7OvW5lB4SIzEju/MAAGwcI/h7updIjMSK1vrojmOwH6vWjJD3UmJw6vPniBIZkdQJwwESI4boYrMB8TR8Bm8ok83ALa0lBgSCU7lTFw2mDXxq5btf5uDkKccbK/r45jgKKjUx4qSC4ZjRBvtlSsYjI1QwbA6cGhlxyFHEiJqmoe4lY2izdPxxDPZLJiRGiH4JqGkDudvYh6MmRrzgTV9TOsNpNQxxpA00EzFyNI2d4Fk6RqFWanPgVCNNh+zvdz+te0l0UGTECFpRvBhH+3oyITFC9Eu8rY/UhhcfTi1tEIcY0buXqGA4ZrJye/yA/IKxDj9Oou4lM+CcyjXCGSUyIgtkdxAP2g2lRGKESGnUGgbOoBjxq9MSSYwYg89S/Gk8BmsYlO4l5WLuohqdmMlMwA/IKVL3khloE5r5KGKEupfiQ3IpqV+GxAiRymitj0YtqHVHU4YuHEbwZCsfjpmyMTHi83WDVbuXBvJgo4GGw8mjQ/UD6jQ4S8epFwzT850ITtWdmkf/aRo2oKRpSIwYQ3YrkZF4ZukkExIjRL9wmXkAAN5g2kCbCSCwFBkxQoaaNjDqB+QLdjSlAlZD6H5ARsWImhZzUI1OQjhdiriIFhnRWqmpYNgYrEe5wXEI8bmvJwsSI0S/8BnKG9ktGnsjB6gNLy6y9PHNIrq7Yn/OvWrBsCA74HBS0bARulhFTPjajQ32c0lUMGwGTi1Ng0C/ApwNKJEo8BQZMQKnztLhAxQZIVIYrfUxw2gNAzmaxkVGZg4CsvLfsqM19jv1YBMxwhheRzYAQDA4S0frXuJJjCSEFhlhGRl+f+ToCCdSZCQenJnqDWUcRfHJhMQI0S8etfUxE8ZaHzVHU5HEiCGU8c1K2qDLwAh+rXvJCxIjRvGpYiTQ2WLoOM0I0kneSwnhcvdEOnzeroj7OahgOC60G0qPZHywXzIhMUL0S0aOkjbIQRfEQCDm43QTMRpQZJhO1ZzQiB+QX0vTUGTEMAGn8dZHSRSRwajeS1SjkxAuV881QuhPjGit1FQwbAhPtjZLh8QIkcJk5w3Tv+9oiz2MLfu1mQAUGTFKN6dcbI2MbyZH0/gRXZofUEvMxwTfwVP3UmIwLAuf7AQA+H3dEffrsTugyIgRMtUbyiy5C5Io2ryayJAYIfrF5c5At6wURHa2xn6nzvipDS9evKoYCRioYQh4ydE0XnQ/IF/sBX7ekO4lEiOJ4mNUMSJ4I+6jixGq0TFElnpDyTEyOjuM20wkCxIjRFQ61LRBV1vsQ6G0NjxQsZlh/GrawIgYEdW0mJ9qdAzDuFU/IAPmhJrdgU92gnPE5TdKBOGHIkYCvshpGl6m7qV4cGdk6ZEnIzeUyYbECBGVTla58/MasKBm1DY8slc3ju4H5I39w1HSu5fo+TaK1vro9MfebeDTCoYZaqM2A7/qYeX3RY6MUPdS/LSrN5QkRoiUxssp3QZ+A3fqnDotkeHpwmEUSRUjjBExQt1LcePMVAr8XAZaHzW7Ay/o+TYDv5qmCQiRa0Z07yUqGDaMNkvHyA1lsiExQkSlx4LagBgRyUQsXmR1BD9rwA9IKxim7iXjaH5AGQYG+/m9VDBsJn41wiRGECOSKMLNKOPiXVSjY5huVp2l00FihEhh/HG0Pva04ZEYMYrmB+T0G5iYqBcM0526UdzaYD859lk6WsGwQAXDphBgVTHiD++crE0YBqh7KR60WTpGotvJhsQIERUxjhoG3dGUIiOG0cc3G6hhYMjRNG4y1BH82Qb8gLRCS+peMoeAGhmRIkRGqHspMfxORYwYuaFMNiRGiKhoFtSsgTkMTmrDixterWEw4gekdS/J1L1kmB4/oAC83bFFRzQjSD8ZQZqCqEZGJH/4Alate8krO8FyXNLWlS7oRfHd1NpLpDCMmjbgDFhQ6214LhIjRuGzlRoGI+ObWTVNw1D3kmEys3KD/IBi6zaQVDFC3UvmIHKKqIskRjS7g26GIlHxoN1QMgZuKJMNiREiKj01DLHfqfNkrx43nmxtYmLsNQy6iRilxQyj+AGps3RiFSNq95JE3UumIKmRETmiGFHnupD3Ulxos3SM3FAmGxIjRFQcmXkAjLU+ukFtePGSmWt8fDN1LyWGUT8g2U+t1GaiRUbkQPgCVr17idJiccFmKNFWh4EbymRDYoSICp9lvIahZyYAFZsZJStXeb5ZRkZ7jH5AmqMpQ91LcdGtDvbzdcbY+kgFw6Yia5GRQPjIiN69RGmauOBUMeIy0qGXZEiMEFHRWx+l2NIGAb8AnlEcfqny3ThuTya8Bsc3O3QTMXq+48GrtT52tMS0v9a9JFGNjinIamQEESIjokAFw4nAZ+UBADziwHXuJTFCREWrYciO0YI6uCPBnZltyZrSnXZGERVdrbH5AfGSWqNDkZG40Fofxa7YIlE93kuUhjQDbT4OE0mMaHYH1EodF271Gm6kKD7ZkBghopKVlw8A8DBCiHV6JLxdSjpHkhm4XHTnGA9dmh9QjBMTnTK1UidCwOBgPzagFQzT+9sMZIcS8WAipGl0MUI1OnHhUaPbsd5Q2gGJESIqWTlD9O87WqN/OPq6VBMx8GBYeovFg1bDIMSYNnDpjqaUpomHntbH2OYw6N1LNNfFHDQxIoaPjPQUDJP4iwetKD6T8SLgF2xeTXjok4KICudwoF1WLgKdMaQNBM1EjHw74sanpQ1iLKh0UfdSYrjzAMTuB6R1L1ErtTkwWppGivBBqU5mlcjuIC6y84bp38dyQ2kHJEaImOhQaxi626K/kbU2PJoJED+aH5AYY9pA614iE7H4YDxKZCTW1kete4mjGh1TYNTICBclMkLdS/HhcPLolBUh19ESWx1asiExQsREF6e2Psbg3OvvJhOxRNH8gBDD+Ga/4APPKPNISIzEB5eppCL5GGfpaHYHHE0YNgXGqVwr2AiREa1gWKK0WNx0aLN02ikyQqQwXlWMCDGkDQI+sldPFCPjm7uDTcQy6MMxHpyqGHHHKEZ4vZWaPhzNgFXFSKTICKNGRkBiJG66WCX1620fmM69JEaImBAcatqgsyXqvj0mYhQZiRctbRDL+GZBFSOizIDn6TmPB1eWIkYyYmx9JLsDc9HFSITICKd1L9Fcl7jp1m4oY+zQSzYkRoiY0FwfY2l9pDa8xNH8gGKpYfBpBcNwUfdSnHhylPb1zBhbHzUxwlP3kimwaou0Qw4vRlgqGE4YweAsnWRDVy4iJvQahhjSBpqJGPl2xI9DTRu4AjFERlQTMXI0jZ/MXG0OQ2x+QLrdAUVGTIFzKildR4TIiIO8lxLGb3CWTrIhMULEhKy3Pkb/cKSZAInDq2IklvHNmr061ejET5Y6h4FlZHS0t/S7ryxJuhGkO4MmDJsBp0ZGnBEiI5rdAcuT+IsX7YZSJjFCpDJ662MMNQyyFhmhNry4cel+QNHFiJ8KhhMm1A+o/9ZHv1+Ag5EAAHwGpWnMwKGLEX/Yx7XuJYebrinxIruVa3iss3SSDYkRIiY4tYaBj8X1kWYCJEyGWsOQJUc3J9QKhqmVOjG01seuKOaEwd1LHhIjpuB0Ke9dJ8JHRqiVOnEY9RrOCbG7rycTEiNETDizlDt1VwxpA83RVKbK97jJUmsYMhgfBF94vw6NgFd5vsnRNDE6NT+gKK2PWo2OX+bg5Ok5NwOtRdqJ8JERrWDYSWIkblgjN5Q2EJcYWbNmDUpKSuB2uzF79mxs3rw54r6vvPIKrrjiCgwfPhw5OTmYN28e/vWvf8W9YMIe+KzYaxhY3dGUxEi8aDUMANAR5U5d9CmvCTmaJka3OodB6OxfjGhGkF7wlq9psOBUW9JdEdI0mvcSTzU6cePUi+LTJDKyYcMGrFixAg8++CB27dqFBQsW4Oqrr0ZdXV3Y/d99911cccUV2LhxI3bs2IFPfepTuPbaa7Fr166EF08kD0+28kbOiqH1UW/DowFFcWPED0iv0aGC4YTwOZTIiD/KYD+tYNhHNTqm4VTdvV2MH7Ik9XncTa3UCcOr0W13jLN0ko1hMfLoo49i6dKlWLZsGcrLy7F69WoUFRVh7dq1YfdfvXo1vv/972POnDkoLS3Fz372M5SWluKNN95IePFE8sjI7alhCHexCEYfUERteAnRqfkBRUkb6N1LZCKWEHrrY1dL//upc11IjJgH7+65Vvh83SGPyZIEj9q95KIJw3HjVoviM9NBjAiCgB07dmDx4sUh2xcvXowtW7bEdA5JktDe3o6hQ4dG3Mfn86GtrS3ki7AXLW3gZER0d/Uf5uMkmglgBl2scuH1RfGSkHVHU4qMJIKojuCXvf13GwS8WvcSiT+zcAWLEW+oGPH5usEysrIfeS/FTUaOEt3OjqEo3g4MiZHm5maIooiCgoKQ7QUFBWhsbIzpHL/61a/Q2dmJm266KeI+q1atQm5urv5VVFRkZJmEBWRk5iAgK2+X9iiuj05yNDWFbodawxBtfLNWMExiJCEklxIZYaMM9guoE4apYNg8nE4ekswAAPy9IiO+YO8lD0VG4iUrbzgAJRXm7R54giSuAlaGYUJ+lmW5z7ZwvPDCC3jooYewYcMGjBgxIuJ+999/P1pbW/WvEydOxLNMwkQYlkW7mjboauv/w5Ha8MxBUMVIIEragA2ordRUo5MQjDaHIcosHfJeMh+GZSHAAQDwq2JPw6t2LwnUvZQQmVm5EFXB13G+/xtKOzAkRvLz88FxXJ8oSFNTU59oSW82bNiApUuX4qWXXsKiRYv63dflciEnJyfki7CfTnUOgzdK2sCpteFRsVlCBPTxzf2nDRjqXjIFNkMJYzuj+AGJgua9RM+3mQiM0p3UOzKitVJ7qUYnIViOQwej3LB0tvXfoWcHhsQIz/OYPXs2qqqqQrZXVVVh/vz5EY974YUX8NWvfhXPP/88PvOZz8S3UsJ2NNfHaDUMLjUy4iTfjoTQahgQZXwzG6DuJTNwZOQBAFxR5jDI5L1kCQKUCbj+XnN19O4lkBhJlA69KH7gOfc6jB6wcuVK3HbbbaisrMS8efPwxBNPoK6uDsuXLwegpFhOnTqFZ555BoAiRG6//XY89thjuOiii/SoisfjQW5urol/CmE1Xi4bCAD+KHMYXCATMTPQxzcL/UdGOFG5k6SC4cRwZqqtj1Fm6VArtTUIDA/IPTU5Gn4vtVKbRTebBYin4esYeM69hsXIkiVLcPbsWTz88MNoaGhARUUFNm7ciOLiYgBAQ0NDyMyRxx9/HIFAAN/61rfwrW99S9/+la98BU8//XTifwGRNPzObMAHiFFqGNyyD2AAJ1W+J4RWwxDND4gcTc3BnRObH5Ds17qXKDJiJn5VjIh+X+h2r1owTN1LCeN1ZAMi4I9WFG8DhsUIANx111246667wj7WW2C888478fwKYgASiMH1URJFeBjFX8JFle8JwalpA2eUtIFWMMxSwXBCZKhiJDvKYD9GUO7UqXvJXAKMkqYRhdCaEfJeMg/BEdsNpR2QNw0RM1oNA9OP66PP2xNidZOJWELoaYMo45t1R1NqpU6IzBxllk4G44Nf8EXcr6dgmJ5vMwmoBayiEFozElDFn59qdBKm54Zy4Dn3khghYke3oI58p+4NmQlAYiQRYvUD4nV7dYqMJIJmTggAHa2Rw9iMWjBMRpDmEmBVMeIPjYxIas0IeS8ljqSKEUSZpWMHJEaImNFcH/tLG/jUNjyf7ATniCsLSKho45szokxM1B1NSYwkhMPJo0P3A4rc+qgVDJPdgbmI6hA5qVdkRKLuJdPQi+L7iW7bBYkRImY41fWxPwtqn9qG52XI0TRRMtQR/NlyR79+QNS9ZB4d6iydrn7mMHBUMGwJohoZkfyhYqSnYJgiUYmizdJxRKlDswMSI0TM8Oobub/WR91EjGYCJIzmB8QzYr/jmzVHUxfZqydMF6vO0umn28BBrdSWIKliRA70qtdRjSBJjCQOp0a3+7uhtAsSI0TM8NlqDUM/rY89MwEopJoomVm5uh9QR4S0gSSKcDN+AGQiZgZe3Q+oJeI+DlH5sCS7A3OROPUGJhAaGWHIe8k0nFl5AABXYOA595IYIWImQ+02yOqn9VF3NKVis4RR/ICUD7zO1vBeEppvB0DdS2bg0/yAOiNHRpwyFQxbgSZGekdGtO4lKhhOHFeWcg3v74bSLkiMEDHTI0a6IYli2H20yAiZiJmD7gcUwZwwuHsp2IadiA/ND6i/WTq8pBYMU2TEVOQIkRGWWqlNw6MWxfd3Q2kXJEaImNFaH1lGRntb+HHCku5oSjUjZtCt1TBEuFPXupe6ZR4sxyVtXemK6FLFiDdyt4GLIiOWIDuUawbTKzKiiRGq0UmcnqL4zn6L4u2AxAgRM25PJryyMiWxsyV82kAiR1NT0WoY/BG8JIQucjQ1E9kVvfXRrXUv0YRhU2HUyAgjhooR6l4yj+y8fAAAx8jo7BhY7b0kRghDtKuuj5FaHzUxItFMAFPwOxUxEml8s+AlR1MzYdRug/78gLTuJZ4Khs1F9frpLUYc6lA/hiYMJ4zLnQFBVuY/dUS4obQLEiOEIbTWR2+E1kfZTwOKzESrYZAi1DD49YJhEiNm0OMHFH4Ef8AvgGcCAGjCsOk4lfcw20uMONVWagdPkahECS6K74pQh2YXJEYIQ2g1DBFdH6kNz1QkzQ8oQg2D3r1ErdSm4FDFiCsQPjISPO+FupfMhVEjI6wohGzXCoapldocOrUbynYSI0QK43NqrY8tYR/XZgJI1IZnDu48AJFrGESBCobNxJXdf+sjdS9ZB+tUxAgn9YqMqAXDTnq+TUEvim+PPGXYDkiMEIbwq2kDMULagBxNzYXxKJERR4S0gehT0mJ+Khg2Bd0PSAo/8VZQIyNdsgsMS5dPM2FUMcJKoZERF3kvmYo+SydCHZpd0P8mwhAi3/8cBq0Nj0zEzEHzA3JFGN+siRGR5rqYQkaOIkZyIvgBCV7qXrIKVq0ZcfQWI9S9ZCr6DSWJESKV0WoY2Ag1DLqjKUVGTMGpiZEIfkB6wTDV6JhCltr6yDMB+LxdfR7XIiPUvWQ+HK8IaoccKkaoe8lcAlFuKO2CxAhhCC1twEVofeQoMmIqrixFjGREGt8sqDU61L1kCplZuRBlBgDQ0dI3p+4nuwPL4NQ6s+DIiF/wgWeUac/UvWQOkjrYj+lnlo4dkBghDMGqcxicEdIG2kwAjmYCmIInR7lTjzS+WSZHU1OJ5gcU8GndSxQZMRsHr7yHnUGRkZDupUxypTYDRi2K53wDy7mXxAhhCIeaNuAD4QsqndSGZyqZuZqXRFdYPyBGFSNkImYeHepgv+4wrY+iWjNC3kvmw7nUyIjs17dpdgeizIDn6Tk3A32wX4QbSrsgMUIYglfFiCdCDQOvRkYcFBkxhSzVS4JlZHS0t/R5nLqXzEdvfQwzgl8rGA5QWsx0HKrY4NETGfF1KTc9XlD3klk4MvMAAK4IN5R2Qa8uYQiX3voYQYyoxWZkImYOwX5A4cY36zU6FBkxDZ8j8mA/ze6AJgybj1NN0/BBkRGhm7qXzIbPUq7hbhIjRCqTodcwhJ/DoIkR3k3FZmbRodYwdIfxA2JVEzEqGDYPwRF5lg7ZHViH062KkaDIiN69RGLENFxZ/d9Q2gWJEcIQWWoNQwbjg+Dz9nlcb8OjyIhp9Ixv7ps2cJCjqelorY/h/IBkgQqGrYJXU7s8I+r1UX4qGDYdT45Whxb+htIuSIwQhtBqGACgvVfaQJYk3V7dRb4dptHNKl0EvjBpg57uJRJ/ZqH7AYWbw6DZHZAYMR3e3fOcCj7leRZ9mt0BRaLMIitXjW4z3Qj4hSh7Jw8SI4QhOIcD7bJy0ejqlTbw+wU4GGVqpSuD2vDMQqthCHT1jYw4SYyYjuxWIiNsmFk6WsGwTAXDpsO7esSIz6s8zwH1XxIj5qFFtwGgo3XgmOWRGCEM06m3PoZ+OHYHmYh5KDJiGtr4ZinM+Gan3r1EYsQsWI/SMeYQ+hb4sXr3EkVGzMbhcOoD5/zq9FtJNYIMkBgxDSfvQpespL06w9Sh2QWJEcIwXazyweft9UbW2vD8MgcnTzlesxDVtAHCpA20gmEn+XaYBpeRBwDgA30jI7r3EkVGTIdhWfjAA+hJ02jdS9RKbS7t6g1lVxtFRogUplt1ffR3hkZGtAFFXvWCQphDf+ObydHUfDQ/oHCtjxx1L1mKwCht7H51notWMEzeS+bSpc3SCTPYzy5IjBCGESJYUFMbnjUwbtWcMEwNg0dWPhypldo8tFk6njCtj5oRJHUvWYOg3sj41ciITAXDluDllOuFEKYo3i5IjBCGCeg1DKGREb86KpvEiLmwGcqdurOXGFG6l5RqeFcGRUbMwpOttT72FSNOkbyXrCSgRkYCfnVsgBoZkUmMmIpPvYb3vqG0ExIjhGH0GgZv6IdjQHM0ZSi/ayYOtYah9/hmn68bLCMrj5GjqWlk5ijt6+H8gKh7yVr8jBIZEdUZRmyAxIgV+J1KdDvcLB27IDFCGEbW0wahNQwBNc/rZykyYibOTHV8cy8/IF9Q95KbClhNI2uIMoeBY2R0doS+x52a3QGJEUvQxYigFgpTwbAlSOpgP5nECJHKaDUMjl4FlTSgyBrc6sTETCk0MuJVC4YF6l4yFbcnEz7dD+hMyGMuiQqGrSSgiRG/FhlR0zU8RUbMRHLnAQBY38Bx7iUxQhiG02oYeqUNRGrDs4SMCOObyUTMOtpVP6CuttC6KJc6YZinSJQliKwmRpSIiFYwTJERc9FuKLkwRfF2QWKEMEyk1ke9DY+juxgz0WoYMhgf/IJP3651L3lB4s9sevyAQrsNtFZq3kMThq1AEyOyX02HUcGwJbBqHZpT6DsuwC5IjBCG4bMUMeLpVcNAYsQagsc3B/sB+b1kImYVXrZv66MsSchgVO8lioxYgiZGJDVN0+O9RGLETLQ6NFdg4Dj3khghDKO5PmbK4cWI5KA7dTNxOHl0qH5AnUETE7WR2SRGzMcXZrCfT32+AcBNdgeWIHHKe1lWxYhTrdHhXPR8mwmvRrfDzdKxCxIjhGE8eutjJ2RJ0rczfmrDs4oOtYahO0iMUMGwdQjaLJ2gbgNvSPcSfThagS5GAmo6TB3q56CCYVNxZytipHdRvJ3EJUbWrFmDkpISuN1uzJ49G5s3b464b0NDA2655RaUlZWBZVmsWLEi3rUSA4SsXEWM8IyI7q6eNzOjm4hRSNVsuvXxzT1+QAGf8uHop4Jh0xF5JTIih4gR5b3uk53gHA47lpX2aGIEahcNT91LlhB8QzlQMCxGNmzYgBUrVuDBBx/Erl27sGDBAlx99dWoq6sLu7/P58Pw4cPx4IMPYsaMGQkvmLCfzKxcBGTlrRNsQc2oFxCZHE1NR/MDEoLSBpI614UcTc1HUgf7Md6eAj+fVjDMkPeSVcic+tyqkRE3NLsDEiNmkpWnzNJxM354uweGIDEsRh599FEsXboUy5YtQ3l5OVavXo2ioiKsXbs27P7jxo3DY489httvvx25ubkJL5iwH4Zlg1ofe+7U9TY88u0wHa2GIdDZom+T9IJhEiNmw3jyAIS2Pup2B6AaHauQ1fcyI6piRKZWaivIys6DJDMAgI7Ws1H2Tg6GxIggCNixYwcWL14csn3x4sXYsmWLaYvy+Xxoa2sL+SIGFp16DUOwGFEHFZEYMZ1AmPHNZCJmHawqRpz+YDGiGUGS+LMMhyL0mIAXkijCzfgBkN2B2bAchw5GuU53tg4MszxDYqS5uRmiKKKgoCBke0FBARobG01b1KpVq5Cbm6t/FRUVmXZuwhy6OeXD0dfRkzZwkKOpZfT4AbX0bPRr3UskRsxGG+zH+3tqonTvJbI7sA5NjIiCPmEYoO4lK+hglOe0uz0FIyMaDMOE/CzLcp9tiXD//fejtbVV/zpx4oRp5ybMQbOgDnQGixGtDY9CqmYjq2IkeHwzo0ZGqHvJfPgspX09eJaO7r1EkRHLYJzKc8uKPupespguvSh+YERGDJWE5+fng+O4PlGQpqamPtGSRHC5XHC56O5jION3ZgO+UDHipDY8y2A8qh9Q0MRErXtJpu4l09FaHzOCWh/1Vmqq0bEMRo2MsJIAn2Z3IDvh5jg7l5WW+LgsQAydpWMnhiIjPM9j9uzZqKqqCtleVVWF+fPnm7owYmAT4JUPRzmo20Bvw6PIiOnofkBBaQNWdzSlyIjZeLL7+gFJuvcSPd9WoUVGOMnXY3dAQ/0swafO0hG7WuxdiIrhZvmVK1fitttuQ2VlJebNm4cnnngCdXV1WL58OQAlxXLq1Ck888wz+jHV1dUAgI6ODpw5cwbV1dXgeR5Tpkwx568gko5mQR3c+uiiyIhlOFQvCVcgjBihGh3T0VofMxkvAn4BDievixGJIiOWwepiRIDgJe8lK9GK4uVUFSNLlizB2bNn8fDDD6OhoQEVFRXYuHEjiouLAShDznrPHJk1a5b+/Y4dO/D888+juLgYx44dS2z1hH3oFtQ9YsRNjqaWodUwZIg9YoS6l6wj1A/oLIYMHxnkvUQfjlbBqlE+hyT0eC9RwbAlSOGK4m0krjGCd911F+66666wjz399NN9tsmyHM+vIQYwjOb6GNT66JZ9AAPwVGxmOtrExIygtIFTm+tCJmKm43Dy6JTdyGS86GxVxAjI7sByOF4Reg7Jhy7dCJLEnxXIbrUoXhgYozPIm4aIC04VI1rrY8AvgGcCAKjy3QoyVHPC7CA/IM3R1MFTJMoK2vXWR6XbQOtekqhGxzJ0MSILEDW7A4qMWII2Sye4KN5OSIwQceFUXR/datogeKQwzQQwn0zdDyigP9e8RK3UVqL5AXnVOQzkvWQ9DqcmRvwQye7AUjg9uj0wzPJIjBBx4VK7DTQL6uCZAC43XazNJis7D2Kv8c1aK7WTnm9L6FZn6QgdLQCoeykZOFzKc8vLQo8YoRodS3BmaTeUHVH2TA4kRoi48GSHuj5qbXhdsgsMS28rswnxA1LFiEsmR1Mr6Wl9VOYwkPeS9Th5RYw44Yfs1wqGSfxZgStTG+xHkREihcnM1eYwdEESRQiaiRjNBLCM3n5ALupeshTdD0gXI0okisSIdTjUYmyXLACC5r1EkRErcKt1aJlyirr2EgQAZKk1DCwjo73tfM+AInI0tYwuVvUD6lQKKjVHU1dGjm1rSmdEdZaONtjPoYoRqtGxDt6tpmmCIiPUvWQNmbnKLJ3gong7ITFCxIXbkwmv7AQAdLaeDZoJQHcxVuFzKDUM/o4W+AUfeEYEQI6mViH3mqXjlEiMWI1T66ZhJLCCkj6QqGDYErQbSgcjobPD/o4aEiNE3HToNQzNCPi0mQAUGbEKwdFTwxDavUQfjlbQ4wekzGFwqt1LDprrYhlaZAQAOG2gIokRS3B7MiHIiuePVhRvJyRGiLjpVNMG3o5zEL3aTACKjFhFQE0bSN0tuomYKDPgeXrOrUCbw6C1PvJkd2A5vKtHjDj9ihih7iVrUIri1Vk6bSRGiBRGm8MgdLRQG14S0MY3M95W+LrU+S6g7iWr0GbpaH5AWvcS76a0mFU4nDz86t26W53uTAXD1tGlF8Wfs3klJEaIBNBqGMTOc7qJGPl2WIfsViIjrK8VgmavTmkxy9D8gDySIkbcuhihyIiVCFBq0TJERYyQ95J19MzSOW/zSkiMEAng1+YwdLcEzQQgMWIVrEe5U3f42/XuJWqltg63OtgvQ1K6DTQjSBdNGLYUgVHESJasCG6W7A4sw6vWofk7SYwQKYyouT52t+qOphK14VmG7gcUaIefCoYtRxvBny13wu8X4GCU9kcygrQWvxoZyVbFiMNN1xSr8KuzdMTuFnsXAhIjRAJILkVVM94WQDMRIzFiGbofUKAdoipGqGDYOjQx4mL8aDt7Wt/uociIpQgMDwC6+KNWauvQiuJlEiNEKsOoFtSc0KabiNGAIutw6WmDDgS8yvNNYsQ6MrNydT+g8011AICAzMLp5O1cVtoTYEKfXyeJEcsILoq3GxIjRNywGT01DJqJGKjYzDI82dr45g5IAkVGrIblON0PqLP5JACgm7qXLMffS4yQ3YGFqIP9OHWWjp3Q/yoibhxqDYMr0BbkaEpixCoyc7Qahi5IPnXOCJmIWYrmB+Q7p4gRKhi2HpHtLUbI7sAqWH2wH0VGiBTGqbs+dpCJWBLIGqJ4SbCMDLm9CQAgkomYpWizdOS2BgAkRpJBoJcYcdGEYctwqHVofMB+514SI0TcaK6PGVKHbq9OMwGsw+3JhE/1A2I7lA9HKhi2Fq9D6TbQnm+BIfFnNb0jI+S9ZB28VhQvdti8EhIjRAJkqGIkS+6EU3c0JTFiJVoNg9urREaoYNhaND8g7fn2sxQZsRqx13PsppoRy9CK4jNJjBCpjFbDkMH4dGVNbXjW0qmmDbKFZgAkRqwmwCuREe35poJh65G4HjEiyBycPAlAq/Co1/BMmcQIkcJk5Q7Vv8+TFG8DB4kRS/GqYiRPUoytqGDYWiRXHoCe55u8l6xHCkrTkN2BtWSq0e1sphtiIGDrWkiMEHHjcPLokJU78yFQCqCc5NthKT61hkF7vsFTZMRKND8g7fkmuwPrkbkeMeIDiREryc7L17/vaLXXuZfECJEQHUyo+KCZANaijW/WoMiItbCevJCfqZXaeuSgDjHqXrIWJ+9Cl6w8xx2t9jr3khghEqKLDa10J3t1a9HGN2tQwbC1cOpgPw3qXrIeOahmxE/dS5aj3VB2t1NkhEhhtNZHDXI0tRZtfLMG66Ln20o0PyANKhhOAkGREYG6lyxHu6H0tpEYIVIYXy8x4iYxYimaH5CGgyIjluLKChUjoLSY9TiCIiOUFrOcbk65Zgud521dB4kRIiECztC0gZsGFFkK2ytt4KCCYUvRWh81ZCd9OFoNEyRGAtRKbTk+dZZOoLPF1nWQGCESQnT1iBGf7ATncNi4mvSndw0DdS9ZS2YvMUJ2B9bDOHsECHUvWU9ALYqXuikyQqQwclANg5cha3Wr4bPyQn8mMWIpWXmhYoTsDqyHDRIjVDBsPaJaFC9322uWR2KESAgmqPWRZgJYjzs79MORWqmtxZ2RpfsBASRGkgGJkeQiu/MAAKyPxAiRwnAZefr3PmrDsxxP9tCQn8lEzHrag2bpkN2B9QSLEepeSgIeJbrNCW22LoPECJEQjiAxQm141pOZGxoZoe4l6+liewQIdS9ZDxcsRqhg2HI4Nbrt9JMYIVIYV1DagAYUWU92rxoG6l6ynm62p32dupeshwu2OKBWasvhMpVoqytgr1keiREiIdxBaYMAR5ERq3E4eXTKiujzyk6wHGfzitKf4Fk61L1kPZyr56aGociI5WhF8R6x3dZ1kBghEiIjp0eM+Dm6i0kG+vhmikQlhWA/IJ4iUZbjDIqMUI2O9bizlGt4hkSRESKFyQpyfZRoJkBS6FLTBtS9lByC/YBIjFhPcF0OQzU6lpORq1zDs+ROW9dBYoRIiMysXIgyA4AGFCULfXwzFQwnhWA/IKrRsR4n33MdcfAUGbEarSjewwjwebtsWweJESIhGJbVWx+pDS85CGraQKA0TVII9gOi7iXr4V2Upkkm2TlDIKk3lO0t9pnlxSVG1qxZg5KSErjdbsyePRubN2/ud/9NmzZh9uzZcLvdGD9+PH7/+9/HtVhiYNLBKBdoiYrNkoJf9QPyU2QkKQT7AbnclDawGmfQc+yk59tyWI5DB6M8z102OvcaFiMbNmzAihUr8OCDD2LXrl1YsGABrr76atTV1YXdv7a2Ftdccw0WLFiAXbt24YEHHsC3v/1tvPzyywkvnhgYdKsW1NSGlxy08c1kIpYcND+gLtkFhqVgstW43D03NdS9lBy0oviUEiOPPvooli5dimXLlqG8vByrV69GUVER1q5dG3b/3//+9xg7dixWr16N8vJyLFu2DHfccQd++ctfRvwdPp8PbW1tIV/EwMXnUMQIteElB1lNGwSoRicpaK2PPoYiUckgOE1DdgfJQbuhFNrtM8szJEYEQcCOHTuwePHikO2LFy/Gli1bwh7zwQcf9Nn/yiuvxPbt2+H3+8Mes2rVKuTm5upfRUVFRpZJJJludyEAgMseYfNKBgdc7mgAgN81NMqehBnkDB8LAGhjc6PsSZgBy3FoQRZEmUFWHl1TkoGXU+vQuuwTI4b83pubmyGKIgoKCkK2FxQUoLGxMewxjY2NYfcPBAJobm7GyJEj+xxz//33Y+XKlfrPbW1tJEgGMEU3/BRbP7gA06/4it1LGRRUXLUUW0U/xl18o91LGRQUl8/GtlmrkFs01e6lDBpOLf4DfO1nccHwvp8PhPl0ln0BH56vxPCx02xbgyExosEwTMjPsiz32RZt/3DbNVwuF1wuCommCoVjS1E49gd2L2PQkJGVi7lL6PlOJnM+f5fdSxhUTJ1/jd1LGFRceMO9di/BWJomPz8fHMf1iYI0NTX1iX5oFBYWht3f4XBg2LBhYY8hCIIgCGLwYEiM8DyP2bNno6qqKmR7VVUV5s+fH/aYefPm9dn/zTffRGVlJZxOp8HlEgRBEASRbhjuplm5ciWefPJJrF+/HjU1Nbj33ntRV1eH5cuXA1DqPW6//XZ9/+XLl+P48eNYuXIlampqsH79eqxbtw7f/e53zfsrCIIgCIJIWQzXjCxZsgRnz57Fww8/jIaGBlRUVGDjxo0oLi4GADQ0NITMHCkpKcHGjRtx77334ne/+x1GjRqF3/zmN7jhhhvM+ysIgiAIgkhZGFmrJh3AtLW1ITc3F62trcjJyYl+AEEQBEEQthPr5zeNEyQIgiAIwlZIjBAEQRAEYSskRgiCIAiCsBUSIwRBEARB2AqJEYIgCIIgbIXECEEQBEEQtkJihCAIgiAIWyExQhAEQRCErcTl2ptstLlsbW1tNq+EIAiCIIhY0T63o81XTQkx0t7eDgAoKiqyeSUEQRAEQRilvb0dubm5ER9PiXHwkiShvr4e2dnZYBjGtPO2tbWhqKgIJ06coDHzAwx6bQYm9LoMXOi1GZgM9tdFlmW0t7dj1KhRYNnIlSEpERlhWRZjxoyx7Pw5OTmD8k2SCtBrMzCh12XgQq/NwGQwvy79RUQ0qICVIAiCIAhbITFCEARBEIStDGox4nK58OMf/xgul8vupRC9oNdmYEKvy8CFXpuBCb0usZESBawEQRAEQaQvgzoyQhAEQRCE/ZAYIQiCIAjCVkiMEARBEARhKyRGCIIgCIKwFRIjBEEQBEHYyqAWI2vWrEFJSQncbjdmz56NzZs3272kQce7776La6+9FqNGjQLDMHjttddCHpdlGQ899BBGjRoFj8eDyy67DHv37rVnsYOEVatWYc6cOcjOzsaIESNw3XXX4cCBAyH70OtiD2vXrsX06dP1aZ7z5s3DP/7xD/1xel0GBqtWrQLDMFixYoW+jV6b/hm0YmTDhg1YsWIFHnzwQezatQsLFizA1Vdfjbq6OruXNqjo7OzEjBkz8Nvf/jbs4z//+c/x6KOP4re//S22bduGwsJCXHHFFbp5ImE+mzZtwre+9S18+OGHqKqqQiAQwOLFi9HZ2anvQ6+LPYwZMwaPPPIItm/fju3bt+PTn/40Pv/5z+sfavS62M+2bdvwxBNPYPr06SHb6bWJgjxIufDCC+Xly5eHbJs8ebJ833332bQiAoD86quv6j9LkiQXFhbKjzzyiL7N6/XKubm58u9//3sbVjg4aWpqkgHImzZtkmWZXpeBxpAhQ+Qnn3ySXpcBQHt7u1xaWipXVVXJCxculL/zne/Iskz/Z2JhUEZGBEHAjh07sHjx4pDtixcvxpYtW2xaFdGb2tpaNDY2hrxOLpcLCxcupNcpibS2tgIAhg4dCoBel4GCKIp48cUX0dnZiXnz5tHrMgD41re+hc985jNYtGhRyHZ6baKTEq69ZtPc3AxRFFFQUBCyvaCgAI2NjTatiuiN9lqEe52OHz9ux5IGHbIsY+XKlbjkkktQUVEBgF4Xu9mzZw/mzZsHr9eLrKwsvPrqq5gyZYr+oUaviz28+OKL2LlzJ7Zt29bnMfo/E51BKUY0GIYJ+VmW5T7bCPuh18k+7r77bnz88cd47733+jxGr4s9lJWVobq6Gi0tLXj55Zfxla98BZs2bdIfp9cl+Zw4cQLf+c538Oabb8Ltdkfcj16byAzKNE1+fj44jusTBWlqauqjXAn7KCwsBAB6nWzinnvuweuvv463334bY8aM0bfT62IvPM9j4sSJqKysxKpVqzBjxgw89thj9LrYyI4dO9DU1ITZs2fD4XDA4XBg06ZN+M1vfgOHw6E///TaRGZQihGe5zF79mxUVVWFbK+qqsL8+fNtWhXRm5KSEhQWFoa8ToIgYNOmTfQ6WYgsy7j77rvxyiuv4D//+Q9KSkpCHqfXZWAhyzJ8Ph+9LjZy+eWXY8+ePaiurta/Kisr8eUvfxnV1dUYP348vTZRGLRpmpUrV+K2225DZWUl5s2bhyeeeAJ1dXVYvny53UsbVHR0dODw4cP6z7W1taiursbQoUMxduxYrFixAj/72c9QWlqK0tJS/OxnP0NGRgZuueUWG1ed3nzrW9/C888/j7/+9a/Izs7W7+Zyc3Ph8Xj0+Qn0uiSfBx54AFdffTWKiorQ3t6OF198Ee+88w7++c9/0utiI9nZ2XpNlUZmZiaGDRumb6fXJgr2NfLYz+9+9zu5uLhY5nlevuCCC/TWRSJ5vP322zKAPl9f+cpXZFlWWuJ+/OMfy4WFhbLL5ZIvvfRSec+ePfYuOs0J93oAkJ966il9H3pd7OGOO+7Qr1nDhw+XL7/8cvnNN9/UH6fXZeAQ3Nory/TaRIORZVm2SQcRBEEQBEEMzpoRgiAIgiAGDiRGCIIgCIKwFRIjBEEQBEHYCokRgiAIgiBshcQIQRAEQRC2QmKEIAiCIAhbITFCEARBEIStkBghCIIgCMJWSIwQBEEQBGErJEYIgiAIgrAVEiMEQRAEQdjK/wdY1kq8HOBFNQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# {\n",
    "#         'train_years': train_years,\n",
    "#         'test_year': test_year,\n",
    "#         'Year': predict_year,\n",
    "#         'accuracy': accuracy,\n",
    "#         'auc': auc_score,\n",
    "#         'model': classifier,\n",
    "#         'predictions': predictions,\n",
    "#         'predictions_prob': predictions_prob,\n",
    "#         'actual_results': y_actual,\n",
    "# }\n",
    "\n",
    "#make a dictionary of the model: [accuracy]\n",
    "model_accuracy = dict()\n",
    "for result in rolling_window_results:\n",
    "    model_name = result['model'].__class__.__name__\n",
    "    if model_name not in model_accuracy:\n",
    "        model_accuracy[model_name] = []\n",
    "    model_accuracy[model_name].append(result['accuracy'])\n",
    "\n",
    "print(model_accuracy)\n",
    "\n",
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

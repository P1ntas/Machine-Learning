{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline \n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, RocCurveDisplay, make_scorer\n",
    "from sklearn.model_selection import learning_curve\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "import datetime\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_transformed = pd.read_csv('new_data/clean-data_without_outliers.csv')\n",
    "#competition_transformed = pd.read_csv('new_data/clean-comp.csv')\n",
    "#data = pd.read_csv('new_data/clean-data_without_outliers.csv')\n",
    "competition = pd.read_csv('new_data/clean-comp.csv')\n",
    "\n",
    "data = pd.read_csv('new_data/complete-data.csv')\n",
    "#data_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>playerID</th>\n",
       "      <th>year</th>\n",
       "      <th>stint</th>\n",
       "      <th>tmID</th>\n",
       "      <th>GP</th>\n",
       "      <th>oRebounds</th>\n",
       "      <th>dRebounds</th>\n",
       "      <th>dq</th>\n",
       "      <th>PostMinutes</th>\n",
       "      <th>PostPoints</th>\n",
       "      <th>PostoRebounds</th>\n",
       "      <th>PostdRebounds</th>\n",
       "      <th>PostRebounds</th>\n",
       "      <th>PostAssists</th>\n",
       "      <th>PostSteals</th>\n",
       "      <th>PostBlocks</th>\n",
       "      <th>PostTurnovers</th>\n",
       "      <th>PostPF</th>\n",
       "      <th>PostDQ</th>\n",
       "      <th>ft%</th>\n",
       "      <th>fg%</th>\n",
       "      <th>three%</th>\n",
       "      <th>gs%</th>\n",
       "      <th>Postft%</th>\n",
       "      <th>Postfg%</th>\n",
       "      <th>Postthree%</th>\n",
       "      <th>Postgs%</th>\n",
       "      <th>efg%</th>\n",
       "      <th>ts%</th>\n",
       "      <th>ppg</th>\n",
       "      <th>rpg</th>\n",
       "      <th>apg</th>\n",
       "      <th>spg</th>\n",
       "      <th>bpg</th>\n",
       "      <th>eff</th>\n",
       "      <th>pp36</th>\n",
       "      <th>defensive_prowess</th>\n",
       "      <th>defensive_discipline</th>\n",
       "      <th>mpg</th>\n",
       "      <th>pos</th>\n",
       "      <th>college</th>\n",
       "      <th>playoff</th>\n",
       "      <th>confID</th>\n",
       "      <th>playoff_progression</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>award_count</th>\n",
       "      <th>career_year</th>\n",
       "      <th>playoff_progression_rolling</th>\n",
       "      <th>playoff_rolling</th>\n",
       "      <th>pp36_rolling</th>\n",
       "      <th>eff_rolling</th>\n",
       "      <th>award_count_rolling</th>\n",
       "      <th>defensive_prowess_rolling</th>\n",
       "      <th>defensive_discipline_rolling</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>191</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>569</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.90</td>\n",
       "      <td>2.77</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-28.38</td>\n",
       "      <td>10.44</td>\n",
       "      <td>11.5</td>\n",
       "      <td>2.76</td>\n",
       "      <td>9.46</td>\n",
       "      <td>2</td>\n",
       "      <td>615</td>\n",
       "      <td>0.0</td>\n",
       "      <td>702</td>\n",
       "      <td>0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>180</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.90</td>\n",
       "      <td>-86.415</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.90</td>\n",
       "      <td>5.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>720</th>\n",
       "      <td>393</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>567</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1.18</td>\n",
       "      <td>1.96</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.67</td>\n",
       "      <td>0.33</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.80</td>\n",
       "      <td>2.68</td>\n",
       "      <td>3.14</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.50</td>\n",
       "      <td>-82.72</td>\n",
       "      <td>6.48</td>\n",
       "      <td>28.6</td>\n",
       "      <td>5.64</td>\n",
       "      <td>14.86</td>\n",
       "      <td>2</td>\n",
       "      <td>612</td>\n",
       "      <td>1.0</td>\n",
       "      <td>702</td>\n",
       "      <td>1</td>\n",
       "      <td>76.0</td>\n",
       "      <td>174</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.04</td>\n",
       "      <td>-44.760</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.00</td>\n",
       "      <td>6.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>136</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>573</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.40</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.80</td>\n",
       "      <td>1.02</td>\n",
       "      <td>9.40</td>\n",
       "      <td>1.90</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-63.30</td>\n",
       "      <td>16.20</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10.40</td>\n",
       "      <td>20.80</td>\n",
       "      <td>1</td>\n",
       "      <td>588</td>\n",
       "      <td>0.0</td>\n",
       "      <td>701</td>\n",
       "      <td>0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>160</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.52</td>\n",
       "      <td>-181.040</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.75</td>\n",
       "      <td>8.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>133</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>563</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.66</td>\n",
       "      <td>1.04</td>\n",
       "      <td>1.33</td>\n",
       "      <td>0.67</td>\n",
       "      <td>1.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-1.34</td>\n",
       "      <td>11.16</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.34</td>\n",
       "      <td>4.33</td>\n",
       "      <td>1</td>\n",
       "      <td>578</td>\n",
       "      <td>0.0</td>\n",
       "      <td>701</td>\n",
       "      <td>0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>13.68</td>\n",
       "      <td>-68.565</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.85</td>\n",
       "      <td>5.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>615</th>\n",
       "      <td>343</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>561</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1.08</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.66</td>\n",
       "      <td>1.42</td>\n",
       "      <td>1.21</td>\n",
       "      <td>3.04</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-65.74</td>\n",
       "      <td>2.88</td>\n",
       "      <td>16.7</td>\n",
       "      <td>6.84</td>\n",
       "      <td>17.04</td>\n",
       "      <td>1</td>\n",
       "      <td>658</td>\n",
       "      <td>0.0</td>\n",
       "      <td>701</td>\n",
       "      <td>0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>147</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.40</td>\n",
       "      <td>-67.345</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.00</td>\n",
       "      <td>6.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>231</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>745</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>615</td>\n",
       "      <td>0.0</td>\n",
       "      <td>701</td>\n",
       "      <td>0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>185</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>14.04</td>\n",
       "      <td>-133.615</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42.50</td>\n",
       "      <td>6.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>835</th>\n",
       "      <td>442</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>562</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>588</td>\n",
       "      <td>0.0</td>\n",
       "      <td>702</td>\n",
       "      <td>0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>183</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>12.24</td>\n",
       "      <td>-82.975</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.65</td>\n",
       "      <td>3.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>111</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>559</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>598</td>\n",
       "      <td>0.0</td>\n",
       "      <td>702</td>\n",
       "      <td>0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>177</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>14.22</td>\n",
       "      <td>-248.860</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44.40</td>\n",
       "      <td>10.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>218</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>745</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>649</td>\n",
       "      <td>0.0</td>\n",
       "      <td>701</td>\n",
       "      <td>0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>168</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>12.06</td>\n",
       "      <td>-120.655</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.60</td>\n",
       "      <td>5.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>58</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>562</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4</td>\n",
       "      <td>597</td>\n",
       "      <td>0.0</td>\n",
       "      <td>702</td>\n",
       "      <td>0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>210</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.00</td>\n",
       "      <td>-183.630</td>\n",
       "      <td>0.0</td>\n",
       "      <td>48.60</td>\n",
       "      <td>8.82</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1029 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     playerID  year  stint  tmID    GP  oRebounds  dRebounds    dq  \\\n",
       "300       191     2      2   569  13.0       0.38       0.85  0.00   \n",
       "720       393     2      0   567  28.0       1.18       1.96  0.00   \n",
       "223       136     2      1   573  10.0       0.50       1.40  0.10   \n",
       "218       133     2      0   563   3.0       0.00       0.67  0.00   \n",
       "615       343     3      0   561  24.0       0.12       1.08  0.04   \n",
       "..        ...   ...    ...   ...   ...        ...        ...   ...   \n",
       "390       231    11      0   745   0.0       0.00       0.00  0.00   \n",
       "835       442    11      0   562   0.0       0.00       0.00  0.00   \n",
       "169       111    11      0   559   0.0       0.00       0.00  0.00   \n",
       "356       218    11      0   745   0.0       0.00       0.00  0.00   \n",
       "77         58    11      0   562   0.0       0.00       0.00  0.00   \n",
       "\n",
       "     PostMinutes  PostPoints  PostoRebounds  PostdRebounds  PostRebounds  \\\n",
       "300         0.00        0.00            0.0            0.0           0.0   \n",
       "720         7.67        0.33            1.0            1.0           2.0   \n",
       "223         0.00        0.00            0.0            0.0           0.0   \n",
       "218         0.00        0.00            0.0            0.0           0.0   \n",
       "615         0.00        0.00            0.0            0.0           0.0   \n",
       "..           ...         ...            ...            ...           ...   \n",
       "390         0.00        0.00            0.0            0.0           0.0   \n",
       "835         0.00        0.00            0.0            0.0           0.0   \n",
       "169         0.00        0.00            0.0            0.0           0.0   \n",
       "356         0.00        0.00            0.0            0.0           0.0   \n",
       "77          0.00        0.00            0.0            0.0           0.0   \n",
       "\n",
       "     PostAssists  PostSteals  PostBlocks  PostTurnovers  PostPF  PostDQ   ft%  \\\n",
       "300          0.0        0.00         0.0            0.0    0.00     0.0  0.57   \n",
       "720          0.0        0.33         0.0            2.0    0.33     0.0  0.75   \n",
       "223          0.0        0.00         0.0            0.0    0.00     0.0  0.79   \n",
       "218          0.0        0.00         0.0            0.0    0.00     0.0  1.00   \n",
       "615          0.0        0.00         0.0            0.0    0.00     0.0  0.44   \n",
       "..           ...         ...         ...            ...     ...     ...   ...   \n",
       "390          0.0        0.00         0.0            0.0    0.00     0.0  0.00   \n",
       "835          0.0        0.00         0.0            0.0    0.00     0.0  0.00   \n",
       "169          0.0        0.00         0.0            0.0    0.00     0.0  0.00   \n",
       "356          0.0        0.00         0.0            0.0    0.00     0.0  0.00   \n",
       "77           0.0        0.00         0.0            0.0    0.00     0.0  0.00   \n",
       "\n",
       "      fg%  three%   gs%  Postft%  Postfg%  Postthree%  Postgs%  efg%   ts%  \\\n",
       "300  0.41    0.00  0.00      0.0      0.0         0.0      0.0  0.82  0.90   \n",
       "720  0.36    0.25  0.11      0.5      0.0         0.0      0.0  0.74  0.80   \n",
       "223  0.37    0.36  0.90      0.0      0.0         0.0      0.0  0.80  1.02   \n",
       "218  0.33    0.00  0.00      0.0      0.0         0.0      0.0  0.66  1.04   \n",
       "615  0.27    0.21  0.33      0.0      0.0         0.0      0.0  0.62  0.66   \n",
       "..    ...     ...   ...      ...      ...         ...      ...   ...   ...   \n",
       "390  0.00    0.00  0.00      0.0      0.0         0.0      0.0  0.00  0.00   \n",
       "835  0.00    0.00  0.00      0.0      0.0         0.0      0.0  0.00  0.00   \n",
       "169  0.00    0.00  0.00      0.0      0.0         0.0      0.0  0.00  0.00   \n",
       "356  0.00    0.00  0.00      0.0      0.0         0.0      0.0  0.00  0.00   \n",
       "77   0.00    0.00  0.00      0.0      0.0         0.0      0.0  0.00  0.00   \n",
       "\n",
       "      ppg   rpg   apg   spg   bpg    eff   pp36  defensive_prowess  \\\n",
       "300  2.77  1.23  0.31  0.31  0.00 -28.38  10.44               11.5   \n",
       "720  2.68  3.14  0.57  0.39  0.50 -82.72   6.48               28.6   \n",
       "223  9.40  1.90  1.80  0.50  0.10 -63.30  16.20               20.0   \n",
       "218  1.33  0.67  1.33  0.33  0.00  -1.34  11.16               10.0   \n",
       "615  1.42  1.21  3.04  0.42  0.17 -65.74   2.88               16.7   \n",
       "..    ...   ...   ...   ...   ...    ...    ...                ...   \n",
       "390  0.00  0.00  0.00  0.00  0.00   0.00   0.00                0.0   \n",
       "835  0.00  0.00  0.00  0.00  0.00   0.00   0.00                0.0   \n",
       "169  0.00  0.00  0.00  0.00  0.00   0.00   0.00                0.0   \n",
       "356  0.00  0.00  0.00  0.00  0.00   0.00   0.00                0.0   \n",
       "77   0.00  0.00  0.00  0.00  0.00   0.00   0.00                0.0   \n",
       "\n",
       "     defensive_discipline    mpg  pos  college  playoff  confID  \\\n",
       "300                  2.76   9.46    2      615      0.0     702   \n",
       "720                  5.64  14.86    2      612      1.0     702   \n",
       "223                 10.40  20.80    1      588      0.0     701   \n",
       "218                  3.34   4.33    1      578      0.0     701   \n",
       "615                  6.84  17.04    1      658      0.0     701   \n",
       "..                    ...    ...  ...      ...      ...     ...   \n",
       "390                  0.00   0.00    2      615      0.0     701   \n",
       "835                  0.00   0.00    1      588      0.0     702   \n",
       "169                  0.00   0.00    2      598      0.0     702   \n",
       "356                  0.00   0.00    2      649      0.0     701   \n",
       "77                   0.00   0.00    4      597      0.0     702   \n",
       "\n",
       "     playoff_progression  height  weight  award_count  career_year  \\\n",
       "300                    0    73.0     180          0.0            3   \n",
       "720                    1    76.0     174          0.0            3   \n",
       "223                    0    70.0     160          0.0            3   \n",
       "218                    0    69.0     150          0.0            3   \n",
       "615                    0    67.0     147          0.0            3   \n",
       "..                   ...     ...     ...          ...          ...   \n",
       "390                    0    75.0     185          0.0            4   \n",
       "835                    0    75.0     183          0.0            4   \n",
       "169                    0    72.0     177          0.0            6   \n",
       "356                    0    72.0     168          0.0            3   \n",
       "77                     0    78.0     210          0.0            6   \n",
       "\n",
       "     playoff_progression_rolling  playoff_rolling  pp36_rolling  eff_rolling  \\\n",
       "300                          1.5              1.0          9.90      -86.415   \n",
       "720                          0.0              0.0          5.04      -44.760   \n",
       "223                          2.0              1.0         11.52     -181.040   \n",
       "218                          1.0              0.5         13.68      -68.565   \n",
       "615                          1.5              1.0          5.40      -67.345   \n",
       "..                           ...              ...           ...          ...   \n",
       "390                          1.0              0.5         14.04     -133.615   \n",
       "835                          1.0              0.5         12.24      -82.975   \n",
       "169                          0.5              0.5         14.22     -248.860   \n",
       "356                          0.5              0.5         12.06     -120.655   \n",
       "77                           3.0              1.0         18.00     -183.630   \n",
       "\n",
       "     award_count_rolling  defensive_prowess_rolling  \\\n",
       "300                  0.0                      24.90   \n",
       "720                  0.0                      28.00   \n",
       "223                  0.0                      27.75   \n",
       "218                  0.0                      14.85   \n",
       "615                  0.0                      20.00   \n",
       "..                   ...                        ...   \n",
       "390                  0.0                      42.50   \n",
       "835                  0.0                      13.65   \n",
       "169                  0.0                      44.40   \n",
       "356                  0.0                      28.60   \n",
       "77                   0.0                      48.60   \n",
       "\n",
       "     defensive_discipline_rolling  \n",
       "300                          5.02  \n",
       "720                          6.04  \n",
       "223                          8.52  \n",
       "218                          5.68  \n",
       "615                          6.87  \n",
       "..                            ...  \n",
       "390                          6.64  \n",
       "835                          3.15  \n",
       "169                         10.24  \n",
       "356                          5.18  \n",
       "77                           8.82  \n",
       "\n",
       "[1029 rows x 55 columns]"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sort_values('year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['playerID', 'year', 'stint', 'tmID', 'height', 'weight', 'pos',\n",
       "       'college', 'confID'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "competition.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['playerID', 'year', 'stint', 'tmID', 'pos', 'college', 'height', 'weight', 'career_year', 'confID', 'playoff_progression_rolling', 'playoff_rolling', 'pp36_rolling', 'eff_rolling', 'award_count_rolling', 'defensive_prowess_rolling', 'defensive_discipline_rolling']\n"
     ]
    }
   ],
   "source": [
    "input_cols = ['playerID', 'year','stint', 'tmID', 'pos', 'college', 'height', 'weight', 'career_year', 'confID']\n",
    "\n",
    "#add all columns ending in rolling from data to input cols\n",
    "input_cols+=[c for c in data.columns if c.endswith(\"_rolling\")]\n",
    "\n",
    "# The output columns are the genres\n",
    "output_cols = 'playoff'\n",
    "\n",
    "known_columns = ['playerID', 'year', 'stint', 'tmID', 'height', 'weight', 'pos','college', 'confID']\n",
    "\n",
    "# Averages to calculate for precision, recall, and f1-score\n",
    "averages = [None, \"macro\", \"weighted\", \"micro\", \"samples\"]\n",
    "\n",
    "print(input_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>playerID</th>\n",
       "      <th>year</th>\n",
       "      <th>stint</th>\n",
       "      <th>tmID</th>\n",
       "      <th>GP</th>\n",
       "      <th>oRebounds</th>\n",
       "      <th>dRebounds</th>\n",
       "      <th>dq</th>\n",
       "      <th>PostMinutes</th>\n",
       "      <th>PostPoints</th>\n",
       "      <th>PostoRebounds</th>\n",
       "      <th>PostdRebounds</th>\n",
       "      <th>PostRebounds</th>\n",
       "      <th>PostAssists</th>\n",
       "      <th>PostSteals</th>\n",
       "      <th>PostBlocks</th>\n",
       "      <th>PostTurnovers</th>\n",
       "      <th>PostPF</th>\n",
       "      <th>PostDQ</th>\n",
       "      <th>ft%</th>\n",
       "      <th>fg%</th>\n",
       "      <th>three%</th>\n",
       "      <th>gs%</th>\n",
       "      <th>Postft%</th>\n",
       "      <th>Postfg%</th>\n",
       "      <th>Postthree%</th>\n",
       "      <th>Postgs%</th>\n",
       "      <th>efg%</th>\n",
       "      <th>ts%</th>\n",
       "      <th>ppg</th>\n",
       "      <th>rpg</th>\n",
       "      <th>apg</th>\n",
       "      <th>spg</th>\n",
       "      <th>bpg</th>\n",
       "      <th>eff</th>\n",
       "      <th>pp36</th>\n",
       "      <th>defensive_prowess</th>\n",
       "      <th>defensive_discipline</th>\n",
       "      <th>mpg</th>\n",
       "      <th>pos</th>\n",
       "      <th>college</th>\n",
       "      <th>playoff</th>\n",
       "      <th>confID</th>\n",
       "      <th>playoff_progression</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>award_count</th>\n",
       "      <th>career_year</th>\n",
       "      <th>playoff_progression_rolling</th>\n",
       "      <th>playoff_rolling</th>\n",
       "      <th>pp36_rolling</th>\n",
       "      <th>eff_rolling</th>\n",
       "      <th>award_count_rolling</th>\n",
       "      <th>defensive_prowess_rolling</th>\n",
       "      <th>defensive_discipline_rolling</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>555</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.47</td>\n",
       "      <td>3.23</td>\n",
       "      <td>0.00</td>\n",
       "      <td>23.0</td>\n",
       "      <td>7.67</td>\n",
       "      <td>0.33</td>\n",
       "      <td>1.33</td>\n",
       "      <td>1.67</td>\n",
       "      <td>1.33</td>\n",
       "      <td>1.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>2.67</td>\n",
       "      <td>2.67</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.83</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.96</td>\n",
       "      <td>10.60</td>\n",
       "      <td>4.70</td>\n",
       "      <td>2.73</td>\n",
       "      <td>1.47</td>\n",
       "      <td>0.37</td>\n",
       "      <td>-272.13</td>\n",
       "      <td>14.40</td>\n",
       "      <td>50.7</td>\n",
       "      <td>11.26</td>\n",
       "      <td>26.40</td>\n",
       "      <td>2</td>\n",
       "      <td>575</td>\n",
       "      <td>1.0</td>\n",
       "      <td>701</td>\n",
       "      <td>1</td>\n",
       "      <td>74.0</td>\n",
       "      <td>169</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.40</td>\n",
       "      <td>-301.960</td>\n",
       "      <td>0.0</td>\n",
       "      <td>63.35</td>\n",
       "      <td>12.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>555</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.77</td>\n",
       "      <td>2.59</td>\n",
       "      <td>0.00</td>\n",
       "      <td>33.5</td>\n",
       "      <td>10.00</td>\n",
       "      <td>1.50</td>\n",
       "      <td>3.00</td>\n",
       "      <td>4.50</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.50</td>\n",
       "      <td>3.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.92</td>\n",
       "      <td>6.64</td>\n",
       "      <td>3.36</td>\n",
       "      <td>2.05</td>\n",
       "      <td>1.36</td>\n",
       "      <td>0.09</td>\n",
       "      <td>-137.50</td>\n",
       "      <td>11.52</td>\n",
       "      <td>40.5</td>\n",
       "      <td>7.72</td>\n",
       "      <td>21.00</td>\n",
       "      <td>2</td>\n",
       "      <td>575</td>\n",
       "      <td>1.0</td>\n",
       "      <td>701</td>\n",
       "      <td>1</td>\n",
       "      <td>74.0</td>\n",
       "      <td>169</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>14.22</td>\n",
       "      <td>-299.970</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53.70</td>\n",
       "      <td>11.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>555</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.94</td>\n",
       "      <td>2.52</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.98</td>\n",
       "      <td>9.81</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1.94</td>\n",
       "      <td>1.55</td>\n",
       "      <td>0.19</td>\n",
       "      <td>-250.06</td>\n",
       "      <td>14.04</td>\n",
       "      <td>42.6</td>\n",
       "      <td>10.70</td>\n",
       "      <td>25.06</td>\n",
       "      <td>2</td>\n",
       "      <td>575</td>\n",
       "      <td>0.0</td>\n",
       "      <td>701</td>\n",
       "      <td>0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>169</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.96</td>\n",
       "      <td>-204.815</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.60</td>\n",
       "      <td>9.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>555</td>\n",
       "      <td>34.0</td>\n",
       "      <td>1.29</td>\n",
       "      <td>1.82</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.98</td>\n",
       "      <td>7.74</td>\n",
       "      <td>3.12</td>\n",
       "      <td>1.59</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-210.52</td>\n",
       "      <td>12.96</td>\n",
       "      <td>28.5</td>\n",
       "      <td>7.48</td>\n",
       "      <td>21.29</td>\n",
       "      <td>2</td>\n",
       "      <td>575</td>\n",
       "      <td>0.0</td>\n",
       "      <td>701</td>\n",
       "      <td>0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>169</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>12.78</td>\n",
       "      <td>-193.780</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41.55</td>\n",
       "      <td>9.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>555</td>\n",
       "      <td>34.0</td>\n",
       "      <td>1.56</td>\n",
       "      <td>2.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.04</td>\n",
       "      <td>1.08</td>\n",
       "      <td>10.15</td>\n",
       "      <td>4.41</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1.29</td>\n",
       "      <td>0.12</td>\n",
       "      <td>-222.53</td>\n",
       "      <td>14.76</td>\n",
       "      <td>42.6</td>\n",
       "      <td>9.06</td>\n",
       "      <td>24.79</td>\n",
       "      <td>2</td>\n",
       "      <td>575</td>\n",
       "      <td>0.0</td>\n",
       "      <td>701</td>\n",
       "      <td>0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>169</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.50</td>\n",
       "      <td>-230.290</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.55</td>\n",
       "      <td>9.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1024</th>\n",
       "      <td>548</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>564</td>\n",
       "      <td>33.0</td>\n",
       "      <td>2.09</td>\n",
       "      <td>4.42</td>\n",
       "      <td>0.03</td>\n",
       "      <td>32.0</td>\n",
       "      <td>19.33</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.33</td>\n",
       "      <td>5.33</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.67</td>\n",
       "      <td>1.67</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.31</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.94</td>\n",
       "      <td>1.06</td>\n",
       "      <td>18.18</td>\n",
       "      <td>6.52</td>\n",
       "      <td>1.64</td>\n",
       "      <td>1.33</td>\n",
       "      <td>0.48</td>\n",
       "      <td>-340.85</td>\n",
       "      <td>19.44</td>\n",
       "      <td>62.4</td>\n",
       "      <td>7.82</td>\n",
       "      <td>33.73</td>\n",
       "      <td>2</td>\n",
       "      <td>634</td>\n",
       "      <td>1.0</td>\n",
       "      <td>701</td>\n",
       "      <td>1</td>\n",
       "      <td>73.0</td>\n",
       "      <td>165</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.90</td>\n",
       "      <td>-305.255</td>\n",
       "      <td>0.0</td>\n",
       "      <td>58.50</td>\n",
       "      <td>9.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1025</th>\n",
       "      <td>548</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>564</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>634</td>\n",
       "      <td>0.0</td>\n",
       "      <td>701</td>\n",
       "      <td>0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>165</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.62</td>\n",
       "      <td>-320.180</td>\n",
       "      <td>0.0</td>\n",
       "      <td>62.10</td>\n",
       "      <td>8.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1026</th>\n",
       "      <td>549</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>574</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.92</td>\n",
       "      <td>5.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.40</td>\n",
       "      <td>-33.20</td>\n",
       "      <td>14.04</td>\n",
       "      <td>31.0</td>\n",
       "      <td>5.80</td>\n",
       "      <td>12.90</td>\n",
       "      <td>2</td>\n",
       "      <td>700</td>\n",
       "      <td>0.0</td>\n",
       "      <td>702</td>\n",
       "      <td>0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>166</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>13.14</td>\n",
       "      <td>-131.490</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.25</td>\n",
       "      <td>6.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027</th>\n",
       "      <td>549</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>574</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>700</td>\n",
       "      <td>0.0</td>\n",
       "      <td>702</td>\n",
       "      <td>0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>166</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>14.40</td>\n",
       "      <td>-30.555</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.30</td>\n",
       "      <td>4.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1028</th>\n",
       "      <td>550</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>573</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.78</td>\n",
       "      <td>1.20</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-8.20</td>\n",
       "      <td>5.76</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.20</td>\n",
       "      <td>7.40</td>\n",
       "      <td>3</td>\n",
       "      <td>582</td>\n",
       "      <td>0.0</td>\n",
       "      <td>701</td>\n",
       "      <td>0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>174</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.72</td>\n",
       "      <td>-24.625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.30</td>\n",
       "      <td>3.78</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1029 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      playerID  year  stint  tmID    GP  oRebounds  dRebounds    dq  \\\n",
       "0            0     4      0   555  30.0       1.47       3.23  0.00   \n",
       "1            0     5      0   555  22.0       0.77       2.59  0.00   \n",
       "2            0     6      0   555  31.0       0.94       2.52  0.00   \n",
       "3            0     7      0   555  34.0       1.29       1.82  0.00   \n",
       "4            0     8      0   555  34.0       1.56       2.85  0.00   \n",
       "...        ...   ...    ...   ...   ...        ...        ...   ...   \n",
       "1024       548    10      0   564  33.0       2.09       4.42  0.03   \n",
       "1025       548    11      0   564   0.0       0.00       0.00  0.00   \n",
       "1026       549    10      2   574  10.0       1.00       2.00  0.00   \n",
       "1027       549    11      0   574   0.0       0.00       0.00  0.00   \n",
       "1028       550     3      2   573   5.0       0.00       0.60  0.00   \n",
       "\n",
       "      PostMinutes  PostPoints  PostoRebounds  PostdRebounds  PostRebounds  \\\n",
       "0            23.0        7.67           0.33           1.33          1.67   \n",
       "1            33.5       10.00           1.50           3.00          4.50   \n",
       "2             0.0        0.00           0.00           0.00          0.00   \n",
       "3             0.0        0.00           0.00           0.00          0.00   \n",
       "4             0.0        0.00           0.00           0.00          0.00   \n",
       "...           ...         ...            ...            ...           ...   \n",
       "1024         32.0       19.33           2.00           3.33          5.33   \n",
       "1025          0.0        0.00           0.00           0.00          0.00   \n",
       "1026          0.0        0.00           0.00           0.00          0.00   \n",
       "1027          0.0        0.00           0.00           0.00          0.00   \n",
       "1028          0.0        0.00           0.00           0.00          0.00   \n",
       "\n",
       "      PostAssists  PostSteals  PostBlocks  PostTurnovers  PostPF  PostDQ  \\\n",
       "0            1.33        1.33        0.33           2.67    2.67     0.0   \n",
       "1            1.50        0.50        1.00           1.50    3.50     0.0   \n",
       "2            0.00        0.00        0.00           0.00    0.00     0.0   \n",
       "3            0.00        0.00        0.00           0.00    0.00     0.0   \n",
       "4            0.00        0.00        0.00           0.00    0.00     0.0   \n",
       "...           ...         ...         ...            ...     ...     ...   \n",
       "1024         2.00        2.00        0.67           1.67    3.00     0.0   \n",
       "1025         0.00        0.00        0.00           0.00    0.00     0.0   \n",
       "1026         0.00        0.00        0.00           0.00    0.00     0.0   \n",
       "1027         0.00        0.00        0.00           0.00    0.00     0.0   \n",
       "1028         0.00        0.00        0.00           0.00    0.00     0.0   \n",
       "\n",
       "       ft%   fg%  three%   gs%  Postft%  Postfg%  Postthree%  Postgs%  efg%  \\\n",
       "0     0.70  0.39    0.30  0.83     1.00     0.27        0.43      1.0  0.88   \n",
       "1     0.61  0.35    0.38  0.50     0.50     0.35        0.25      1.0  0.84   \n",
       "2     0.73  0.39    0.40  1.00     0.00     0.00        0.00      0.0  0.90   \n",
       "3     0.66  0.41    0.37  0.06     0.00     0.00        0.00      0.0  0.92   \n",
       "4     0.84  0.44    0.45  0.85     0.00     0.00        0.00      0.0  1.04   \n",
       "...    ...   ...     ...   ...      ...      ...         ...      ...   ...   \n",
       "1024  0.77  0.45    0.31  1.00     0.68     0.46        0.50      1.0  0.94   \n",
       "1025  0.00  0.00    0.00  0.00     0.00     0.00        0.00      0.0  0.00   \n",
       "1026  0.89  0.35    0.25  0.60     0.00     0.00        0.00      0.0  0.74   \n",
       "1027  0.00  0.00    0.00  0.00     0.00     0.00        0.00      0.0  0.00   \n",
       "1028  0.50  0.33    0.00  0.00     0.00     0.00        0.00      0.0  0.66   \n",
       "\n",
       "       ts%    ppg   rpg   apg   spg   bpg     eff   pp36  defensive_prowess  \\\n",
       "0     0.96  10.60  4.70  2.73  1.47  0.37 -272.13  14.40               50.7   \n",
       "1     0.92   6.64  3.36  2.05  1.36  0.09 -137.50  11.52               40.5   \n",
       "2     0.98   9.81  3.45  1.94  1.55  0.19 -250.06  14.04               42.6   \n",
       "3     0.98   7.74  3.12  1.59  1.00  0.03 -210.52  12.96               28.5   \n",
       "4     1.08  10.15  4.41  2.50  1.29  0.12 -222.53  14.76               42.6   \n",
       "...    ...    ...   ...   ...   ...   ...     ...    ...                ...   \n",
       "1024  1.06  18.18  6.52  1.64  1.33  0.48 -340.85  19.44               62.4   \n",
       "1025  0.00   0.00  0.00  0.00  0.00  0.00    0.00   0.00                0.0   \n",
       "1026  0.92   5.00  3.00  0.70  0.70  0.40  -33.20  14.04               31.0   \n",
       "1027  0.00   0.00  0.00  0.00  0.00  0.00    0.00   0.00                0.0   \n",
       "1028  0.78   1.20  0.60  0.00  0.00  0.00   -8.20   5.76                6.0   \n",
       "\n",
       "      defensive_discipline    mpg  pos  college  playoff  confID  \\\n",
       "0                    11.26  26.40    2      575      1.0     701   \n",
       "1                     7.72  21.00    2      575      1.0     701   \n",
       "2                    10.70  25.06    2      575      0.0     701   \n",
       "3                     7.48  21.29    2      575      0.0     701   \n",
       "4                     9.06  24.79    2      575      0.0     701   \n",
       "...                    ...    ...  ...      ...      ...     ...   \n",
       "1024                  7.82  33.73    2      634      1.0     701   \n",
       "1025                  0.00   0.00    2      634      0.0     701   \n",
       "1026                  5.80  12.90    2      700      0.0     702   \n",
       "1027                  0.00   0.00    2      700      0.0     702   \n",
       "1028                  3.20   7.40    3      582      0.0     701   \n",
       "\n",
       "      playoff_progression  height  weight  award_count  career_year  \\\n",
       "0                       1    74.0     169          0.0            3   \n",
       "1                       1    74.0     169          0.0            4   \n",
       "2                       0    74.0     169          0.0            5   \n",
       "3                       0    74.0     169          0.0            6   \n",
       "4                       0    74.0     169          0.0            7   \n",
       "...                   ...     ...     ...          ...          ...   \n",
       "1024                    1    73.0     165          0.0            4   \n",
       "1025                    0    73.0     165          0.0            5   \n",
       "1026                    0    74.0     166          0.0            3   \n",
       "1027                    0    74.0     166          0.0            4   \n",
       "1028                    0    78.0     174          0.0            3   \n",
       "\n",
       "      playoff_progression_rolling  playoff_rolling  pp36_rolling  eff_rolling  \\\n",
       "0                             0.0              0.0         14.40     -301.960   \n",
       "1                             0.5              0.5         14.22     -299.970   \n",
       "2                             1.0              1.0         12.96     -204.815   \n",
       "3                             0.5              0.5         12.78     -193.780   \n",
       "4                             0.0              0.0         13.50     -230.290   \n",
       "...                           ...              ...           ...          ...   \n",
       "1024                          2.5              1.0         18.90     -305.255   \n",
       "1025                          2.0              1.0         19.62     -320.180   \n",
       "1026                          0.5              0.5         13.14     -131.490   \n",
       "1027                          0.5              0.5         14.40      -30.555   \n",
       "1028                          0.0              0.0          9.72      -24.625   \n",
       "\n",
       "      award_count_rolling  defensive_prowess_rolling  \\\n",
       "0                     0.0                      63.35   \n",
       "1                     0.0                      53.70   \n",
       "2                     0.0                      45.60   \n",
       "3                     0.0                      41.55   \n",
       "4                     0.0                      35.55   \n",
       "...                   ...                        ...   \n",
       "1024                  0.0                      58.50   \n",
       "1025                  0.0                      62.10   \n",
       "1026                  0.0                      27.25   \n",
       "1027                  0.0                      22.30   \n",
       "1028                  0.0                      12.30   \n",
       "\n",
       "      defensive_discipline_rolling  \n",
       "0                            12.07  \n",
       "1                            11.74  \n",
       "2                             9.49  \n",
       "3                             9.21  \n",
       "4                             9.09  \n",
       "...                            ...  \n",
       "1024                          9.21  \n",
       "1025                          8.24  \n",
       "1026                          6.37  \n",
       "1027                          4.54  \n",
       "1028                          3.78  \n",
       "\n",
       "[1029 rows x 55 columns]"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 20)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "playerID                        False\n",
       "year                            False\n",
       "stint                           False\n",
       "tmID                            False\n",
       "GP                              False\n",
       "                                ...  \n",
       "pp36_rolling                    False\n",
       "eff_rolling                     False\n",
       "award_count_rolling             False\n",
       "defensive_prowess_rolling       False\n",
       "defensive_discipline_rolling    False\n",
       "Length: 55, dtype: bool"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from https://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import learning_curve\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "def plot_learning_curve(\n",
    "    title,\n",
    "    train_sizes, \n",
    "    train_scores, \n",
    "    test_scores, \n",
    "    fit_times,\n",
    "    score_times,\n",
    "    axes=None,\n",
    "    ylim=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate 3 plots: the test and training learning curve, the training\n",
    "    samples vs fit times curve, the fit times vs score curve.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : estimator instance\n",
    "        An estimator instance implementing `fit` and `predict` methods which\n",
    "        will be cloned for each validation.\n",
    "\n",
    "    title : str\n",
    "        Title for the chart.\n",
    "\n",
    "    X : array-like of shape (n_samples, n_features)\n",
    "        Training vector, where ``n_samples`` is the number of samples and\n",
    "        ``n_features`` is the number of features.\n",
    "\n",
    "    y : array-like of shape (n_samples) or (n_samples, n_features)\n",
    "        Target relative to ``X`` for classification or regression;\n",
    "        None for unsupervised learning.\n",
    "\n",
    "    axes : array-like of shape (3,), default=None\n",
    "        Axes to use for plotting the curves.\n",
    "\n",
    "    ylim : tuple of shape (2,), default=None\n",
    "        Defines minimum and maximum y-values plotted, e.g. (ymin, ymax).\n",
    "\n",
    "    cv : int, cross-validation generator or an iterable, default=None\n",
    "        Determines the cross-validation splitting strategy.\n",
    "        Possible inputs for cv are:\n",
    "\n",
    "          - None, to use the default 5-fold cross-validation,\n",
    "          - integer, to specify the number of folds.\n",
    "          - :term:`CV splitter`,\n",
    "          - An iterable yielding (train, test) splits as arrays of indices.\n",
    "\n",
    "        For integer/None inputs, if ``y`` is binary or multiclass,\n",
    "        :class:`StratifiedKFold` used. If the estimator is not a classifier\n",
    "        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n",
    "\n",
    "        Refer :ref:`User Guide <cross_validation>` for the various\n",
    "        cross-validators that can be used here.\n",
    "\n",
    "    n_jobs : int or None, default=None\n",
    "        Number of jobs to run in parallel.\n",
    "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
    "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
    "        for more details.\n",
    "\n",
    "    train_sizes : array-like of shape (n_ticks,)\n",
    "        Relative or absolute numbers of training examples that will be used to\n",
    "        generate the learning curve. If the ``dtype`` is float, it is regarded\n",
    "        as a fraction of the maximum size of the training set (that is\n",
    "        determined by the selected validation method), i.e. it has to be within\n",
    "        (0, 1]. Otherwise it is interpreted as absolute sizes of the training\n",
    "        sets. Note that for classification the number of samples usually have\n",
    "        to be big enough to contain at least one sample from each class.\n",
    "        (default: np.linspace(0.1, 1.0, 5))\n",
    "    \"\"\"\n",
    "    if axes is None:\n",
    "        fig, axes = plt.subplots(3, 2, figsize=(10, 15))\n",
    "\n",
    "    axes = axes.reshape(-1)\n",
    "    fig.subplots_adjust(hspace=0.5, wspace=0.5)\n",
    "    fig = fig.delaxes(axes[-1])\n",
    "    \n",
    "    axes[0].set_title(title)\n",
    "    if ylim is not None:\n",
    "        axes[0].set_ylim(*ylim)\n",
    "    axes[0].set_xlabel(\"Training examples\")\n",
    "    axes[0].set_ylabel(\"Score\")\n",
    "\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    fit_times_mean = np.mean(fit_times, axis=1)\n",
    "    fit_times_std = np.std(fit_times, axis=1)\n",
    "    score_times_mean = np.mean(score_times, axis=1)\n",
    "    score_times_std = np.std(score_times, axis=1)\n",
    "\n",
    "    # Plot learning curve\n",
    "    axes[0].grid()\n",
    "    axes[0].fill_between(\n",
    "        train_sizes,\n",
    "        train_scores_mean - train_scores_std,\n",
    "        train_scores_mean + train_scores_std,\n",
    "        alpha=0.1,\n",
    "        color=\"r\",\n",
    "    )\n",
    "    axes[0].fill_between(\n",
    "        train_sizes,\n",
    "        test_scores_mean - test_scores_std,\n",
    "        test_scores_mean + test_scores_std,\n",
    "        alpha=0.1,\n",
    "        color=\"g\",\n",
    "    )\n",
    "    axes[0].plot(\n",
    "        train_sizes, train_scores_mean, \"o-\", color=\"r\", label=\"Training score\"\n",
    "    )\n",
    "    axes[0].plot(\n",
    "        train_sizes, test_scores_mean, \"o-\", color=\"g\", label=\"Cross-validation score\"\n",
    "    )\n",
    "    axes[0].legend(loc=\"best\")\n",
    "\n",
    "    # Plot n_samples vs fit_times\n",
    "    axes[1].grid()\n",
    "    axes[1].plot(train_sizes, fit_times_mean, \"o-\")\n",
    "    axes[1].fill_between(\n",
    "        train_sizes,\n",
    "        fit_times_mean - fit_times_std,\n",
    "        fit_times_mean + fit_times_std,\n",
    "        alpha=0.1,\n",
    "    )\n",
    "    axes[1].set_xlabel(\"Training examples\")\n",
    "    axes[1].set_ylabel(\"fit_times\")\n",
    "    axes[1].set_title(\"Scalability of the model\")\n",
    "\n",
    "    # Plot fit_time vs score\n",
    "    fit_time_argsort = fit_times_mean.argsort()\n",
    "    fit_time_sorted = fit_times_mean[fit_time_argsort]\n",
    "    test_scores_mean_sorted = test_scores_mean[fit_time_argsort]\n",
    "    test_scores_std_sorted = test_scores_std[fit_time_argsort]\n",
    "    axes[2].grid()\n",
    "    axes[2].plot(fit_time_sorted, test_scores_mean_sorted, \"o-\")\n",
    "    axes[2].fill_between(\n",
    "        fit_time_sorted,\n",
    "        test_scores_mean_sorted - test_scores_std_sorted,\n",
    "        test_scores_mean_sorted + test_scores_std_sorted,\n",
    "        alpha=0.1,\n",
    "    )\n",
    "    axes[2].set_xlabel(\"fit_times\")\n",
    "    axes[2].set_ylabel(\"Score\")\n",
    "    axes[2].set_title(\"Performance of the model\")\n",
    "\n",
    "    # Plot n_samples vs score_times\n",
    "    axes[3].grid()\n",
    "    axes[3].plot(train_sizes, score_times_mean, \"o-\")\n",
    "    axes[3].fill_between(\n",
    "        train_sizes,\n",
    "        score_times_mean - score_times_std,\n",
    "        score_times_mean + score_times_std,\n",
    "        alpha=0.1,\n",
    "    )\n",
    "    axes[3].set_xlabel(\"Training examples\")\n",
    "    axes[3].set_ylabel(\"score_times\")\n",
    "    axes[3].set_title(\"Scalability of the model\")\n",
    "\n",
    "    # Plot score_time vs score\n",
    "    score_time_argsort = score_times_mean.argsort()\n",
    "    score_time_sorted = score_times_mean[score_time_argsort]\n",
    "    test_scores_mean_sorted = test_scores_mean[score_time_argsort]\n",
    "    test_scores_std_sorted = test_scores_std[score_time_argsort]\n",
    "    axes[4].grid()\n",
    "    axes[4].plot(score_time_sorted, test_scores_mean_sorted, \"o-\")\n",
    "    axes[4].fill_between(\n",
    "        score_time_sorted,\n",
    "        test_scores_mean_sorted - test_scores_std_sorted,\n",
    "        test_scores_mean_sorted + test_scores_std_sorted,\n",
    "        alpha=0.1,\n",
    "    )\n",
    "    axes[4].set_xlabel(\"score_times\")\n",
    "    axes[4].set_ylabel(\"Score\")\n",
    "    axes[4].set_title(\"Performance of the model\")\n",
    "\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2  3  4  5  6  7  8  9 10 11]\n"
     ]
    }
   ],
   "source": [
    "model = DecisionTreeClassifier()\n",
    "#loop in rolling window method through all data\n",
    "unique_years = data['year'].unique()\n",
    "#sort the years\n",
    "unique_years.sort()\n",
    "rolling_window_results = []\n",
    "competition_predictions = []\n",
    "trained_models = list()\n",
    "\n",
    "\n",
    "\n",
    "print(unique_years)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_columns_to_remove():\n",
    "    return ['playoff']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GP', 'oRebounds', 'dRebounds', 'dq', 'PostMinutes', 'PostPoints', 'PostoRebounds', 'PostdRebounds', 'PostRebounds', 'PostAssists', 'PostSteals', 'PostBlocks', 'PostTurnovers', 'PostPF', 'PostDQ', 'ft%', 'fg%', 'three%', 'gs%', 'Postft%', 'Postfg%', 'Postthree%', 'Postgs%', 'efg%', 'ts%', 'ppg', 'rpg', 'apg', 'spg', 'bpg', 'eff', 'pp36', 'defensive_prowess', 'defensive_discipline', 'mpg', 'playoff_progression', 'award_count']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "toZeroArr = [c for c in data.columns if c not in input_cols]\n",
    "#add playoff to the list of columns to remove\n",
    "toZeroArr.remove('playoff')\n",
    "print(toZeroArr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_training_data(data, unique_years, i):\n",
    "    train_years_before = unique_years[:i]\n",
    "    train_years = unique_years[:i+1]\n",
    "    test_year = unique_years[i]\n",
    "\n",
    "    if i + 1 > len(unique_years): \n",
    "        return\n",
    "\n",
    "    predict_year = unique_years[i+1]  # Predicting for year i+2 \n",
    "\n",
    "    if predict_year < test_year:\n",
    "        return\n",
    "\n",
    "    # print(\"Train years:\", train_years)\n",
    "    # print(\"Predict Year: \", predict_year)\n",
    "    \n",
    "    train_before = data[data['year'].isin(train_years_before)]\n",
    "    train = data[data['year'].isin(train_years)]\n",
    "    test = data[data['year'] == test_year]\n",
    "    actual = data[data['year'] == predict_year]\n",
    "\n",
    "    return train_before, train, test, actual, predict_year, train_years, test_year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_data(train_before, train, test, actual, data, unique_years):\n",
    "    #one hot encoding\n",
    "    # train_before = pd.get_dummies(train_before, columns=['tmID', 'confID', 'pos', 'college', 'playerID'])\n",
    "    # train = pd.get_dummies(train, columns=['tmID', 'confID', 'pos', 'college', 'playerID']) \n",
    "    # Removing and adding rows based on certain conditions\n",
    "    # train_before_indices_to_remove = []\n",
    "    # train_indices_to_remove = []\n",
    "    # #check if a player have a entry in the train df with the next year\n",
    "    # for index, row in train_before.iterrows():\n",
    "    #     player_id = row['playerID']\n",
    "    #     year = row['year'] + 1 \n",
    "    #     if row['stint'] > 1:\n",
    "    #         train_before_indices_to_remove.append(index)\n",
    "    #     elif not (((train['playerID'] == player_id) & (train['year'] == year)).any()):\n",
    "    #         data_to_add = {\n",
    "    #             'playerID': player_id, \n",
    "    #             'year': year,        \n",
    "    #         }\n",
    "    #         new_row = pd.DataFrame(data_to_add, index=[0])\n",
    "    #         train = pd.concat([train, new_row], ignore_index=True)\n",
    "       \n",
    "\n",
    "    # train_before = train_before.drop(train_before_indices_to_remove)\n",
    "    # for index, row in train.iterrows():\n",
    "    #     player_id = row['playerID']\n",
    "    #     year = row['year'] - 1 \n",
    "    #     if row['stint'] > 1:\n",
    "    #         train_indices_to_remove.append(index)\n",
    "    #     elif not (((train_before['playerID'] == player_id) & (train_before['year'] == year)).any()):\n",
    "    #         data_to_add = {\n",
    "    #             'playerID': player_id, \n",
    "    #             'year': year,        \n",
    "    #         }\n",
    "    #         new_row = pd.DataFrame(data_to_add, index=[0])\n",
    "    #         train_before = pd.concat([train_before , new_row], ignore_index=True)\n",
    "\n",
    "    # train = train_before.drop(train_indices_to_remove)\n",
    "    # train.fillna(0, inplace=True)\n",
    "    # train.sort_values(by='playerID', inplace=True)\n",
    "    # train_before.fillna(0, inplace=True)\n",
    "    # train_before.sort_values(by='playerID', inplace=True)\n",
    "    remove_columns = get_columns_to_remove()\n",
    "    actual[toZeroArr] = 0\n",
    "\n",
    "    #remove stint > 0\n",
    "    train = train[train['stint'] == 0]\n",
    "    actual = actual[actual['stint'] == 0]\n",
    "\n",
    "\n",
    "    X_train = train.drop(remove_columns, axis=1)\n",
    "    y_train = train[output_cols]\n",
    "    X_actual = actual.drop(remove_columns, axis=1)\n",
    "    y_actual = actual[output_cols]\n",
    "\n",
    "    return X_train, y_train, X_actual, y_actual\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_and_evaluate_model(X_train, y_train, model):\n",
    "    classifier = model.fit(X_train, y_train)\n",
    "    return classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_and_evaluate(classifier, X_actual, y_actual):\n",
    "    accuracy = 0.0\n",
    "    auc_score = 0.0\n",
    "    # Evaluate the model\n",
    "    predictions = classifier.predict(X_actual)\n",
    "    predictions_prob = classifier.predict_proba(X_actual)[:, 1]\n",
    "\n",
    "    if X_actual['year'].unique()[0]==11:\n",
    "        competition_predictions.append(predictions)\n",
    "    else:\n",
    "        accuracy = accuracy_score(y_actual, predictions)\n",
    "        auc_score = roc_auc_score(y_actual, predictions_prob)\n",
    "\n",
    "    return accuracy, auc_score, predictions, predictions_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rolling_window_method(i, model, data, unique_years):\n",
    "    train_before, train, test, actual, predict_year, train_years, test_year = prepare_training_data(data, unique_years, i)\n",
    "    \n",
    "    if train_before is None:\n",
    "        return None\n",
    "\n",
    "    X_train, y_train, X_actual, y_actual= process_data(train_before, train, test, actual, data, unique_years)\n",
    "\n",
    "    #print years in X_train and X_actual\n",
    "    print(\"Unique years in X_train:\", X_train['year'].unique())\n",
    "    print(\"Unique years in X_actual:\", X_actual['year'].unique())\n",
    "    \n",
    "    #create a csv file for each year, storing the data for that year and that model, in the folder output/rolling_window\n",
    "    #X_train.to_csv(f'output/rolling_window/{model.__class__.__name__}_{test_year}_train.csv', index=False)\n",
    "    #X_test.to_csv(f'output/rolling_window/{model.__class__.__name__}_{test_year}_test.csv', index=False)\n",
    "    #X_actual.to_csv(f'output/rolling_window/{model.__class__.__name__}_{test_year}_actual.csv', index=False)\n",
    "    # y_actual.to_csv(f'output/rolling_window/{model.__class__.__name__}_{test_year}_y_actual.csv', index=False)\n",
    "\n",
    "\n",
    "    classifier = train_and_evaluate_model(X_train, y_train, model)\n",
    "\n",
    "    # Calculate accuracy\n",
    "\n",
    "    accuracy, auc_score, predictions, predictions_prob = predict_and_evaluate(classifier, X_actual, y_actual)\n",
    "\n",
    "    #print(f\"Best params for {model.__class__.__name_}: {model.best_params_}\")\n",
    "    # print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    # print(f\"AUC: {auc_score:.4f}\")\n",
    "\n",
    "\n",
    "    return {\n",
    "        'train_years': train_years,\n",
    "        'test_year': test_year,\n",
    "        'Year': predict_year,\n",
    "        'accuracy': accuracy,\n",
    "        'auc': auc_score,\n",
    "        'model': classifier,\n",
    "        'predictions': predictions,\n",
    "        'predictions_prob': predictions_prob,\n",
    "        'actual_results': y_actual,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def model_training_loop(model, data, unique_years):\n",
    "    rolling_window_results = []\n",
    "    for i in range(len(unique_years) - 1):\n",
    "        result= rolling_window_method(i, model, data, unique_years)\n",
    "        if result:\n",
    "            rolling_window_results.append(result)\n",
    "    return rolling_window_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = DecisionTreeClassifier()\n",
    "# rolling_window_results += model_training_loop(model, data, unique_years)\n",
    "# trained_models.append(rolling_window_results[-1]['model'])  # Get the last trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique years in X_train: [2]\n",
      "Unique years in X_actual: [3]\n",
      "Unique years in X_train: [3 2]\n",
      "Unique years in X_actual: [4]\n",
      "Unique years in X_train: [4 3 2]\n",
      "Unique years in X_actual: [5]\n",
      "Unique years in X_train: [4 5 3 2]\n",
      "Unique years in X_actual: [6]\n",
      "Unique years in X_train: [4 5 6 3 2]\n",
      "Unique years in X_actual: [7]\n",
      "Unique years in X_train: [4 5 6 7 3 2]\n",
      "Unique years in X_actual: [8]\n",
      "Unique years in X_train: [4 5 6 7 8 3 2]\n",
      "Unique years in X_actual: [9]\n",
      "Unique years in X_train: [4 5 6 7 8 9 3 2]\n",
      "Unique years in X_actual: [10]\n",
      "Unique years in X_train: [ 4  5  6  7  8  9  3 10  2]\n",
      "Unique years in X_actual: [11]\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier()\n",
    "rolling_window_results += model_training_loop(model, data, unique_years)\n",
    "trained_models.append(rolling_window_results[-1]['model'])  # Get the last trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique years in X_train: [2]\n",
      "Unique years in X_actual: [3]\n",
      "Unique years in X_train: [3 2]\n",
      "Unique years in X_actual: [4]\n",
      "Unique years in X_train: [4 3 2]\n",
      "Unique years in X_actual: [5]\n",
      "Unique years in X_train: [4 5 3 2]\n",
      "Unique years in X_actual: [6]\n",
      "Unique years in X_train: [4 5 6 3 2]\n",
      "Unique years in X_actual: [7]\n",
      "Unique years in X_train: [4 5 6 7 3 2]\n",
      "Unique years in X_actual: [8]\n",
      "Unique years in X_train: [4 5 6 7 8 3 2]\n",
      "Unique years in X_actual: [9]\n",
      "Unique years in X_train: [4 5 6 7 8 9 3 2]\n",
      "Unique years in X_actual: [10]\n",
      "Unique years in X_train: [ 4  5  6  7  8  9  3 10  2]\n",
      "Unique years in X_actual: [11]\n"
     ]
    }
   ],
   "source": [
    "model = GradientBoostingClassifier()\n",
    "rolling_window_results += model_training_loop(model, data, unique_years)\n",
    "trained_models.append(rolling_window_results[-1]['model'])  # Get the last trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique years in X_train: [2]\n",
      "Unique years in X_actual: [3]\n",
      "Unique years in X_train: [3 2]\n",
      "Unique years in X_actual: [4]\n",
      "Unique years in X_train: [4 3 2]\n",
      "Unique years in X_actual: [5]\n",
      "Unique years in X_train: [4 5 3 2]\n",
      "Unique years in X_actual: [6]\n",
      "Unique years in X_train: [4 5 6 3 2]\n",
      "Unique years in X_actual: [7]\n",
      "Unique years in X_train: [4 5 6 7 3 2]\n",
      "Unique years in X_actual: [8]\n",
      "Unique years in X_train: [4 5 6 7 8 3 2]\n",
      "Unique years in X_actual: [9]\n",
      "Unique years in X_train: [4 5 6 7 8 9 3 2]\n",
      "Unique years in X_actual: [10]\n",
      "Unique years in X_train: [ 4  5  6  7  8  9  3 10  2]\n",
      "Unique years in X_actual: [11]\n"
     ]
    }
   ],
   "source": [
    "#gaussian naive bayes\n",
    "model = GaussianNB()\n",
    "rolling_window_results += model_training_loop(model, data, unique_years)\n",
    "trained_models.append(rolling_window_results[-1]['model'])  # Get the last trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique years in X_train: [2]\n",
      "Unique years in X_actual: [3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique years in X_train: [3 2]\n",
      "Unique years in X_actual: [4]\n",
      "Unique years in X_train: [4 3 2]\n",
      "Unique years in X_actual: [5]\n",
      "Unique years in X_train: [4 5 3 2]\n",
      "Unique years in X_actual: [6]\n",
      "Unique years in X_train: [4 5 6 3 2]\n",
      "Unique years in X_actual: [7]\n",
      "Unique years in X_train: [4 5 6 7 3 2]\n",
      "Unique years in X_actual: [8]\n",
      "Unique years in X_train: [4 5 6 7 8 3 2]\n",
      "Unique years in X_actual: [9]\n",
      "Unique years in X_train: [4 5 6 7 8 9 3 2]\n",
      "Unique years in X_actual: [10]\n",
      "Unique years in X_train: [ 4  5  6  7  8  9  3 10  2]\n",
      "Unique years in X_actual: [11]\n"
     ]
    }
   ],
   "source": [
    "#logistic regression\n",
    "model = LogisticRegression()\n",
    "rolling_window_results += model_training_loop(model, data, unique_years)\n",
    "trained_models.append(rolling_window_results[-1]['model'])  # Get the last trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique years in X_train: [2]\n",
      "Unique years in X_actual: [3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique years in X_train: [3 2]\n",
      "Unique years in X_actual: [4]\n",
      "Unique years in X_train: [4 3 2]\n",
      "Unique years in X_actual: [5]\n",
      "Unique years in X_train: [4 5 3 2]\n",
      "Unique years in X_actual: [6]\n",
      "Unique years in X_train: [4 5 6 3 2]\n",
      "Unique years in X_actual: [7]\n",
      "Unique years in X_train: [4 5 6 7 3 2]\n",
      "Unique years in X_actual: [8]\n",
      "Unique years in X_train: [4 5 6 7 8 3 2]\n",
      "Unique years in X_actual: [9]\n",
      "Unique years in X_train: [4 5 6 7 8 9 3 2]\n",
      "Unique years in X_actual: [10]\n",
      "Unique years in X_train: [ 4  5  6  7  8  9  3 10  2]\n",
      "Unique years in X_actual: [11]\n"
     ]
    }
   ],
   "source": [
    "#ada boost\n",
    "model = AdaBoostClassifier()\n",
    "rolling_window_results += model_training_loop(model, data, unique_years)\n",
    "trained_models.append(rolling_window_results[-1]['model'])  # Get the last trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique years in X_train: [2]\n",
      "Unique years in X_actual: [3]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot have number of splits n_splits=3 greater than the number of samples: n_samples=2.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/juanbellon/Documents/feup/ac/src/6-predictive.ipynb Cell 25\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/juanbellon/Documents/feup/ac/src/6-predictive.ipynb#X33sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m level1 \u001b[39m=\u001b[39m RandomForestClassifier(n_estimators\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m, min_samples_split\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, min_samples_leaf\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, max_features\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msqrt\u001b[39m\u001b[39m'\u001b[39m, max_depth\u001b[39m=\u001b[39m\u001b[39m9\u001b[39m, bootstrap\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, criterion\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mentropy\u001b[39m\u001b[39m'\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/juanbellon/Documents/feup/ac/src/6-predictive.ipynb#X33sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m clf \u001b[39m=\u001b[39m StackingClassifier(estimators\u001b[39m=\u001b[39mlevel0, final_estimator\u001b[39m=\u001b[39mlevel1, cv\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/juanbellon/Documents/feup/ac/src/6-predictive.ipynb#X33sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m rolling_window_results \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m model_training_loop(clf, data, unique_years)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juanbellon/Documents/feup/ac/src/6-predictive.ipynb#X33sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m trained_models\u001b[39m.\u001b[39mappend(rolling_window_results[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m'\u001b[39m])  \u001b[39m# Get the last trained model\u001b[39;00m\n",
      "\u001b[1;32m/home/juanbellon/Documents/feup/ac/src/6-predictive.ipynb Cell 25\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/juanbellon/Documents/feup/ac/src/6-predictive.ipynb#X33sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m rolling_window_results \u001b[39m=\u001b[39m []\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/juanbellon/Documents/feup/ac/src/6-predictive.ipynb#X33sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(unique_years) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/juanbellon/Documents/feup/ac/src/6-predictive.ipynb#X33sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     result\u001b[39m=\u001b[39m rolling_window_method(i, model, data, unique_years)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/juanbellon/Documents/feup/ac/src/6-predictive.ipynb#X33sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mif\u001b[39;00m result:\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/juanbellon/Documents/feup/ac/src/6-predictive.ipynb#X33sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m         rolling_window_results\u001b[39m.\u001b[39mappend(result)\n",
      "\u001b[1;32m/home/juanbellon/Documents/feup/ac/src/6-predictive.ipynb Cell 25\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juanbellon/Documents/feup/ac/src/6-predictive.ipynb#X33sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mUnique years in X_actual:\u001b[39m\u001b[39m\"\u001b[39m, X_actual[\u001b[39m'\u001b[39m\u001b[39myear\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39munique())\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juanbellon/Documents/feup/ac/src/6-predictive.ipynb#X33sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m#create a csv file for each year, storing the data for that year and that model, in the folder output/rolling_window\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juanbellon/Documents/feup/ac/src/6-predictive.ipynb#X33sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m#X_train.to_csv(f'output/rolling_window/{model.__class__.__name__}_{test_year}_train.csv', index=False)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juanbellon/Documents/feup/ac/src/6-predictive.ipynb#X33sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m#X_test.to_csv(f'output/rolling_window/{model.__class__.__name__}_{test_year}_test.csv', index=False)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juanbellon/Documents/feup/ac/src/6-predictive.ipynb#X33sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m#X_actual.to_csv(f'output/rolling_window/{model.__class__.__name__}_{test_year}_actual.csv', index=False)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juanbellon/Documents/feup/ac/src/6-predictive.ipynb#X33sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# y_actual.to_csv(f'output/rolling_window/{model.__class__.__name__}_{test_year}_y_actual.csv', index=False)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/juanbellon/Documents/feup/ac/src/6-predictive.ipynb#X33sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m classifier \u001b[39m=\u001b[39m train_and_evaluate_model(X_train, y_train, model)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juanbellon/Documents/feup/ac/src/6-predictive.ipynb#X33sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# Calculate accuracy\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juanbellon/Documents/feup/ac/src/6-predictive.ipynb#X33sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m accuracy, auc_score, predictions, predictions_prob \u001b[39m=\u001b[39m predict_and_evaluate(classifier, X_actual, y_actual)\n",
      "\u001b[1;32m/home/juanbellon/Documents/feup/ac/src/6-predictive.ipynb Cell 25\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/juanbellon/Documents/feup/ac/src/6-predictive.ipynb#X33sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_and_evaluate_model\u001b[39m(X_train, y_train, model):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/juanbellon/Documents/feup/ac/src/6-predictive.ipynb#X33sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     classifier \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/juanbellon/Documents/feup/ac/src/6-predictive.ipynb#X33sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m classifier\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/ensemble/_stacking.py:660\u001b[0m, in \u001b[0;36mStackingClassifier.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    658\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_label_encoder\u001b[39m.\u001b[39mclasses_\n\u001b[1;32m    659\u001b[0m     y_encoded \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_label_encoder\u001b[39m.\u001b[39mtransform(y)\n\u001b[0;32m--> 660\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit(X, y_encoded, sample_weight)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/ensemble/_stacking.py:252\u001b[0m, in \u001b[0;36m_BaseStacking.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    247\u001b[0m         cv\u001b[39m.\u001b[39mrandom_state \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mRandomState()\n\u001b[1;32m    249\u001b[0m     fit_params \u001b[39m=\u001b[39m (\n\u001b[1;32m    250\u001b[0m         {\u001b[39m\"\u001b[39m\u001b[39msample_weight\u001b[39m\u001b[39m\"\u001b[39m: sample_weight} \u001b[39mif\u001b[39;00m sample_weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    251\u001b[0m     )\n\u001b[0;32m--> 252\u001b[0m     predictions \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs)(\n\u001b[1;32m    253\u001b[0m         delayed(cross_val_predict)(\n\u001b[1;32m    254\u001b[0m             clone(est),\n\u001b[1;32m    255\u001b[0m             X,\n\u001b[1;32m    256\u001b[0m             y,\n\u001b[1;32m    257\u001b[0m             cv\u001b[39m=\u001b[39;49mdeepcopy(cv),\n\u001b[1;32m    258\u001b[0m             method\u001b[39m=\u001b[39;49mmeth,\n\u001b[1;32m    259\u001b[0m             n_jobs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs,\n\u001b[1;32m    260\u001b[0m             fit_params\u001b[39m=\u001b[39;49mfit_params,\n\u001b[1;32m    261\u001b[0m             verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[1;32m    262\u001b[0m         )\n\u001b[1;32m    263\u001b[0m         \u001b[39mfor\u001b[39;49;00m est, meth \u001b[39min\u001b[39;49;00m \u001b[39mzip\u001b[39;49m(all_estimators, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstack_method_)\n\u001b[1;32m    264\u001b[0m         \u001b[39mif\u001b[39;49;00m est \u001b[39m!=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mdrop\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m    265\u001b[0m     )\n\u001b[1;32m    267\u001b[0m \u001b[39m# Only not None or not 'drop' estimators will be used in transform.\u001b[39;00m\n\u001b[1;32m    268\u001b[0m \u001b[39m# Remove the None from the method as well.\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstack_method_ \u001b[39m=\u001b[39m [\n\u001b[1;32m    270\u001b[0m     meth\n\u001b[1;32m    271\u001b[0m     \u001b[39mfor\u001b[39;00m (meth, est) \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstack_method_, all_estimators)\n\u001b[1;32m    272\u001b[0m     \u001b[39mif\u001b[39;00m est \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdrop\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    273\u001b[0m ]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     58\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[1;32m     59\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[1;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     61\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[1;32m     62\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/joblib/parallel.py:1085\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1076\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1077\u001b[0m     \u001b[39m# Only set self._iterating to True if at least a batch\u001b[39;00m\n\u001b[1;32m   1078\u001b[0m     \u001b[39m# was dispatched. In particular this covers the edge\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1082\u001b[0m     \u001b[39m# was very quick and its callback already dispatched all the\u001b[39;00m\n\u001b[1;32m   1083\u001b[0m     \u001b[39m# remaining jobs.\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m-> 1085\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[1;32m   1086\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1088\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/joblib/parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    899\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    900\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 901\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[1;32m    902\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/joblib/parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    818\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[0;32m--> 819\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[1;32m    820\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    821\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    823\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    824\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m     \u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[1;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/joblib/_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[1;32m    595\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    596\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 597\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[1;32m    122\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[0;32m--> 123\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:960\u001b[0m, in \u001b[0;36mcross_val_predict\u001b[0;34m(estimator, X, y, groups, cv, n_jobs, verbose, fit_params, pre_dispatch, method)\u001b[0m\n\u001b[1;32m    957\u001b[0m X, y, groups \u001b[39m=\u001b[39m indexable(X, y, groups)\n\u001b[1;32m    959\u001b[0m cv \u001b[39m=\u001b[39m check_cv(cv, y, classifier\u001b[39m=\u001b[39mis_classifier(estimator))\n\u001b[0;32m--> 960\u001b[0m splits \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n\u001b[1;32m    962\u001b[0m test_indices \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate([test \u001b[39mfor\u001b[39;00m _, test \u001b[39min\u001b[39;00m splits])\n\u001b[1;32m    963\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _check_is_permutation(test_indices, _num_samples(X)):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/model_selection/_split.py:345\u001b[0m, in \u001b[0;36m_BaseKFold.split\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m    343\u001b[0m n_samples \u001b[39m=\u001b[39m _num_samples(X)\n\u001b[1;32m    344\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_splits \u001b[39m>\u001b[39m n_samples:\n\u001b[0;32m--> 345\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    346\u001b[0m         (\n\u001b[1;32m    347\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mCannot have number of splits n_splits=\u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m greater\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    348\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m than the number of samples: n_samples=\u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    349\u001b[0m         )\u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_splits, n_samples)\n\u001b[1;32m    350\u001b[0m     )\n\u001b[1;32m    352\u001b[0m \u001b[39mfor\u001b[39;00m train, test \u001b[39min\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39msplit(X, y, groups):\n\u001b[1;32m    353\u001b[0m     \u001b[39myield\u001b[39;00m train, test\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot have number of splits n_splits=3 greater than the number of samples: n_samples=2."
     ]
    }
   ],
   "source": [
    "level0 = list()\n",
    "#append models in trained_models to level0\n",
    "for i in range(len(trained_models)):\n",
    "    level0.append((str(i), trained_models[i]))\n",
    "\n",
    "level1 = RandomForestClassifier(n_estimators=100, min_samples_split=2, min_samples_leaf=2, max_features='sqrt', max_depth=9, bootstrap=True, criterion='entropy', random_state=20)\n",
    "clf = StackingClassifier(estimators=level0, final_estimator=level1, cv=3)\n",
    "\n",
    "rolling_window_results += model_training_loop(clf, data, unique_years)\n",
    "trained_models.append(rolling_window_results[-1]['model'])  # Get the last trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'RandomForestClassifier': [0.5229357798165137, 0.41284403669724773, 0.40404040404040403, 0.37777777777777777, 0.4392523364485981, 0.4074074074074074, 0.42201834862385323, 0.4117647058823529, 0.0], 'GradientBoostingClassifier': [0.5229357798165137, 0.4036697247706422, 0.40404040404040403, 0.37777777777777777, 0.40186915887850466, 0.3333333333333333, 0.3853211009174312, 0.3235294117647059, 0.0], 'GaussianNB': [0.5229357798165137, 0.43119266055045874, 0.3939393939393939, 0.37777777777777777, 0.40186915887850466, 0.32407407407407407, 0.3853211009174312, 0.3235294117647059, 0.0], 'LogisticRegression': [0.5229357798165137, 0.5871559633027523, 0.5858585858585859, 0.6333333333333333, 0.616822429906542, 0.6759259259259259, 0.6146788990825688, 0.6764705882352942, 0.0], 'AdaBoostClassifier': [0.5229357798165137, 0.4036697247706422, 0.40404040404040403, 0.37777777777777777, 0.40186915887850466, 0.32407407407407407, 0.3853211009174312, 0.3235294117647059, 0.0]}\n"
     ]
    }
   ],
   "source": [
    "# {\n",
    "#         'train_years': train_years,\n",
    "#         'test_year': test_year,\n",
    "#         'Year': predict_year,\n",
    "#         'accuracy': accuracy,\n",
    "#         'auc': auc_score,\n",
    "#         'model': classifier,\n",
    "#         'predictions': predictions,\n",
    "#         'predictions_prob': predictions_prob,\n",
    "#         'actual_results': y_actual,\n",
    "# }\n",
    "\n",
    "#make a dictionary of the model: [accuracy]\n",
    "model_accuracy = dict()\n",
    "for result in rolling_window_results:\n",
    "    model_name = result['model'].__class__.__name__\n",
    "    if model_name not in model_accuracy:\n",
    "        model_accuracy[model_name] = []\n",
    "    model_accuracy[model_name].append(result['accuracy'])\n",
    "\n",
    "print(model_accuracy)\n",
    "\n",
    "#plot model accuracy in one line chart (x-axis: year, y-axis: accuracy) years are 3,4,5,6,7,8,9,10,11\n",
    "years = [3,4,5,6,7,8,9,10,11]\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Accuracy')\n",
    "for model_name in model_accuracy:\n",
    "    plt.plot(years, model_accuracy[model_name], label=model_name)\n",
    "plt.legend()\n",
    "plt.savefig('output/rolling_window/model_accuracy.png')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

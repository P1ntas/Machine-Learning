{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline \n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, RocCurveDisplay, make_scorer\n",
    "from sklearn.model_selection import learning_curve\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "import datetime\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_transformed = pd.read_csv('new_data/clean-data_without_outliers.csv')\n",
    "#competition_transformed = pd.read_csv('new_data/clean-comp.csv')\n",
    "#data = pd.read_csv('new_data/clean-data_without_outliers.csv')\n",
    "competition = pd.read_csv('new_data/clean-comp.csv')\n",
    "\n",
    "data = pd.read_csv('new_data/complete-data.csv')\n",
    "#data_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>playerID</th>\n",
       "      <th>year</th>\n",
       "      <th>stint</th>\n",
       "      <th>tmID</th>\n",
       "      <th>GP</th>\n",
       "      <th>oRebounds</th>\n",
       "      <th>dRebounds</th>\n",
       "      <th>dq</th>\n",
       "      <th>PostMinutes</th>\n",
       "      <th>PostPoints</th>\n",
       "      <th>PostoRebounds</th>\n",
       "      <th>PostdRebounds</th>\n",
       "      <th>PostRebounds</th>\n",
       "      <th>PostAssists</th>\n",
       "      <th>PostSteals</th>\n",
       "      <th>PostBlocks</th>\n",
       "      <th>PostTurnovers</th>\n",
       "      <th>PostPF</th>\n",
       "      <th>PostDQ</th>\n",
       "      <th>ft%</th>\n",
       "      <th>fg%</th>\n",
       "      <th>three%</th>\n",
       "      <th>gs%</th>\n",
       "      <th>Postft%</th>\n",
       "      <th>Postfg%</th>\n",
       "      <th>Postthree%</th>\n",
       "      <th>Postgs%</th>\n",
       "      <th>efg%</th>\n",
       "      <th>ts%</th>\n",
       "      <th>ppg</th>\n",
       "      <th>rpg</th>\n",
       "      <th>apg</th>\n",
       "      <th>spg</th>\n",
       "      <th>bpg</th>\n",
       "      <th>eff</th>\n",
       "      <th>pp36</th>\n",
       "      <th>defensive_prowess</th>\n",
       "      <th>defensive_discipline</th>\n",
       "      <th>mpg</th>\n",
       "      <th>pos</th>\n",
       "      <th>college</th>\n",
       "      <th>playoff</th>\n",
       "      <th>confID</th>\n",
       "      <th>playoff_progression</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>award_count</th>\n",
       "      <th>career_year</th>\n",
       "      <th>playoff_progression_rolling</th>\n",
       "      <th>playoff_rolling</th>\n",
       "      <th>pp36_rolling</th>\n",
       "      <th>eff_rolling</th>\n",
       "      <th>award_count_rolling</th>\n",
       "      <th>defensive_prowess_rolling</th>\n",
       "      <th>defensive_discipline_rolling</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>640</th>\n",
       "      <td>392</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>567</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.82</td>\n",
       "      <td>7.60</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.60</td>\n",
       "      <td>-26.80</td>\n",
       "      <td>16.20</td>\n",
       "      <td>16.0</td>\n",
       "      <td>6.40</td>\n",
       "      <td>17.00</td>\n",
       "      <td>2</td>\n",
       "      <td>678</td>\n",
       "      <td>0.0</td>\n",
       "      <td>697</td>\n",
       "      <td>0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>168</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>14.22</td>\n",
       "      <td>-149.875</td>\n",
       "      <td>0.0</td>\n",
       "      <td>48.60</td>\n",
       "      <td>4.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>639</th>\n",
       "      <td>391</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>559</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.26</td>\n",
       "      <td>1.32</td>\n",
       "      <td>5.20</td>\n",
       "      <td>1.30</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-19.60</td>\n",
       "      <td>16.56</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.80</td>\n",
       "      <td>11.20</td>\n",
       "      <td>2</td>\n",
       "      <td>593</td>\n",
       "      <td>1.0</td>\n",
       "      <td>697</td>\n",
       "      <td>1</td>\n",
       "      <td>73.0</td>\n",
       "      <td>164</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.78</td>\n",
       "      <td>-151.800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.45</td>\n",
       "      <td>8.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>225</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>554</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.40</td>\n",
       "      <td>1.20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.68</td>\n",
       "      <td>1.20</td>\n",
       "      <td>2.60</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.60</td>\n",
       "      <td>-7.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>24.0</td>\n",
       "      <td>5.20</td>\n",
       "      <td>13.80</td>\n",
       "      <td>4</td>\n",
       "      <td>648</td>\n",
       "      <td>1.0</td>\n",
       "      <td>697</td>\n",
       "      <td>1</td>\n",
       "      <td>75.0</td>\n",
       "      <td>179</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.28</td>\n",
       "      <td>-67.700</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.15</td>\n",
       "      <td>5.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>226</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>566</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.62</td>\n",
       "      <td>1.22</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.98</td>\n",
       "      <td>9.81</td>\n",
       "      <td>1.84</td>\n",
       "      <td>2.97</td>\n",
       "      <td>1.72</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-236.63</td>\n",
       "      <td>16.20</td>\n",
       "      <td>29.7</td>\n",
       "      <td>7.68</td>\n",
       "      <td>21.62</td>\n",
       "      <td>1</td>\n",
       "      <td>615</td>\n",
       "      <td>0.0</td>\n",
       "      <td>697</td>\n",
       "      <td>0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>118</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.60</td>\n",
       "      <td>-120.680</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.15</td>\n",
       "      <td>5.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>236</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>553</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.04</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.88</td>\n",
       "      <td>2.68</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.07</td>\n",
       "      <td>-66.96</td>\n",
       "      <td>10.08</td>\n",
       "      <td>7.1</td>\n",
       "      <td>3.64</td>\n",
       "      <td>9.61</td>\n",
       "      <td>2</td>\n",
       "      <td>657</td>\n",
       "      <td>1.0</td>\n",
       "      <td>697</td>\n",
       "      <td>0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>150</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>9.36</td>\n",
       "      <td>-89.410</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.40</td>\n",
       "      <td>4.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>243</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>552</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>571</td>\n",
       "      <td>0.0</td>\n",
       "      <td>698</td>\n",
       "      <td>0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>196</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>19.98</td>\n",
       "      <td>-267.790</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53.75</td>\n",
       "      <td>10.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>256</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>569</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>630</td>\n",
       "      <td>0.0</td>\n",
       "      <td>698</td>\n",
       "      <td>0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>180</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>12.42</td>\n",
       "      <td>-228.340</td>\n",
       "      <td>0.0</td>\n",
       "      <td>54.55</td>\n",
       "      <td>8.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>258</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>741</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>662</td>\n",
       "      <td>0.0</td>\n",
       "      <td>697</td>\n",
       "      <td>0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>175</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>10.44</td>\n",
       "      <td>-131.025</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.25</td>\n",
       "      <td>6.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>738</th>\n",
       "      <td>438</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>558</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>583</td>\n",
       "      <td>0.0</td>\n",
       "      <td>698</td>\n",
       "      <td>0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>183</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>12.24</td>\n",
       "      <td>-82.975</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.65</td>\n",
       "      <td>3.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>900</th>\n",
       "      <td>544</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>560</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>628</td>\n",
       "      <td>0.0</td>\n",
       "      <td>697</td>\n",
       "      <td>0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>165</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.62</td>\n",
       "      <td>-320.180</td>\n",
       "      <td>0.0</td>\n",
       "      <td>62.10</td>\n",
       "      <td>8.24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>901 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     playerID  year  stint  tmID    GP  oRebounds  dRebounds    dq  \\\n",
       "640       392     3      0   567   5.0       0.20       0.60  0.00   \n",
       "639       391     3      0   559  10.0       0.80       0.50  0.00   \n",
       "344       225     3      0   554   5.0       1.40       1.20  0.00   \n",
       "345       226     3      0   566  32.0       0.62       1.22  0.00   \n",
       "355       236     3      0   553  28.0       0.29       0.39  0.04   \n",
       "..        ...   ...    ...   ...   ...        ...        ...   ...   \n",
       "387       243    11      0   552   0.0       0.00       0.00  0.00   \n",
       "396       256    11      0   569   0.0       0.00       0.00  0.00   \n",
       "399       258    11      0   741   0.0       0.00       0.00  0.00   \n",
       "738       438    11      0   558   0.0       0.00       0.00  0.00   \n",
       "900       544    11      0   560   0.0       0.00       0.00  0.00   \n",
       "\n",
       "     PostMinutes  PostPoints  PostoRebounds  PostdRebounds  PostRebounds  \\\n",
       "640          0.0         0.0            0.0            0.0           0.0   \n",
       "639          0.0         0.0            0.0            0.0           0.0   \n",
       "344          0.0         0.0            0.0            0.0           0.0   \n",
       "345          0.0         0.0            0.0            0.0           0.0   \n",
       "355          8.0         3.4            0.4            0.4           0.8   \n",
       "..           ...         ...            ...            ...           ...   \n",
       "387          0.0         0.0            0.0            0.0           0.0   \n",
       "396          0.0         0.0            0.0            0.0           0.0   \n",
       "399          0.0         0.0            0.0            0.0           0.0   \n",
       "738          0.0         0.0            0.0            0.0           0.0   \n",
       "900          0.0         0.0            0.0            0.0           0.0   \n",
       "\n",
       "     PostAssists  PostSteals  PostBlocks  PostTurnovers  PostPF  PostDQ   ft%  \\\n",
       "640          0.0         0.0         0.0            0.0     0.0     0.0  0.73   \n",
       "639          0.0         0.0         0.0            0.0     0.0     0.0  0.75   \n",
       "344          0.0         0.0         0.0            0.0     0.0     0.0  0.00   \n",
       "345          0.0         0.0         0.0            0.0     0.0     0.0  0.70   \n",
       "355          0.4         0.0         0.0            0.2     0.4     0.0  0.75   \n",
       "..           ...         ...         ...            ...     ...     ...   ...   \n",
       "387          0.0         0.0         0.0            0.0     0.0     0.0  0.00   \n",
       "396          0.0         0.0         0.0            0.0     0.0     0.0  0.00   \n",
       "399          0.0         0.0         0.0            0.0     0.0     0.0  0.00   \n",
       "738          0.0         0.0         0.0            0.0     0.0     0.0  0.00   \n",
       "900          0.0         0.0         0.0            0.0     0.0     0.0  0.00   \n",
       "\n",
       "      fg%  three%   gs%  Postft%  Postfg%  Postthree%  Postgs%  efg%   ts%  \\\n",
       "640  0.37    0.00  0.80      0.0      0.0        0.00      0.0  0.74  0.82   \n",
       "639  0.50    0.50  0.00      0.0      0.0        0.00      0.0  1.26  1.32   \n",
       "344  0.38    0.00  0.20      0.0      0.0        0.00      0.0  0.76  0.68   \n",
       "345  0.42    0.32  0.31      0.0      0.0        0.00      0.0  0.92  0.98   \n",
       "355  0.33    0.31  0.04      0.5      0.5        0.22      0.0  0.78  0.88   \n",
       "..    ...     ...   ...      ...      ...         ...      ...   ...   ...   \n",
       "387  0.00    0.00  0.00      0.0      0.0        0.00      0.0  0.00  0.00   \n",
       "396  0.00    0.00  0.00      0.0      0.0        0.00      0.0  0.00  0.00   \n",
       "399  0.00    0.00  0.00      0.0      0.0        0.00      0.0  0.00  0.00   \n",
       "738  0.00    0.00  0.00      0.0      0.0        0.00      0.0  0.00  0.00   \n",
       "900  0.00    0.00  0.00      0.0      0.0        0.00      0.0  0.00  0.00   \n",
       "\n",
       "      ppg   rpg   apg   spg   bpg     eff   pp36  defensive_prowess  \\\n",
       "640  7.60  0.80  0.80  0.40  0.60  -26.80  16.20               16.0   \n",
       "639  5.20  1.30  0.70  0.20  0.00  -19.60  16.56                7.0   \n",
       "344  1.20  2.60  0.20  0.60  0.60   -7.80   3.24               24.0   \n",
       "345  9.81  1.84  2.97  1.72  0.03 -236.63  16.20               29.7   \n",
       "355  2.68  0.68  0.36  0.25  0.07  -66.96  10.08                7.1   \n",
       "..    ...   ...   ...   ...   ...     ...    ...                ...   \n",
       "387  0.00  0.00  0.00  0.00  0.00    0.00   0.00                0.0   \n",
       "396  0.00  0.00  0.00  0.00  0.00    0.00   0.00                0.0   \n",
       "399  0.00  0.00  0.00  0.00  0.00    0.00   0.00                0.0   \n",
       "738  0.00  0.00  0.00  0.00  0.00    0.00   0.00                0.0   \n",
       "900  0.00  0.00  0.00  0.00  0.00    0.00   0.00                0.0   \n",
       "\n",
       "     defensive_discipline    mpg  pos  college  playoff  confID  \\\n",
       "640                  6.40  17.00    2      678      0.0     697   \n",
       "639                  4.80  11.20    2      593      1.0     697   \n",
       "344                  5.20  13.80    4      648      1.0     697   \n",
       "345                  7.68  21.62    1      615      0.0     697   \n",
       "355                  3.64   9.61    2      657      1.0     697   \n",
       "..                    ...    ...  ...      ...      ...     ...   \n",
       "387                  0.00   0.00    2      571      0.0     698   \n",
       "396                  0.00   0.00    2      630      0.0     698   \n",
       "399                  0.00   0.00    2      662      0.0     697   \n",
       "738                  0.00   0.00    1      583      0.0     698   \n",
       "900                  0.00   0.00    2      628      0.0     697   \n",
       "\n",
       "     playoff_progression  height  weight  award_count  career_year  \\\n",
       "640                    0    73.0     168          0.0            3   \n",
       "639                    1    73.0     164          0.0            3   \n",
       "344                    1    75.0     179          0.0            3   \n",
       "345                    0    66.0     118          0.0            3   \n",
       "355                    0    73.0     150          0.0            3   \n",
       "..                   ...     ...     ...          ...          ...   \n",
       "387                    0    74.0     196          0.0            9   \n",
       "396                    0    76.0     180          0.0            6   \n",
       "399                    0    75.0     175          0.0            5   \n",
       "738                    0    75.0     183          0.0            4   \n",
       "900                    0    73.0     165          0.0            5   \n",
       "\n",
       "     playoff_progression_rolling  playoff_rolling  pp36_rolling  eff_rolling  \\\n",
       "640                          0.5              0.5         14.22     -149.875   \n",
       "639                          0.0              0.0         12.78     -151.800   \n",
       "344                          2.5              1.0          8.28      -67.700   \n",
       "345                          0.0              0.0         12.60     -120.680   \n",
       "355                          0.5              0.5          9.36      -89.410   \n",
       "..                           ...              ...           ...          ...   \n",
       "387                          0.5              0.5         19.98     -267.790   \n",
       "396                          1.0              0.5         12.42     -228.340   \n",
       "399                          0.5              0.5         10.44     -131.025   \n",
       "738                          1.0              0.5         12.24      -82.975   \n",
       "900                          2.0              1.0         19.62     -320.180   \n",
       "\n",
       "     award_count_rolling  defensive_prowess_rolling  \\\n",
       "640                  0.0                      48.60   \n",
       "639                  0.0                      20.45   \n",
       "344                  0.0                      24.15   \n",
       "345                  0.0                      21.15   \n",
       "355                  0.0                      12.40   \n",
       "..                   ...                        ...   \n",
       "387                  0.0                      53.75   \n",
       "396                  0.0                      54.55   \n",
       "399                  0.0                      20.25   \n",
       "738                  0.0                      13.65   \n",
       "900                  0.0                      62.10   \n",
       "\n",
       "     defensive_discipline_rolling  \n",
       "640                          4.44  \n",
       "639                          8.04  \n",
       "344                          5.53  \n",
       "345                          5.50  \n",
       "355                          4.91  \n",
       "..                            ...  \n",
       "387                         10.39  \n",
       "396                          8.59  \n",
       "399                          6.59  \n",
       "738                          3.15  \n",
       "900                          8.24  \n",
       "\n",
       "[901 rows x 55 columns]"
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sort_values('year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['playerID', 'year', 'stint', 'tmID', 'height', 'weight', 'pos',\n",
       "       'college', 'confID'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "competition.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['playerID', 'year', 'stint', 'tmID', 'pos', 'college', 'height', 'weight', 'career_year', 'confID', 'playoff_progression_rolling', 'playoff_rolling', 'pp36_rolling', 'eff_rolling', 'award_count_rolling', 'defensive_prowess_rolling', 'defensive_discipline_rolling']\n"
     ]
    }
   ],
   "source": [
    "input_cols = ['playerID', 'year','stint', 'tmID', 'pos', 'college', 'height', 'weight', 'career_year', 'confID']\n",
    "\n",
    "#add all columns ending in rolling from data to input cols\n",
    "input_cols+=[c for c in data.columns if c.endswith(\"_rolling\")]\n",
    "\n",
    "# The output columns are the genres\n",
    "output_cols = 'playoff'\n",
    "\n",
    "known_columns = ['playerID', 'year', 'stint', 'tmID', 'height', 'weight', 'pos','college', 'confID']\n",
    "\n",
    "# Averages to calculate for precision, recall, and f1-score\n",
    "averages = [None, \"macro\", \"weighted\", \"micro\", \"samples\"]\n",
    "\n",
    "print(input_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>playerID</th>\n",
       "      <th>year</th>\n",
       "      <th>stint</th>\n",
       "      <th>tmID</th>\n",
       "      <th>GP</th>\n",
       "      <th>oRebounds</th>\n",
       "      <th>dRebounds</th>\n",
       "      <th>dq</th>\n",
       "      <th>PostMinutes</th>\n",
       "      <th>PostPoints</th>\n",
       "      <th>PostoRebounds</th>\n",
       "      <th>PostdRebounds</th>\n",
       "      <th>PostRebounds</th>\n",
       "      <th>PostAssists</th>\n",
       "      <th>PostSteals</th>\n",
       "      <th>PostBlocks</th>\n",
       "      <th>PostTurnovers</th>\n",
       "      <th>PostPF</th>\n",
       "      <th>PostDQ</th>\n",
       "      <th>ft%</th>\n",
       "      <th>fg%</th>\n",
       "      <th>three%</th>\n",
       "      <th>gs%</th>\n",
       "      <th>Postft%</th>\n",
       "      <th>Postfg%</th>\n",
       "      <th>Postthree%</th>\n",
       "      <th>Postgs%</th>\n",
       "      <th>efg%</th>\n",
       "      <th>ts%</th>\n",
       "      <th>ppg</th>\n",
       "      <th>rpg</th>\n",
       "      <th>apg</th>\n",
       "      <th>spg</th>\n",
       "      <th>bpg</th>\n",
       "      <th>eff</th>\n",
       "      <th>pp36</th>\n",
       "      <th>defensive_prowess</th>\n",
       "      <th>defensive_discipline</th>\n",
       "      <th>mpg</th>\n",
       "      <th>pos</th>\n",
       "      <th>college</th>\n",
       "      <th>playoff</th>\n",
       "      <th>confID</th>\n",
       "      <th>playoff_progression</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>award_count</th>\n",
       "      <th>career_year</th>\n",
       "      <th>playoff_progression_rolling</th>\n",
       "      <th>playoff_rolling</th>\n",
       "      <th>pp36_rolling</th>\n",
       "      <th>eff_rolling</th>\n",
       "      <th>award_count_rolling</th>\n",
       "      <th>defensive_prowess_rolling</th>\n",
       "      <th>defensive_discipline_rolling</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>551</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.47</td>\n",
       "      <td>3.23</td>\n",
       "      <td>0.00</td>\n",
       "      <td>23.00</td>\n",
       "      <td>7.67</td>\n",
       "      <td>0.33</td>\n",
       "      <td>1.33</td>\n",
       "      <td>1.67</td>\n",
       "      <td>1.33</td>\n",
       "      <td>1.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>2.67</td>\n",
       "      <td>2.67</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.83</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.96</td>\n",
       "      <td>10.60</td>\n",
       "      <td>4.70</td>\n",
       "      <td>2.73</td>\n",
       "      <td>1.47</td>\n",
       "      <td>0.37</td>\n",
       "      <td>-272.13</td>\n",
       "      <td>14.40</td>\n",
       "      <td>50.7</td>\n",
       "      <td>11.26</td>\n",
       "      <td>26.40</td>\n",
       "      <td>2</td>\n",
       "      <td>571</td>\n",
       "      <td>1.0</td>\n",
       "      <td>697</td>\n",
       "      <td>1</td>\n",
       "      <td>74.0</td>\n",
       "      <td>169</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.40</td>\n",
       "      <td>-301.960</td>\n",
       "      <td>0.0</td>\n",
       "      <td>63.35</td>\n",
       "      <td>12.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>551</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.77</td>\n",
       "      <td>2.59</td>\n",
       "      <td>0.00</td>\n",
       "      <td>33.50</td>\n",
       "      <td>10.00</td>\n",
       "      <td>1.50</td>\n",
       "      <td>3.00</td>\n",
       "      <td>4.50</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.50</td>\n",
       "      <td>3.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.92</td>\n",
       "      <td>6.64</td>\n",
       "      <td>3.36</td>\n",
       "      <td>2.05</td>\n",
       "      <td>1.36</td>\n",
       "      <td>0.09</td>\n",
       "      <td>-137.50</td>\n",
       "      <td>11.52</td>\n",
       "      <td>40.5</td>\n",
       "      <td>7.72</td>\n",
       "      <td>21.00</td>\n",
       "      <td>2</td>\n",
       "      <td>571</td>\n",
       "      <td>1.0</td>\n",
       "      <td>697</td>\n",
       "      <td>1</td>\n",
       "      <td>74.0</td>\n",
       "      <td>169</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>14.22</td>\n",
       "      <td>-299.970</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53.70</td>\n",
       "      <td>11.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>551</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.94</td>\n",
       "      <td>2.52</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.98</td>\n",
       "      <td>9.81</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1.94</td>\n",
       "      <td>1.55</td>\n",
       "      <td>0.19</td>\n",
       "      <td>-250.06</td>\n",
       "      <td>14.04</td>\n",
       "      <td>42.6</td>\n",
       "      <td>10.70</td>\n",
       "      <td>25.06</td>\n",
       "      <td>2</td>\n",
       "      <td>571</td>\n",
       "      <td>0.0</td>\n",
       "      <td>697</td>\n",
       "      <td>0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>169</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.96</td>\n",
       "      <td>-204.815</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.60</td>\n",
       "      <td>9.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>551</td>\n",
       "      <td>34.0</td>\n",
       "      <td>1.29</td>\n",
       "      <td>1.82</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.98</td>\n",
       "      <td>7.74</td>\n",
       "      <td>3.12</td>\n",
       "      <td>1.59</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-210.52</td>\n",
       "      <td>12.96</td>\n",
       "      <td>28.5</td>\n",
       "      <td>7.48</td>\n",
       "      <td>21.29</td>\n",
       "      <td>2</td>\n",
       "      <td>571</td>\n",
       "      <td>0.0</td>\n",
       "      <td>697</td>\n",
       "      <td>0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>169</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>12.78</td>\n",
       "      <td>-193.780</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41.55</td>\n",
       "      <td>9.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>551</td>\n",
       "      <td>34.0</td>\n",
       "      <td>1.56</td>\n",
       "      <td>2.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.04</td>\n",
       "      <td>1.08</td>\n",
       "      <td>10.15</td>\n",
       "      <td>4.41</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1.29</td>\n",
       "      <td>0.12</td>\n",
       "      <td>-222.53</td>\n",
       "      <td>14.76</td>\n",
       "      <td>42.6</td>\n",
       "      <td>9.06</td>\n",
       "      <td>24.79</td>\n",
       "      <td>2</td>\n",
       "      <td>571</td>\n",
       "      <td>0.0</td>\n",
       "      <td>697</td>\n",
       "      <td>0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>169</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.50</td>\n",
       "      <td>-230.290</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.55</td>\n",
       "      <td>9.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>896</th>\n",
       "      <td>539</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>569</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.41</td>\n",
       "      <td>1.91</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.94</td>\n",
       "      <td>2.21</td>\n",
       "      <td>2.32</td>\n",
       "      <td>1.35</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.59</td>\n",
       "      <td>-82.12</td>\n",
       "      <td>4.68</td>\n",
       "      <td>29.1</td>\n",
       "      <td>6.70</td>\n",
       "      <td>17.47</td>\n",
       "      <td>2</td>\n",
       "      <td>631</td>\n",
       "      <td>0.0</td>\n",
       "      <td>698</td>\n",
       "      <td>0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>183</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.94</td>\n",
       "      <td>-66.255</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34.95</td>\n",
       "      <td>6.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>897</th>\n",
       "      <td>539</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>569</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>2.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.02</td>\n",
       "      <td>1.08</td>\n",
       "      <td>3.07</td>\n",
       "      <td>2.70</td>\n",
       "      <td>1.41</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.74</td>\n",
       "      <td>-67.71</td>\n",
       "      <td>5.76</td>\n",
       "      <td>31.9</td>\n",
       "      <td>6.66</td>\n",
       "      <td>19.67</td>\n",
       "      <td>2</td>\n",
       "      <td>631</td>\n",
       "      <td>0.0</td>\n",
       "      <td>698</td>\n",
       "      <td>0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>183</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.76</td>\n",
       "      <td>-78.280</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.50</td>\n",
       "      <td>6.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>898</th>\n",
       "      <td>544</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>560</td>\n",
       "      <td>33.0</td>\n",
       "      <td>1.58</td>\n",
       "      <td>4.06</td>\n",
       "      <td>0.03</td>\n",
       "      <td>36.11</td>\n",
       "      <td>17.67</td>\n",
       "      <td>1.78</td>\n",
       "      <td>4.11</td>\n",
       "      <td>5.89</td>\n",
       "      <td>1.67</td>\n",
       "      <td>1.56</td>\n",
       "      <td>0.11</td>\n",
       "      <td>2.11</td>\n",
       "      <td>1.89</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.96</td>\n",
       "      <td>1.10</td>\n",
       "      <td>17.45</td>\n",
       "      <td>5.64</td>\n",
       "      <td>2.27</td>\n",
       "      <td>1.61</td>\n",
       "      <td>0.52</td>\n",
       "      <td>-299.51</td>\n",
       "      <td>19.80</td>\n",
       "      <td>61.8</td>\n",
       "      <td>8.66</td>\n",
       "      <td>31.94</td>\n",
       "      <td>2</td>\n",
       "      <td>628</td>\n",
       "      <td>1.0</td>\n",
       "      <td>697</td>\n",
       "      <td>3</td>\n",
       "      <td>73.0</td>\n",
       "      <td>165</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>16.02</td>\n",
       "      <td>-292.955</td>\n",
       "      <td>0.0</td>\n",
       "      <td>63.20</td>\n",
       "      <td>8.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>544</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>560</td>\n",
       "      <td>33.0</td>\n",
       "      <td>2.09</td>\n",
       "      <td>4.42</td>\n",
       "      <td>0.03</td>\n",
       "      <td>32.00</td>\n",
       "      <td>19.33</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.33</td>\n",
       "      <td>5.33</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.67</td>\n",
       "      <td>1.67</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.31</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.94</td>\n",
       "      <td>1.06</td>\n",
       "      <td>18.18</td>\n",
       "      <td>6.52</td>\n",
       "      <td>1.64</td>\n",
       "      <td>1.33</td>\n",
       "      <td>0.48</td>\n",
       "      <td>-340.85</td>\n",
       "      <td>19.44</td>\n",
       "      <td>62.4</td>\n",
       "      <td>7.82</td>\n",
       "      <td>33.73</td>\n",
       "      <td>2</td>\n",
       "      <td>628</td>\n",
       "      <td>1.0</td>\n",
       "      <td>697</td>\n",
       "      <td>1</td>\n",
       "      <td>73.0</td>\n",
       "      <td>165</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.90</td>\n",
       "      <td>-305.255</td>\n",
       "      <td>0.0</td>\n",
       "      <td>58.50</td>\n",
       "      <td>9.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>900</th>\n",
       "      <td>544</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>560</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>628</td>\n",
       "      <td>0.0</td>\n",
       "      <td>697</td>\n",
       "      <td>0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>165</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.62</td>\n",
       "      <td>-320.180</td>\n",
       "      <td>0.0</td>\n",
       "      <td>62.10</td>\n",
       "      <td>8.24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>901 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     playerID  year  stint  tmID    GP  oRebounds  dRebounds    dq  \\\n",
       "0           0     4      0   551  30.0       1.47       3.23  0.00   \n",
       "1           0     5      0   551  22.0       0.77       2.59  0.00   \n",
       "2           0     6      0   551  31.0       0.94       2.52  0.00   \n",
       "3           0     7      0   551  34.0       1.29       1.82  0.00   \n",
       "4           0     8      0   551  34.0       1.56       2.85  0.00   \n",
       "..        ...   ...    ...   ...   ...        ...        ...   ...   \n",
       "896       539     9      0   569  34.0       0.41       1.91  0.00   \n",
       "897       539    10      0   569  27.0       0.63       2.07  0.00   \n",
       "898       544     9      0   560  33.0       1.58       4.06  0.03   \n",
       "899       544    10      0   560  33.0       2.09       4.42  0.03   \n",
       "900       544    11      0   560   0.0       0.00       0.00  0.00   \n",
       "\n",
       "     PostMinutes  PostPoints  PostoRebounds  PostdRebounds  PostRebounds  \\\n",
       "0          23.00        7.67           0.33           1.33          1.67   \n",
       "1          33.50       10.00           1.50           3.00          4.50   \n",
       "2           0.00        0.00           0.00           0.00          0.00   \n",
       "3           0.00        0.00           0.00           0.00          0.00   \n",
       "4           0.00        0.00           0.00           0.00          0.00   \n",
       "..           ...         ...            ...            ...           ...   \n",
       "896         0.00        0.00           0.00           0.00          0.00   \n",
       "897         0.00        0.00           0.00           0.00          0.00   \n",
       "898        36.11       17.67           1.78           4.11          5.89   \n",
       "899        32.00       19.33           2.00           3.33          5.33   \n",
       "900         0.00        0.00           0.00           0.00          0.00   \n",
       "\n",
       "     PostAssists  PostSteals  PostBlocks  PostTurnovers  PostPF  PostDQ   ft%  \\\n",
       "0           1.33        1.33        0.33           2.67    2.67     0.0  0.70   \n",
       "1           1.50        0.50        1.00           1.50    3.50     0.0  0.61   \n",
       "2           0.00        0.00        0.00           0.00    0.00     0.0  0.73   \n",
       "3           0.00        0.00        0.00           0.00    0.00     0.0  0.66   \n",
       "4           0.00        0.00        0.00           0.00    0.00     0.0  0.84   \n",
       "..           ...         ...         ...            ...     ...     ...   ...   \n",
       "896         0.00        0.00        0.00           0.00    0.00     0.0  0.56   \n",
       "897         0.00        0.00        0.00           0.00    0.00     0.0  0.88   \n",
       "898         1.67        1.56        0.11           2.11    1.89     0.0  0.79   \n",
       "899         2.00        2.00        0.67           1.67    3.00     0.0  0.77   \n",
       "900         0.00        0.00        0.00           0.00    0.00     0.0  0.00   \n",
       "\n",
       "      fg%  three%   gs%  Postft%  Postfg%  Postthree%  Postgs%  efg%   ts%  \\\n",
       "0    0.39    0.30  0.83     1.00     0.27        0.43      1.0  0.88  0.96   \n",
       "1    0.35    0.38  0.50     0.50     0.35        0.25      1.0  0.84  0.92   \n",
       "2    0.39    0.40  1.00     0.00     0.00        0.00      0.0  0.90  0.98   \n",
       "3    0.41    0.37  0.06     0.00     0.00        0.00      0.0  0.92  0.98   \n",
       "4    0.44    0.45  0.85     0.00     0.00        0.00      0.0  1.04  1.08   \n",
       "..    ...     ...   ...      ...      ...         ...      ...   ...   ...   \n",
       "896  0.34    0.31  0.68     0.00     0.00        0.00      0.0  0.90  0.94   \n",
       "897  0.38    0.39  0.85     0.00     0.00        0.00      0.0  1.02  1.08   \n",
       "898  0.48    0.00  1.00     0.75     0.46        0.00      1.0  0.96  1.10   \n",
       "899  0.45    0.31  1.00     0.68     0.46        0.50      1.0  0.94  1.06   \n",
       "900  0.00    0.00  0.00     0.00     0.00        0.00      0.0  0.00  0.00   \n",
       "\n",
       "       ppg   rpg   apg   spg   bpg     eff   pp36  defensive_prowess  \\\n",
       "0    10.60  4.70  2.73  1.47  0.37 -272.13  14.40               50.7   \n",
       "1     6.64  3.36  2.05  1.36  0.09 -137.50  11.52               40.5   \n",
       "2     9.81  3.45  1.94  1.55  0.19 -250.06  14.04               42.6   \n",
       "3     7.74  3.12  1.59  1.00  0.03 -210.52  12.96               28.5   \n",
       "4    10.15  4.41  2.50  1.29  0.12 -222.53  14.76               42.6   \n",
       "..     ...   ...   ...   ...   ...     ...    ...                ...   \n",
       "896   2.21  2.32  1.35  0.41  0.59  -82.12   4.68               29.1   \n",
       "897   3.07  2.70  1.41  0.37  0.74  -67.71   5.76               31.9   \n",
       "898  17.45  5.64  2.27  1.61  0.52 -299.51  19.80               61.8   \n",
       "899  18.18  6.52  1.64  1.33  0.48 -340.85  19.44               62.4   \n",
       "900   0.00  0.00  0.00  0.00  0.00    0.00   0.00                0.0   \n",
       "\n",
       "     defensive_discipline    mpg  pos  college  playoff  confID  \\\n",
       "0                   11.26  26.40    2      571      1.0     697   \n",
       "1                    7.72  21.00    2      571      1.0     697   \n",
       "2                   10.70  25.06    2      571      0.0     697   \n",
       "3                    7.48  21.29    2      571      0.0     697   \n",
       "4                    9.06  24.79    2      571      0.0     697   \n",
       "..                    ...    ...  ...      ...      ...     ...   \n",
       "896                  6.70  17.47    2      631      0.0     698   \n",
       "897                  6.66  19.67    2      631      0.0     698   \n",
       "898                  8.66  31.94    2      628      1.0     697   \n",
       "899                  7.82  33.73    2      628      1.0     697   \n",
       "900                  0.00   0.00    2      628      0.0     697   \n",
       "\n",
       "     playoff_progression  height  weight  award_count  career_year  \\\n",
       "0                      1    74.0     169          0.0            3   \n",
       "1                      1    74.0     169          0.0            4   \n",
       "2                      0    74.0     169          0.0            5   \n",
       "3                      0    74.0     169          0.0            6   \n",
       "4                      0    74.0     169          0.0            7   \n",
       "..                   ...     ...     ...          ...          ...   \n",
       "896                    0    73.0     183          0.0            7   \n",
       "897                    0    73.0     183          0.0            8   \n",
       "898                    3    73.0     165          0.0            3   \n",
       "899                    1    73.0     165          0.0            4   \n",
       "900                    0    73.0     165          0.0            5   \n",
       "\n",
       "     playoff_progression_rolling  playoff_rolling  pp36_rolling  eff_rolling  \\\n",
       "0                            0.0              0.0         14.40     -301.960   \n",
       "1                            0.5              0.5         14.22     -299.970   \n",
       "2                            1.0              1.0         12.96     -204.815   \n",
       "3                            0.5              0.5         12.78     -193.780   \n",
       "4                            0.0              0.0         13.50     -230.290   \n",
       "..                           ...              ...           ...          ...   \n",
       "896                          0.0              0.0          5.94      -66.255   \n",
       "897                          0.0              0.0          5.76      -78.280   \n",
       "898                          1.0              0.5         16.02     -292.955   \n",
       "899                          2.5              1.0         18.90     -305.255   \n",
       "900                          2.0              1.0         19.62     -320.180   \n",
       "\n",
       "     award_count_rolling  defensive_prowess_rolling  \\\n",
       "0                    0.0                      63.35   \n",
       "1                    0.0                      53.70   \n",
       "2                    0.0                      45.60   \n",
       "3                    0.0                      41.55   \n",
       "4                    0.0                      35.55   \n",
       "..                   ...                        ...   \n",
       "896                  0.0                      34.95   \n",
       "897                  0.0                      32.50   \n",
       "898                  0.0                      63.20   \n",
       "899                  0.0                      58.50   \n",
       "900                  0.0                      62.10   \n",
       "\n",
       "     defensive_discipline_rolling  \n",
       "0                           12.07  \n",
       "1                           11.74  \n",
       "2                            9.49  \n",
       "3                            9.21  \n",
       "4                            9.09  \n",
       "..                            ...  \n",
       "896                          6.91  \n",
       "897                          6.53  \n",
       "898                          8.26  \n",
       "899                          9.21  \n",
       "900                          8.24  \n",
       "\n",
       "[901 rows x 55 columns]"
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 20)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "playerID                        False\n",
       "year                            False\n",
       "stint                           False\n",
       "tmID                            False\n",
       "GP                              False\n",
       "                                ...  \n",
       "pp36_rolling                    False\n",
       "eff_rolling                     False\n",
       "award_count_rolling             False\n",
       "defensive_prowess_rolling       False\n",
       "defensive_discipline_rolling    False\n",
       "Length: 55, dtype: bool"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from https://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import learning_curve\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "def plot_learning_curve(\n",
    "    title,\n",
    "    train_sizes, \n",
    "    train_scores, \n",
    "    test_scores, \n",
    "    fit_times,\n",
    "    score_times,\n",
    "    axes=None,\n",
    "    ylim=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate 3 plots: the test and training learning curve, the training\n",
    "    samples vs fit times curve, the fit times vs score curve.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : estimator instance\n",
    "        An estimator instance implementing `fit` and `predict` methods which\n",
    "        will be cloned for each validation.\n",
    "\n",
    "    title : str\n",
    "        Title for the chart.\n",
    "\n",
    "    X : array-like of shape (n_samples, n_features)\n",
    "        Training vector, where ``n_samples`` is the number of samples and\n",
    "        ``n_features`` is the number of features.\n",
    "\n",
    "    y : array-like of shape (n_samples) or (n_samples, n_features)\n",
    "        Target relative to ``X`` for classification or regression;\n",
    "        None for unsupervised learning.\n",
    "\n",
    "    axes : array-like of shape (3,), default=None\n",
    "        Axes to use for plotting the curves.\n",
    "\n",
    "    ylim : tuple of shape (2,), default=None\n",
    "        Defines minimum and maximum y-values plotted, e.g. (ymin, ymax).\n",
    "\n",
    "    cv : int, cross-validation generator or an iterable, default=None\n",
    "        Determines the cross-validation splitting strategy.\n",
    "        Possible inputs for cv are:\n",
    "\n",
    "          - None, to use the default 5-fold cross-validation,\n",
    "          - integer, to specify the number of folds.\n",
    "          - :term:`CV splitter`,\n",
    "          - An iterable yielding (train, test) splits as arrays of indices.\n",
    "\n",
    "        For integer/None inputs, if ``y`` is binary or multiclass,\n",
    "        :class:`StratifiedKFold` used. If the estimator is not a classifier\n",
    "        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n",
    "\n",
    "        Refer :ref:`User Guide <cross_validation>` for the various\n",
    "        cross-validators that can be used here.\n",
    "\n",
    "    n_jobs : int or None, default=None\n",
    "        Number of jobs to run in parallel.\n",
    "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
    "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
    "        for more details.\n",
    "\n",
    "    train_sizes : array-like of shape (n_ticks,)\n",
    "        Relative or absolute numbers of training examples that will be used to\n",
    "        generate the learning curve. If the ``dtype`` is float, it is regarded\n",
    "        as a fraction of the maximum size of the training set (that is\n",
    "        determined by the selected validation method), i.e. it has to be within\n",
    "        (0, 1]. Otherwise it is interpreted as absolute sizes of the training\n",
    "        sets. Note that for classification the number of samples usually have\n",
    "        to be big enough to contain at least one sample from each class.\n",
    "        (default: np.linspace(0.1, 1.0, 5))\n",
    "    \"\"\"\n",
    "    if axes is None:\n",
    "        fig, axes = plt.subplots(3, 2, figsize=(10, 15))\n",
    "\n",
    "    axes = axes.reshape(-1)\n",
    "    fig.subplots_adjust(hspace=0.5, wspace=0.5)\n",
    "    fig = fig.delaxes(axes[-1])\n",
    "    \n",
    "    axes[0].set_title(title)\n",
    "    if ylim is not None:\n",
    "        axes[0].set_ylim(*ylim)\n",
    "    axes[0].set_xlabel(\"Training examples\")\n",
    "    axes[0].set_ylabel(\"Score\")\n",
    "\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    fit_times_mean = np.mean(fit_times, axis=1)\n",
    "    fit_times_std = np.std(fit_times, axis=1)\n",
    "    score_times_mean = np.mean(score_times, axis=1)\n",
    "    score_times_std = np.std(score_times, axis=1)\n",
    "\n",
    "    # Plot learning curve\n",
    "    axes[0].grid()\n",
    "    axes[0].fill_between(\n",
    "        train_sizes,\n",
    "        train_scores_mean - train_scores_std,\n",
    "        train_scores_mean + train_scores_std,\n",
    "        alpha=0.1,\n",
    "        color=\"r\",\n",
    "    )\n",
    "    axes[0].fill_between(\n",
    "        train_sizes,\n",
    "        test_scores_mean - test_scores_std,\n",
    "        test_scores_mean + test_scores_std,\n",
    "        alpha=0.1,\n",
    "        color=\"g\",\n",
    "    )\n",
    "    axes[0].plot(\n",
    "        train_sizes, train_scores_mean, \"o-\", color=\"r\", label=\"Training score\"\n",
    "    )\n",
    "    axes[0].plot(\n",
    "        train_sizes, test_scores_mean, \"o-\", color=\"g\", label=\"Cross-validation score\"\n",
    "    )\n",
    "    axes[0].legend(loc=\"best\")\n",
    "\n",
    "    # Plot n_samples vs fit_times\n",
    "    axes[1].grid()\n",
    "    axes[1].plot(train_sizes, fit_times_mean, \"o-\")\n",
    "    axes[1].fill_between(\n",
    "        train_sizes,\n",
    "        fit_times_mean - fit_times_std,\n",
    "        fit_times_mean + fit_times_std,\n",
    "        alpha=0.1,\n",
    "    )\n",
    "    axes[1].set_xlabel(\"Training examples\")\n",
    "    axes[1].set_ylabel(\"fit_times\")\n",
    "    axes[1].set_title(\"Scalability of the model\")\n",
    "\n",
    "    # Plot fit_time vs score\n",
    "    fit_time_argsort = fit_times_mean.argsort()\n",
    "    fit_time_sorted = fit_times_mean[fit_time_argsort]\n",
    "    test_scores_mean_sorted = test_scores_mean[fit_time_argsort]\n",
    "    test_scores_std_sorted = test_scores_std[fit_time_argsort]\n",
    "    axes[2].grid()\n",
    "    axes[2].plot(fit_time_sorted, test_scores_mean_sorted, \"o-\")\n",
    "    axes[2].fill_between(\n",
    "        fit_time_sorted,\n",
    "        test_scores_mean_sorted - test_scores_std_sorted,\n",
    "        test_scores_mean_sorted + test_scores_std_sorted,\n",
    "        alpha=0.1,\n",
    "    )\n",
    "    axes[2].set_xlabel(\"fit_times\")\n",
    "    axes[2].set_ylabel(\"Score\")\n",
    "    axes[2].set_title(\"Performance of the model\")\n",
    "\n",
    "    # Plot n_samples vs score_times\n",
    "    axes[3].grid()\n",
    "    axes[3].plot(train_sizes, score_times_mean, \"o-\")\n",
    "    axes[3].fill_between(\n",
    "        train_sizes,\n",
    "        score_times_mean - score_times_std,\n",
    "        score_times_mean + score_times_std,\n",
    "        alpha=0.1,\n",
    "    )\n",
    "    axes[3].set_xlabel(\"Training examples\")\n",
    "    axes[3].set_ylabel(\"score_times\")\n",
    "    axes[3].set_title(\"Scalability of the model\")\n",
    "\n",
    "    # Plot score_time vs score\n",
    "    score_time_argsort = score_times_mean.argsort()\n",
    "    score_time_sorted = score_times_mean[score_time_argsort]\n",
    "    test_scores_mean_sorted = test_scores_mean[score_time_argsort]\n",
    "    test_scores_std_sorted = test_scores_std[score_time_argsort]\n",
    "    axes[4].grid()\n",
    "    axes[4].plot(score_time_sorted, test_scores_mean_sorted, \"o-\")\n",
    "    axes[4].fill_between(\n",
    "        score_time_sorted,\n",
    "        test_scores_mean_sorted - test_scores_std_sorted,\n",
    "        test_scores_mean_sorted + test_scores_std_sorted,\n",
    "        alpha=0.1,\n",
    "    )\n",
    "    axes[4].set_xlabel(\"score_times\")\n",
    "    axes[4].set_ylabel(\"Score\")\n",
    "    axes[4].set_title(\"Performance of the model\")\n",
    "\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3  4  5  6  7  8  9 10 11]\n"
     ]
    }
   ],
   "source": [
    "model = DecisionTreeClassifier()\n",
    "#loop in rolling window method through all data\n",
    "unique_years = data['year'].unique()\n",
    "#sort the years\n",
    "unique_years.sort()\n",
    "rolling_window_results = []\n",
    "competition_predictions = []\n",
    "trained_models = list()\n",
    "\n",
    "\n",
    "\n",
    "print(unique_years)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_columns_to_remove():\n",
    "    return ['playoff']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GP', 'oRebounds', 'dRebounds', 'dq', 'PostMinutes', 'PostPoints', 'PostoRebounds', 'PostdRebounds', 'PostRebounds', 'PostAssists', 'PostSteals', 'PostBlocks', 'PostTurnovers', 'PostPF', 'PostDQ', 'ft%', 'fg%', 'three%', 'gs%', 'Postft%', 'Postfg%', 'Postthree%', 'Postgs%', 'efg%', 'ts%', 'ppg', 'rpg', 'apg', 'spg', 'bpg', 'eff', 'pp36', 'defensive_prowess', 'defensive_discipline', 'mpg', 'playoff_progression', 'award_count']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "toZeroArr = [c for c in data.columns if c not in input_cols]\n",
    "#add playoff to the list of columns to remove\n",
    "toZeroArr.remove('playoff')\n",
    "print(toZeroArr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_training_data(data, unique_years, i):\n",
    "    train_years_before = unique_years[:i]\n",
    "    train_years = unique_years[:i+1]\n",
    "    test_year = unique_years[i]\n",
    "\n",
    "    if i + 1 > len(unique_years): \n",
    "        return\n",
    "\n",
    "    predict_year = unique_years[i+1]  # Predicting for year i+2 \n",
    "\n",
    "    if predict_year < test_year:\n",
    "        return\n",
    "\n",
    "    # print(\"Train years:\", train_years)\n",
    "    # print(\"Predict Year: \", predict_year)\n",
    "    \n",
    "    train_before = data[data['year'].isin(train_years_before)]\n",
    "    train = data[data['year'].isin(train_years)]\n",
    "    test = data[data['year'] == test_year]\n",
    "    actual = data[data['year'] == predict_year]\n",
    "\n",
    "    return train_before, train, test, actual, predict_year, train_years, test_year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_data(train_before, train, test, actual, data, unique_years):\n",
    "    #one hot encoding\n",
    "    # train_before = pd.get_dummies(train_before, columns=['tmID', 'confID', 'pos', 'college', 'playerID'])\n",
    "    # train = pd.get_dummies(train, columns=['tmID', 'confID', 'pos', 'college', 'playerID']) \n",
    "    # Removing and adding rows based on certain conditions\n",
    "    # train_before_indices_to_remove = []\n",
    "    # train_indices_to_remove = []\n",
    "    # #check if a player have a entry in the train df with the next year\n",
    "    # for index, row in train_before.iterrows():\n",
    "    #     player_id = row['playerID']\n",
    "    #     year = row['year'] + 1 \n",
    "    #     if row['stint'] > 1:\n",
    "    #         train_before_indices_to_remove.append(index)\n",
    "    #     elif not (((train['playerID'] == player_id) & (train['year'] == year)).any()):\n",
    "    #         data_to_add = {\n",
    "    #             'playerID': player_id, \n",
    "    #             'year': year,        \n",
    "    #         }\n",
    "    #         new_row = pd.DataFrame(data_to_add, index=[0])\n",
    "    #         train = pd.concat([train, new_row], ignore_index=True)\n",
    "       \n",
    "\n",
    "    # train_before = train_before.drop(train_before_indices_to_remove)\n",
    "    # for index, row in train.iterrows():\n",
    "    #     player_id = row['playerID']\n",
    "    #     year = row['year'] - 1 \n",
    "    #     if row['stint'] > 1:\n",
    "    #         train_indices_to_remove.append(index)\n",
    "    #     elif not (((train_before['playerID'] == player_id) & (train_before['year'] == year)).any()):\n",
    "    #         data_to_add = {\n",
    "    #             'playerID': player_id, \n",
    "    #             'year': year,        \n",
    "    #         }\n",
    "    #         new_row = pd.DataFrame(data_to_add, index=[0])\n",
    "    #         train_before = pd.concat([train_before , new_row], ignore_index=True)\n",
    "\n",
    "    # train = train_before.drop(train_indices_to_remove)\n",
    "    # train.fillna(0, inplace=True)\n",
    "    # train.sort_values(by='playerID', inplace=True)\n",
    "    # train_before.fillna(0, inplace=True)\n",
    "    # train_before.sort_values(by='playerID', inplace=True)\n",
    "    remove_columns = get_columns_to_remove()\n",
    "    actual[toZeroArr] = 0\n",
    "\n",
    "    #remove stint > 0\n",
    "    train = train[train['stint'] == 0]\n",
    "    actual = actual[actual['stint'] == 0]\n",
    "\n",
    "\n",
    "    X_train = train.drop(remove_columns, axis=1)\n",
    "    y_train = train[output_cols]\n",
    "    X_actual = actual.drop(remove_columns, axis=1)\n",
    "    y_actual = actual[output_cols]\n",
    "\n",
    "    return X_train, y_train, X_actual, y_actual\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_and_evaluate_model(X_train, y_train, model):\n",
    "    classifier = model.fit(X_train, y_train)\n",
    "    return classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_and_evaluate(classifier, X_actual, y_actual):\n",
    "    accuracy = 0.0\n",
    "    auc_score = 0.0\n",
    "    # Evaluate the model\n",
    "    predictions = classifier.predict(X_actual)\n",
    "    predictions_prob = classifier.predict_proba(X_actual)[:, 1]\n",
    "\n",
    "    if X_actual['year'].unique()[0]==11:\n",
    "        competition_predictions.append(predictions)\n",
    "    else:\n",
    "        accuracy = accuracy_score(y_actual, predictions)\n",
    "        auc_score = roc_auc_score(y_actual, predictions_prob)\n",
    "\n",
    "    return accuracy, auc_score, predictions, predictions_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rolling_window_method(i, model, data, unique_years):\n",
    "    train_before, train, test, actual, predict_year, train_years, test_year = prepare_training_data(data, unique_years, i)\n",
    "    \n",
    "    if train_before is None:\n",
    "        return None\n",
    "\n",
    "    X_train, y_train, X_actual, y_actual= process_data(train_before, train, test, actual, data, unique_years)\n",
    "\n",
    "    #print years in X_train and X_actual\n",
    "    print(\"Unique years in X_train:\", X_train['year'].unique())\n",
    "    print(\"Unique years in X_actual:\", X_actual['year'].unique())\n",
    "    \n",
    "    #create a csv file for each year, storing the data for that year and that model, in the folder output/rolling_window\n",
    "    #X_train.to_csv(f'output/rolling_window/{model.__class__.__name__}_{test_year}_train.csv', index=False)\n",
    "    #X_test.to_csv(f'output/rolling_window/{model.__class__.__name__}_{test_year}_test.csv', index=False)\n",
    "    #X_actual.to_csv(f'output/rolling_window/{model.__class__.__name__}_{test_year}_actual.csv', index=False)\n",
    "    # y_actual.to_csv(f'output/rolling_window/{model.__class__.__name__}_{test_year}_y_actual.csv', index=False)\n",
    "\n",
    "\n",
    "    classifier = train_and_evaluate_model(X_train, y_train, model)\n",
    "\n",
    "    # Calculate accuracy\n",
    "\n",
    "    accuracy, auc_score, predictions, predictions_prob = predict_and_evaluate(classifier, X_actual, y_actual)\n",
    "\n",
    "    #print(f\"Best params for {model.__class__.__name_}: {model.best_params_}\")\n",
    "    # print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    # print(f\"AUC: {auc_score:.4f}\")\n",
    "\n",
    "\n",
    "    return {\n",
    "        'train_years': train_years,\n",
    "        'test_year': test_year,\n",
    "        'Year': predict_year,\n",
    "        'accuracy': accuracy,\n",
    "        'auc': auc_score,\n",
    "        'model': classifier,\n",
    "        'predictions': predictions,\n",
    "        'predictions_prob': predictions_prob,\n",
    "        'actual_results': y_actual,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def model_training_loop(model, data, unique_years):\n",
    "    rolling_window_results = []\n",
    "    for i in range(len(unique_years) - 1):\n",
    "        result= rolling_window_method(i, model, data, unique_years)\n",
    "        if result:\n",
    "            rolling_window_results.append(result)\n",
    "    return rolling_window_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = DecisionTreeClassifier()\n",
    "# rolling_window_results += model_training_loop(model, data, unique_years)\n",
    "# trained_models.append(rolling_window_results[-1]['model'])  # Get the last trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique years in X_train: [3]\n",
      "Unique years in X_actual: [4]\n",
      "Unique years in X_train: [4 3]\n",
      "Unique years in X_actual: [5]\n",
      "Unique years in X_train: [4 5 3]\n",
      "Unique years in X_actual: [6]\n",
      "Unique years in X_train: [4 5 6 3]\n",
      "Unique years in X_actual: [7]\n",
      "Unique years in X_train: [4 5 6 7 3]\n",
      "Unique years in X_actual: [8]\n",
      "Unique years in X_train: [4 5 6 7 8 3]\n",
      "Unique years in X_actual: [9]\n",
      "Unique years in X_train: [4 5 6 7 8 9 3]\n",
      "Unique years in X_actual: [10]\n",
      "Unique years in X_train: [ 4  5  6  7  8  9  3 10]\n",
      "Unique years in X_actual: [11]\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier()\n",
    "rolling_window_results += model_training_loop(model, data, unique_years)\n",
    "trained_models.append(rolling_window_results[-1]['model'])  # Get the last trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique years in X_train: [3]\n",
      "Unique years in X_actual: [4]\n",
      "Unique years in X_train: [4 3]\n",
      "Unique years in X_actual: [5]\n",
      "Unique years in X_train: [4 5 3]\n",
      "Unique years in X_actual: [6]\n",
      "Unique years in X_train: [4 5 6 3]\n",
      "Unique years in X_actual: [7]\n",
      "Unique years in X_train: [4 5 6 7 3]\n",
      "Unique years in X_actual: [8]\n",
      "Unique years in X_train: [4 5 6 7 8 3]\n",
      "Unique years in X_actual: [9]\n",
      "Unique years in X_train: [4 5 6 7 8 9 3]\n",
      "Unique years in X_actual: [10]\n",
      "Unique years in X_train: [ 4  5  6  7  8  9  3 10]\n",
      "Unique years in X_actual: [11]\n"
     ]
    }
   ],
   "source": [
    "model = GradientBoostingClassifier()\n",
    "rolling_window_results += model_training_loop(model, data, unique_years)\n",
    "trained_models.append(rolling_window_results[-1]['model'])  # Get the last trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique years in X_train: [3]\n",
      "Unique years in X_actual: [4]\n",
      "Unique years in X_train: [4 3]\n",
      "Unique years in X_actual: [5]\n",
      "Unique years in X_train: [4 5 3]\n",
      "Unique years in X_actual: [6]\n",
      "Unique years in X_train: [4 5 6 3]\n",
      "Unique years in X_actual: [7]\n",
      "Unique years in X_train: [4 5 6 7 3]\n",
      "Unique years in X_actual: [8]\n",
      "Unique years in X_train: [4 5 6 7 8 3]\n",
      "Unique years in X_actual: [9]\n",
      "Unique years in X_train: [4 5 6 7 8 9 3]\n",
      "Unique years in X_actual: [10]\n",
      "Unique years in X_train: [ 4  5  6  7  8  9  3 10]\n",
      "Unique years in X_actual: [11]\n"
     ]
    }
   ],
   "source": [
    "#gaussian naive bayes\n",
    "model = GaussianNB()\n",
    "rolling_window_results += model_training_loop(model, data, unique_years)\n",
    "trained_models.append(rolling_window_results[-1]['model'])  # Get the last trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique years in X_train: [3]\n",
      "Unique years in X_actual: [4]\n",
      "Unique years in X_train: [4 3]\n",
      "Unique years in X_actual: [5]\n",
      "Unique years in X_train: [4 5 3]\n",
      "Unique years in X_actual: [6]\n",
      "Unique years in X_train: [4 5 6 3]\n",
      "Unique years in X_actual: [7]\n",
      "Unique years in X_train: [4 5 6 7 3]\n",
      "Unique years in X_actual: [8]\n",
      "Unique years in X_train: [4 5 6 7 8 3]\n",
      "Unique years in X_actual: [9]\n",
      "Unique years in X_train: [4 5 6 7 8 9 3]\n",
      "Unique years in X_actual: [10]\n",
      "Unique years in X_train: [ 4  5  6  7  8  9  3 10]\n",
      "Unique years in X_actual: [11]\n"
     ]
    }
   ],
   "source": [
    "#logistic regression\n",
    "model = LogisticRegression()\n",
    "rolling_window_results += model_training_loop(model, data, unique_years)\n",
    "trained_models.append(rolling_window_results[-1]['model'])  # Get the last trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique years in X_train: [3]\n",
      "Unique years in X_actual: [4]\n",
      "Unique years in X_train: [4 3]\n",
      "Unique years in X_actual: [5]\n",
      "Unique years in X_train: [4 5 3]\n",
      "Unique years in X_actual: [6]\n",
      "Unique years in X_train: [4 5 6 3]\n",
      "Unique years in X_actual: [7]\n",
      "Unique years in X_train: [4 5 6 7 3]\n",
      "Unique years in X_actual: [8]\n",
      "Unique years in X_train: [4 5 6 7 8 3]\n",
      "Unique years in X_actual: [9]\n",
      "Unique years in X_train: [4 5 6 7 8 9 3]\n",
      "Unique years in X_actual: [10]\n",
      "Unique years in X_train: [ 4  5  6  7  8  9  3 10]\n",
      "Unique years in X_actual: [11]\n"
     ]
    }
   ],
   "source": [
    "#ada boost\n",
    "model = AdaBoostClassifier()\n",
    "rolling_window_results += model_training_loop(model, data, unique_years)\n",
    "trained_models.append(rolling_window_results[-1]['model'])  # Get the last trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique years in X_train: [3]\n",
      "Unique years in X_actual: [4]\n",
      "Unique years in X_train: [4 3]\n",
      "Unique years in X_actual: [5]\n",
      "Unique years in X_train: [4 5 3]\n",
      "Unique years in X_actual: [6]\n",
      "Unique years in X_train: [4 5 6 3]\n",
      "Unique years in X_actual: [7]\n",
      "Unique years in X_train: [4 5 6 7 3]\n",
      "Unique years in X_actual: [8]\n",
      "Unique years in X_train: [4 5 6 7 8 3]\n",
      "Unique years in X_actual: [9]\n",
      "Unique years in X_train: [4 5 6 7 8 9 3]\n",
      "Unique years in X_actual: [10]\n",
      "Unique years in X_train: [ 4  5  6  7  8  9  3 10]\n",
      "Unique years in X_actual: [11]\n"
     ]
    }
   ],
   "source": [
    "level0 = list()\n",
    "#append models in trained_models to level0\n",
    "for i in range(len(trained_models)):\n",
    "    level0.append((str(i), trained_models[i]))\n",
    "\n",
    "level1 = RandomForestClassifier(n_estimators=100, min_samples_split=2, min_samples_leaf=2, max_features='sqrt', max_depth=9, bootstrap=True, criterion='entropy', random_state=20)\n",
    "clf = StackingClassifier(estimators=level0, final_estimator=level1, cv=3)\n",
    "\n",
    "rolling_window_results += model_training_loop(clf, data, unique_years)\n",
    "trained_models.append(rolling_window_results[-1]['model'])  # Get the last trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'RandomForestClassifier': [0.39814814814814814, 0.3979591836734694, 0.375, 0.39805825242718446, 0.3177570093457944, 0.38461538461538464, 0.33, 0.0], 'GradientBoostingClassifier': [0.39814814814814814, 0.3979591836734694, 0.375, 0.39805825242718446, 0.32710280373831774, 0.38461538461538464, 0.33, 0.0], 'GaussianNB': [0.4166666666666667, 0.3877551020408163, 0.375, 0.39805825242718446, 0.3177570093457944, 0.38461538461538464, 0.33, 0.0], 'LogisticRegression': [0.6018518518518519, 0.5204081632653061, 0.6136363636363636, 0.6019417475728155, 0.6822429906542056, 0.6153846153846154, 0.67, 0.0], 'AdaBoostClassifier': [0.4074074074074074, 0.3979591836734694, 0.375, 0.39805825242718446, 0.3177570093457944, 0.38461538461538464, 0.33, 0.0], 'StackingClassifier': [0.4166666666666667, 0.3979591836734694, 0.375, 0.39805825242718446, 0.3177570093457944, 0.38461538461538464, 0.33, 0.0]}\n",
      "RandomForestClassifier\n",
      "[0.39814814814814814, 0.3979591836734694, 0.375, 0.39805825242718446, 0.3177570093457944, 0.38461538461538464, 0.33, 0.0]\n",
      "GradientBoostingClassifier\n",
      "[0.39814814814814814, 0.3979591836734694, 0.375, 0.39805825242718446, 0.32710280373831774, 0.38461538461538464, 0.33, 0.0]\n",
      "GaussianNB\n",
      "[0.4166666666666667, 0.3877551020408163, 0.375, 0.39805825242718446, 0.3177570093457944, 0.38461538461538464, 0.33, 0.0]\n",
      "LogisticRegression\n",
      "[0.6018518518518519, 0.5204081632653061, 0.6136363636363636, 0.6019417475728155, 0.6822429906542056, 0.6153846153846154, 0.67, 0.0]\n",
      "AdaBoostClassifier\n",
      "[0.4074074074074074, 0.3979591836734694, 0.375, 0.39805825242718446, 0.3177570093457944, 0.38461538461538464, 0.33, 0.0]\n",
      "StackingClassifier\n",
      "[0.4166666666666667, 0.3979591836734694, 0.375, 0.39805825242718446, 0.3177570093457944, 0.38461538461538464, 0.33, 0.0]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# {\n",
    "#         'train_years': train_years,\n",
    "#         'test_year': test_year,\n",
    "#         'Year': predict_year,\n",
    "#         'accuracy': accuracy,\n",
    "#         'auc': auc_score,\n",
    "#         'model': classifier,\n",
    "#         'predictions': predictions,\n",
    "#         'predictions_prob': predictions_prob,\n",
    "#         'actual_results': y_actual,\n",
    "# }\n",
    "\n",
    "#make a dictionary of the model: [accuracy]\n",
    "model_accuracy = dict()\n",
    "for result in rolling_window_results:\n",
    "    model_name = result['model'].__class__.__name__\n",
    "    if model_name not in model_accuracy:\n",
    "        model_accuracy[model_name] = []\n",
    "    model_accuracy[model_name].append(result['accuracy'])\n",
    "\n",
    "print(model_accuracy)\n",
    "\n",
    "#plot model accuracy in one line chart (x-axis: year, y-axis: accuracy) years are 3,4,5,6,7,8,9,10,11\n",
    "years = [3,4,5,6,7,8,9,10,11]\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Accuracy')\n",
    "for model_name in model_accuracy:\n",
    "    try:\n",
    "        plt.plot(years, model_accuracy[model_name], label=model_name)\n",
    "    except:\n",
    "        print(model_accuracy[model_name])\n",
    "plt.legend()\n",
    "plt.savefig('output/rolling_window/model_accuracy.png')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
